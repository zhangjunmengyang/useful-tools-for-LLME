{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b7b5fe-732f-4bf5-a860-ea3474fa4951",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "- tokenizer rule\n",
    "- train\n",
    "- encode\n",
    "- decode\n",
    "- tokenizer process\n",
    "    - pre-process\n",
    "    - padding\n",
    "    - trunction\n",
    "- tokenizer IO\n",
    "    - save\n",
    "    - load\n",
    "- `TokenizerBase` class\n",
    "- `TokenizerWord` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a6ab5-57e0-413d-b3e3-5ecec5c14f7f",
   "metadata": {},
   "source": [
    "## Tokenizer Rule\n",
    "\n",
    "给定文本：\n",
    "\n",
    "1. “我唱跳和rap有 2 年半。”\n",
    "2. “I have 12 apples!”\n",
    "\n",
    "token 定义： 文本离散序列表示中的最小元素。最小的粒度，可以是 “字符” character，“单词” word 实际上也是一种粒度。 \n",
    "\n",
    "例如“live”可以是一个 token， 也可以是序列 `l`,`i`, `v`, `e`\n",
    "\n",
    "什么是分词器（Tokenizer）？\n",
    "\n",
    "分词 是将文本进行离散化表示的 规则，例如 character-level 切分 或 word-level 切分规则\n",
    "\n",
    "所分的词，可以构建一个词表来存储。\n",
    "\n",
    "分词器是 “分词规则 + 词表” 的操作集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf48d7a-d0eb-4295-8ddb-daa2fd5567fd",
   "metadata": {},
   "source": [
    "期望分词后的 token 列表为:\n",
    "1. `我`,`唱`,`跳`,`和`,`rap`,`有`,` `,`2`,` `,`年`,`半`,`。`”\n",
    "2. `I`,` `,`have`,` `,`1`,`2`,` `,`apples`,`!`\n",
    "\n",
    "    自定义分割规则\n",
    "    \n",
    "    1. 先将“特殊词元”, “标点符号”, “中文字符” 进行分割。 特殊词元如 `<EOS>` 是一个整体，要先提取，否则存在符号 `<`, `>` 会被拆分\n",
    "    2. 数字要离散分割，如“12” -> `1`, `2`\n",
    "    3. 空格 ` ` 也是独立的 token\n",
    "    4. 将常用的符号进行初始化词表, 如`a`, `!`, `%`... 常见符号, 如果有 26 个字母, 可以排列出所有**不带其他符号**的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c6afc5-7d95-4de5-8c6d-77f861a1f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<SOS>我唱跳和rap有 2 年半。<EOS><SOS>I have 12 apples!<PAD><PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50601724-7101-42e3-9708-e8a72323742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，。！？；：“”‘’【】（）《》、!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~ \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "zh_symbols = '，。！？；：“”‘’【】（）《》、'\n",
    "en_symbols = re.escape(string.punctuation)  # 转义特殊字符\n",
    "all_symbols = zh_symbols + en_symbols + ' '  # add space\n",
    "print(all_symbols)\n",
    "\n",
    "# 构建正则表达式：(不要求掌握）\n",
    "# 1. [{}] - 匹配任意标点符号（1个）\n",
    "# 2. \\d    - 匹配任意数字（1个）\n",
    "# 3. [\\u4e00-\\u9fa5] - 匹配任意中文字符（1个）\n",
    "# 4. [^{}\\d\\u4e00-\\u9fa5]+ - 匹配其他连续字符\n",
    "# pattern = f'[^{all_symbols}\\d\\u4e00-\\u9fa5]+|[{all_symbols}]|\\d|[\\u4e00-\\u9fa5]'\n",
    "\n",
    "special_tokens = ['<SOS>', '<EOS>', '<PAD>', '<UNK>']\n",
    "\n",
    "pattern = (\n",
    "    r'(?:' + '|'.join(special_tokens) + ')'   # 非捕获组，匹配任意固定标签\n",
    "    r'|[' + re.escape(all_symbols) + ']'  # 匹配标点符号\n",
    "    r'|\\d'  # 匹配单个数字\n",
    "    r'|[\\u4e00-\\u9fa5]'  # 匹配单个中文字符\n",
    "    r'|[^' + re.escape(all_symbols) + r'\\d\\u4e00-\\u9fa5<>]+'  # 匹配其他连续字符\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e57bcf-3043-4767-aabb-06bf1ccbae3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', '我', '唱', '跳', '和', 'rap', '有', ' ', '2', ' ', '年', '半', '。', '<EOS>', '<SOS>', 'I', ' ', 'have', ' ', '1', '2', ' ', 'apples', '!', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "token_list = re.findall(pattern, text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525b059-b3b6-4976-90ff-7b01152454df",
   "metadata": {},
   "source": [
    "输出有重复的词元，构建词表需要进行去重，如使用 set 或 dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f6260-21e2-4da3-a9fb-7c57c6f4f07e",
   "metadata": {},
   "source": [
    "## 语料示例\n",
    "\n",
    "仅做展示，无需了解文本内容。\n",
    "\n",
    "**Large Language Models (LLMs):**  \n",
    "Modern LLMs, such as OpenAI's GPT-4 (2023) and Meta's LLaMA-3 (2024), leverage transformer architectures (Vaswani et al., 2017) with self-attention mechanisms:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "These models scale to hundreds of billions of parameters (e.g., GPT-4: ~1.8T, LLaMA-3: ~400B), enabling state-of-the-art performance in NLP tasks. Training requires massive datasets (e.g., >1T tokens) and distributed computing frameworks.  \n",
    "\n",
    "**大语言模型（LLM）技术：**  \n",
    "现代大语言模型（如OpenAI的GPT-4（2023）和Meta的LLaMA-3（2024））基于Transformer架构（Vaswani等，2017），其自注意力机制公式为：  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "此类模型的参数量已达数千亿（如GPT-4约1.8万亿，LLaMA-3约4000亿），依赖超大规模训练数据（>1万亿词元）和分布式计算框架，推动NLP任务性能突破。  \n",
    "\n",
    " \n",
    "**Scaling Laws & Trends:**  \n",
    "**LLM Scaling Trends:**  \n",
    "Empirical studies (Kaplan et al., 2020) show model performance scales as a power-law with compute budget ($C$), dataset size ($D$), and parameters ($N$):  \n",
    "\n",
    "\\begin{equation}\n",
    "L(N, D) \\approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N} + \\left(\\frac{D_c}{D}\\right)^{\\alpha_D} + L_\\infty\n",
    "\\end{equation}\n",
    "\n",
    "For example, Google's PaLM-2 (2023, 340B params) achieved 85% multilingual accuracy, while smaller models (e.g., Mistral-7B, 2024) optimize efficiency via sparse architectures.  \n",
    "\n",
    "**大语言模型的扩展定律：**  \n",
    "实证研究（Kaplan等，2020）表明，模型性能随算力（$C$）、数据量（$D$）和参数量（$N$）呈幂律增长：  \n",
    "\n",
    "\\begin{equation}\n",
    "L(N, D) \\approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N} + \\left(\\frac{D_c}{D}\\right)^{\\alpha_D} + L_\\infty\n",
    "\\end{equation}\n",
    "\n",
    "例如，谷歌的PaLM-2（2023，3400亿参数）实现了85%的多语言准确率，而小规模模型（如Mistral-7B，2024）通过稀疏架构提升效率。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a62728-83c6-4d0b-a51c-ba8e0dd3d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\#'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\#'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/d7/f63c4ndj5cv0ky5nj5t6ycq40000gn/T/ipykernel_11172/686499991.py:7: SyntaxWarning: invalid escape sequence '\\#'\n",
      "  ， 。 ！？；：“”‘’【】（）《》、!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "/var/folders/d7/f63c4ndj5cv0ky5nj5t6ycq40000gn/T/ipykernel_11172/686499991.py:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \\ begin{equation}\n"
     ]
    }
   ],
   "source": [
    "# 注意增加一段 26 大小写字母 和 10 个数字\n",
    "text_init = \"\"\"\n",
    " a b c d e f g h i j k l m n o p q r s t u v w x y z \n",
    " A B C D E F G H I J K L M N O P Q R S T U V W X Y Z \n",
    " 0 1 2 3 4 5 6 7 8 9 10 \n",
    " <SOS> <EOS> <UNK> <PAD>\n",
    " ， 。 ！？；：“”‘’【】（）《》、!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~ \n",
    "\"\"\"\n",
    "\n",
    "corpus_text = \"\"\"\n",
    "\n",
    "**Large Language Models (LLMs):**  \n",
    "Modern LLMs, such as OpenAI's GPT-4 (2023) and Meta's LLaMA-3 (2024), leverage transformer architectures (Vaswani et al., 2017) with self-attention mechanisms:  \n",
    "\n",
    "\\ begin{equation}\n",
    "\\ text{Attention}(Q, K, V) = \\ text{softmax}\\left(\\ frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "These models scale to hundreds of billions of parameters (e.g., GPT-4: ~1.8T, LLaMA-3: ~400B), enabling state-of-the-art performance in NLP tasks. Training requires massive datasets (e.g., >1T tokens) and distributed computing frameworks.  \n",
    "\n",
    "**大语言模型（LLM）技术：**  \n",
    "现代大语言模型（如OpenAI的GPT-4（2023）和Meta的LLaMA-3（2024））基于Transformer架构（Vaswani等，2017），其自注意力机制公式为：  \n",
    "\n",
    "\\ begin{equation}\n",
    "\\ text{Attention}(Q, K, V) = \\ text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "此类模型的参数量已达数千亿（如GPT-4约1.8万亿，LLaMA-3约4000亿），依赖超大规模训练数据（>1万亿词元）和分布式计算框架，推动NLP任务性能突破。  \n",
    "\n",
    "\n",
    "\n",
    "**Scaling Laws & Trends:**  \n",
    "**LLM Scaling Trends:**  \n",
    "Empirical studies (Kaplan et al., 2020) show model performance scales as a power-law with compute budget ($C$), dataset size ($D$), and parameters ($N$):  \n",
    "\n",
    "\\ begin{equation}\n",
    "L(N, D) \\ approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N} + \\left(\\frac{D_c}{D}\\right)^{\\alpha_D} + L_\\infty\n",
    "\\end{equation}\n",
    "\n",
    "For example, Google's PaLM-2 (2023, 340B params) achieved 85% multilingual accuracy, while smaller models (e.g., Mistral-7B, 2024) optimize efficiency via sparse architectures.  \n",
    "\n",
    "**大语言模型的扩展定律：**  \n",
    "实证研究（Kaplan等，2020）表明，模型性能随算力（$C$）、数据量（$D$）和参数量（$N$）呈幂律增长：  \n",
    "\n",
    "\\begin{equation}\n",
    "L(N, D) \\approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N} + \\left(\\frac{D_c}{D}\\right)^{\\alpha_D} + L_\\infty\n",
    "\\end{equation}\n",
    "\n",
    "例如，谷歌的PaLM-2（2023，3400亿参数）实现了85%的多语言准确率，而小规模模型（如Mistral-7B，2024）通过稀疏架构提升效率。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426366fc-0cfc-49aa-96af-573886ce2114",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c289e3bc-d30d-4200-a7d6-f7aba3f02262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', 'a', ' ', 'b', ' ', 'c', ' ', 'd', ' ', 'e', ' ', 'f', ' ', 'g', ' ', 'h', ' ', 'i', ' ', 'j', ' ', 'k', ' ', 'l', ' ', 'm', ' ', 'n', ' ', 'o', ' ', 'p', ' ', 'q', ' ', 'r', ' ', 's', ' ', 't', ' ', 'u', ' ', 'v', ' ', 'w', ' ', 'x', ' ', 'y', ' ', 'z', ' ', '\\n', ' ', 'A', ' ', 'B', ' ', 'C', ' ', 'D', ' ', 'E', ' ', 'F', ' ', 'G', ' ', 'H', ' ', 'I', ' ', 'J', ' ', 'K', ' ', 'L', ' ', 'M', ' ', 'N', ' ', 'O', ' ', 'P', ' ', 'Q', ' ', 'R', ' ', 'S', ' ', 'T', ' ', 'U', ' ', 'V', ' ', 'W', ' ', 'X', ' ', 'Y', ' ', 'Z', ' ', '\\n', ' ', '0', ' ', '1', ' ', '2', ' ', '3', ' ', '4', ' ', '5', ' ', '6', ' ', '7', ' ', '8', ' ', '9', ' ', '1', '0', ' ', '\\n', ' ', '<SOS>', ' ', '<EOS>', ' ', '<UNK>', ' ', '<PAD>', '\\n', ' ', '，', ' ', '。', ' ', '！', '？', '；', '：', '“', '”', '‘', '’', '【', '】', '（', '）', '《', '》', '、', '!', '\"', '\\\\', '#', '\\\\', '$', '%', '\\\\', '&', \"'\", '\\\\', '(', '\\\\', ')', '\\\\', '*', '\\\\', '+', ',', '\\\\', '-', '\\\\', '.', '/', ':', ';', '<', '=', '>', '\\\\', '?', '@', '\\\\', '[', '\\\\', '\\\\', ']', '\\\\', '^', '_', '`', '\\\\', '{', '\\\\', '|', '\\\\', '}', '\\\\', '~', ' ', '\\n']\n",
      "['\\n\\n', '*', '*', 'Large', ' ', 'Language', ' ', 'Models', ' ', '(', 'LLMs', ')', ':', '*', '*', ' ', ' ', '\\nModern', ' ', 'LLMs', ',', ' ', 'such', ' ', 'as', ' ', 'OpenAI', \"'\", 's', ' ', 'GPT', '-', '4', ' ', '(', '2', '0', '2', '3', ')', ' ', 'and', ' ', 'Meta', \"'\", 's', ' ', 'LLaMA', '-', '3', ' ', '(', '2', '0', '2', '4', ')', ',', ' ', 'leverage', ' ', 'transformer', ' ', 'architectures', ' ', '(', 'Vaswani', ' ', 'et', ' ', 'al', '.', ',', ' ', '2', '0', '1', '7', ')', ' ', 'with', ' ', 'self', '-', 'attention', ' ', 'mechanisms', ':', ' ', ' ', '\\n\\n', '\\\\', ' ', 'begin', '{', 'equation', '}', '\\n', '\\\\', ' ']\n",
      "924\n"
     ]
    }
   ],
   "source": [
    "token_init_list = re.findall(pattern, text_init)\n",
    "token_corpus_list = re.findall(pattern, corpus_text)\n",
    "print(token_init_list)\n",
    "print(token_corpus_list[:100])\n",
    "print(len(token_corpus_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b3aaf27-994c-4b7b-9de8-ddf3ede3a9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始token列表大小: 1138\n",
      "词表大小: 308\n",
      "词表: {'\\n': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '0': 54, '1': 55, '2': 56, '3': 57, '4': 58, '5': 59, '6': 60, '7': 61, '8': 62, '9': 63, '<SOS>': 64, '<EOS>': 65, '<UNK>': 66, '<PAD>': 67, '，': 68, '。': 69, '！': 70, '？': 71, '；': 72, '：': 73, '“': 74, '”': 75, '‘': 76, '’': 77, '【': 78, '】': 79, '（': 80, '）': 81, '《': 82, '》': 83, '、': 84, '!': 85, '\"': 86, '\\\\': 87, '#': 88, '$': 89, '%': 90, '&': 91, \"'\": 92, '(': 93, ')': 94, '*': 95, '+': 96, ',': 97, '-': 98, '.': 99, '/': 100, ':': 101, ';': 102, '<': 103, '=': 104, '>': 105, '?': 106, '@': 107, '[': 108, ']': 109, '^': 110, '_': 111, '`': 112, '{': 113, '|': 114, '}': 115, '~': 116, '\\n\\n': 117, 'Large': 118, 'Language': 119, 'Models': 120, 'LLMs': 121, '\\nModern': 122, 'such': 123, 'as': 124, 'OpenAI': 125, 'GPT': 126, 'and': 127, 'Meta': 128, 'LLaMA': 129, 'leverage': 130, 'transformer': 131, 'architectures': 132, 'Vaswani': 133, 'et': 134, 'al': 135, 'with': 136, 'self': 137, 'attention': 138, 'mechanisms': 139, 'begin': 140, 'equation': 141, 'text': 142, 'Attention': 143, 'softmax': 144, 'left': 145, 'frac': 146, 'QK': 147, 'sqrt': 148, '\\right': 149, 'V\\n': 150, 'end': 151, '\\n\\nThese': 152, 'models': 153, 'scale': 154, 'to': 155, 'hundreds': 156, 'of': 157, 'billions': 158, 'parameters': 159, 'enabling': 160, 'state': 161, 'the': 162, 'art': 163, 'performance': 164, 'in': 165, 'NLP': 166, 'tasks': 167, 'Training': 168, 'requires': 169, 'massive': 170, 'datasets': 171, 'tokens': 172, 'distributed': 173, 'computing': 174, 'frameworks': 175, '大': 176, '语': 177, '言': 178, '模': 179, '型': 180, 'LLM': 181, '技': 182, '术': 183, '现': 184, '代': 185, '如': 186, '的': 187, '和': 188, '基': 189, '于': 190, 'Transformer': 191, '架': 192, '构': 193, '等': 194, '其': 195, '自': 196, '注': 197, '意': 198, '力': 199, '机': 200, '制': 201, '公': 202, '式': 203, '为': 204, '\\x0crac': 205, '此': 206, '类': 207, '参': 208, '数': 209, '量': 210, '已': 211, '达': 212, '千': 213, '亿': 214, '约': 215, '万': 216, '依': 217, '赖': 218, '超': 219, '规': 220, '训': 221, '练': 222, '据': 223, '词': 224, '元': 225, '分': 226, '布': 227, '计': 228, '算': 229, '框': 230, '推': 231, '动': 232, '任': 233, '务': 234, '性': 235, '能': 236, '突': 237, '破': 238, '\\n\\n\\n\\n': 239, 'Scaling': 240, 'Laws': 241, 'Trends': 242, '\\nEmpirical': 243, 'studies': 244, 'Kaplan': 245, 'show': 246, 'model': 247, 'scales': 248, 'power': 249, 'law': 250, 'compute': 251, 'budget': 252, 'dataset': 253, 'size': 254, '\\nL': 255, 'approx': 256, '\\x07lpha': 257, 'infty\\n': 258, '\\n\\nFor': 259, 'example': 260, 'Google': 261, 'PaLM': 262, 'params': 263, 'achieved': 264, 'multilingual': 265, 'accuracy': 266, 'while': 267, 'smaller': 268, 'Mistral': 269, 'optimize': 270, 'efficiency': 271, 'via': 272, 'sparse': 273, '扩': 274, '展': 275, '定': 276, '律': 277, '实': 278, '证': 279, '研': 280, '究': 281, '表': 282, '明': 283, '随': 284, '呈': 285, '幂': 286, '增': 287, '长': 288, '\\n\\n\\x08egin': 289, '\\x07pprox': 290, '例': 291, '谷': 292, '歌': 293, '了': 294, '多': 295, '准': 296, '确': 297, '率': 298, '而': 299, '小': 300, '通': 301, '过': 302, '稀': 303, '疏': 304, '提': 305, '升': 306, '效': 307}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "# 构建词表 {\"token\", token_id}\n",
    "\n",
    "# 拼接\n",
    "token_all = token_init_list + token_corpus_list\n",
    "print(\"原始token列表大小:\",len(token_all))\n",
    "\n",
    "# Dict[str, int]\n",
    "vocab : Dict[str, int] = {}\n",
    "vocab_reverse: Dict[str, int] = {}\n",
    "idx = 0\n",
    "for value in token_all:\n",
    "    if value not in vocab:\n",
    "        vocab[value] = idx\n",
    "        vocab_reverse[idx] = value\n",
    "        idx += 1\n",
    "        \n",
    "print(\"词表大小:\", len(vocab))\n",
    "print(\"词表:\", vocab)  # 输出: {'a': 0, 'b': 1}\n",
    "# print(\"词表反向:\", vocab_reverse)  # 输出: {0: 'a', 1: 'b'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bb090-c965-4f7c-9caf-a43aca6d654f",
   "metadata": {},
   "source": [
    "## encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840bfe4b-5be1-4688-b9b1-faa4c3547592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(vocab, pattern, text):\n",
    "    \"\"\"\n",
    "    词表编码\n",
    "    \"\"\"\n",
    "    tokens = re.findall(pattern, text) # 分词规则\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            token_ids.append(vocab[token])\n",
    "        else: \n",
    "            token_ids.append(vocab['<UNK>'])\n",
    "    return tokens, token_ids\n",
    "\n",
    "# text = \"<SOS>我唱跳和rap有 2 年半。<EOS><SOS>I have 12 apples!<PAD><PAD>\"\n",
    "# tokens_new, token_ids_new = encode(vocab, pattern, text)\n",
    "# print(tokens_new)\n",
    "# print(token_ids_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69402a37-e58c-4adb-997b-693392823f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Modern', ' ', 'LLMs', ',', ' ', 'such', ' ', 'as', ' ', 'OpenAI', \"'\", 's', ' ', 'GPT', '-', '4', ' ', '(', '2', '0', '2', '3', ')', ' ', 'and', ' ', 'Meta', \"'\", 's', ' ', 'LLaMA', '-', '3', ' ', '(', '2', '0', '2', '4', ')', ',', ' ', 'leverage', ' ', 'transformer', ' ', 'architectures', ' ', '(', 'Vaswani', ' ', 'et', ' ', 'al', '.', ',', ' ', '2', '0', '1', '7', ')', ' ', 'with', ' ', 'self', '-', 'attention', ' ', 'mechanisms']\n",
      "[66, 1, 121, 97, 1, 123, 1, 124, 1, 125, 92, 20, 1, 126, 98, 58, 1, 93, 56, 54, 56, 57, 94, 1, 127, 1, 128, 92, 20, 1, 129, 98, 57, 1, 93, 56, 54, 56, 58, 94, 97, 1, 130, 1, 131, 1, 132, 1, 93, 133, 1, 134, 1, 135, 99, 97, 1, 56, 54, 55, 61, 94, 1, 136, 1, 137, 98, 138, 1, 139]\n"
     ]
    }
   ],
   "source": [
    "# 对训练文本编码\n",
    "text_tmp = '''Modern LLMs, such as OpenAI's GPT-4 (2023) and Meta's LLaMA-3 (2024), leverage transformer architectures (Vaswani et al., 2017) with self-attention mechanisms'''\n",
    "tokens, token_ids = encode(vocab, pattern, text_tmp)\n",
    "print(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc0964-0208-44ee-afbd-83ca6d7f21bb",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e6ac7f-ea05-48c0-aedd-7cdb25d6090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(vocab_reverse, token_ids, skip_special_tokens = False):\n",
    "    decode_token = []\n",
    "    for idx in token_ids:\n",
    "        # if idx in \n",
    "        decode_token.append(vocab_reverse[idx])\n",
    "    return decode_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107984a7-c51f-4d44-848f-09216d03fa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', ' ', 'LLMs', ',', ' ', 'such', ' ', 'as', ' ', 'OpenAI', \"'\", 's', ' ', 'GPT', '-', '4', ' ', '(', '2', '0', '2', '3', ')', ' ', 'and', ' ', 'Meta', \"'\", 's', ' ', 'LLaMA', '-', '3', ' ', '(', '2', '0', '2', '4', ')', ',', ' ', 'leverage', ' ', 'transformer', ' ', 'architectures', ' ', '(', 'Vaswani', ' ', 'et', ' ', 'al', '.', ',', ' ', '2', '0', '1', '7', ')', ' ', 'with', ' ', 'self', '-', 'attention', ' ', 'mechanisms']\n",
      "<UNK> LLMs, such as OpenAI's GPT-4 (2023) and Meta's LLaMA-3 (2024), leverage transformer architectures (Vaswani et al., 2017) with self-attention mechanisms\n"
     ]
    }
   ],
   "source": [
    "decode_token = decode(vocab_reverse, token_ids)\n",
    "print(decode_token)\n",
    "print(''.join(decode_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916b0ba-06b1-48af-a818-958072835d3d",
   "metadata": {},
   "source": [
    "## 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8db2dc5e-277d-4e7d-9391-b9f34893fd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原文本: <SOS>我唱跳和rap有 2 年半。<EOS><SOS>I have 12 apples!<PAD><PAD>\n",
      "\n",
      "分词: ['<SOS>', '我', '唱', '跳', '和', 'rap', '有', ' ', '2', ' ', '年', '半', '。', '<EOS>', '<SOS>', 'I', ' ', 'have', ' ', '1', '2', ' ', 'apples', '!', '<PAD>', '<PAD>']\n",
      "\n",
      "token ids: [64, 66, 66, 66, 188, 66, 66, 1, 56, 1, 66, 66, 69, 65, 64, 36, 1, 66, 1, 55, 56, 1, 66, 85, 67, 67]\n",
      "\n",
      "decode ids: ['<SOS>', '<UNK>', '<UNK>', '<UNK>', '和', '<UNK>', '<UNK>', ' ', '2', ' ', '<UNK>', '<UNK>', '。', '<EOS>', '<SOS>', 'I', ' ', '<UNK>', ' ', '1', '2', ' ', '<UNK>', '!', '<PAD>', '<PAD>']\n",
      "\n",
      "解码文本 <SOS><UNK><UNK><UNK>和<UNK><UNK> 2 <UNK><UNK>。<EOS><SOS>I <UNK> 12 <UNK>!<PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "text = \"<SOS>我唱跳和rap有 2 年半。<EOS><SOS>I have 12 apples!<PAD><PAD>\"\n",
    "tokens, token_ids = encode(vocab, pattern, text)\n",
    "print('\\n原文本:', text)\n",
    "print('\\n分词:',tokens)\n",
    "print('\\ntoken ids:',token_ids)\n",
    "decode_token = decode(vocab_reverse, token_ids)\n",
    "print('\\ndecode ids:',decode_token)\n",
    "print('\\n解码文本',''.join(decode_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881e5bb-8927-4809-8e6e-bb92a366c3e2",
   "metadata": {},
   "source": [
    "上述 '我' 词元并不在 词表中，所以按照规则，被编成了 '<UNK>', 会导致原始文本信息错误\n",
    "\n",
    "优化 编码 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760616b3-b00a-435d-90cf-3bf50e7fdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_anything(vocab, pattern, text):\n",
    "    \"\"\"\n",
    "    词表编码\n",
    "    \"\"\"\n",
    "    tokens = re.findall(pattern, text) # 分词规则\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            token_ids.append(vocab[token])\n",
    "        else: \n",
    "            if len(token) == 1:\n",
    "                token_ids.append(vocab['<UNK>'])\n",
    "            else:\n",
    "                for t in token:\n",
    "                    token_ids.append( vocab[t] )\n",
    "    return tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ea853f9-aad9-4b09-be7f-d24c660777e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原文本: <SOS>我唱跳和rap有 2 年半。<EOS><SOS>I have 12 apples!<PAD><PAD>\n",
      "\n",
      "分词: ['<SOS>', '我', '唱', '跳', '和', 'rap', '有', ' ', '2', ' ', '年', '半', '。', '<EOS>', '<SOS>', 'I', ' ', 'have', ' ', '1', '2', ' ', 'apples', '!', '<PAD>', '<PAD>']\n",
      "\n",
      "token ids: [64, 66, 66, 66, 188, 19, 2, 17, 66, 1, 56, 1, 66, 66, 69, 65, 64, 36, 1, 9, 2, 23, 6, 1, 55, 56, 1, 2, 17, 17, 13, 6, 20, 85, 67, 67]\n",
      "\n",
      "decode ids: ['<SOS>', '<UNK>', '<UNK>', '<UNK>', '和', 'r', 'a', 'p', '<UNK>', ' ', '2', ' ', '<UNK>', '<UNK>', '。', '<EOS>', '<SOS>', 'I', ' ', 'h', 'a', 'v', 'e', ' ', '1', '2', ' ', 'a', 'p', 'p', 'l', 'e', 's', '!', '<PAD>', '<PAD>']\n",
      "\n",
      "解码文本 <SOS><UNK><UNK><UNK>和rap<UNK> 2 <UNK><UNK>。<EOS><SOS>I have 12 apples!<PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "text = \"<SOS>我唱跳和rap有 2 年半。<EOS><SOS>I have 12 apples!<PAD><PAD>\"\n",
    "tokens, token_ids = encode_anything(vocab, pattern, text) # 新的编码函数\n",
    "print('\\n原文本:', text)\n",
    "print('\\n分词:',tokens)\n",
    "print('\\ntoken ids:',token_ids)\n",
    "decode_token = decode(vocab_reverse, token_ids)\n",
    "print('\\ndecode ids:',decode_token)\n",
    "print('\\n解码文本',''.join(decode_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892ae2a-30a2-45a3-ba17-fa84c1652fc1",
   "metadata": {},
   "source": [
    "以上文本中 `rap`, `have`, `apples` 可以被编码，\n",
    "\n",
    "如 `apples` -> `'a', 'p', 'p', 'l', 'e', 's'`\n",
    "\n",
    "由于词表中不存在 `我`, 则永远编码不出来，这是所有 tokenizer 的缺陷,\n",
    "\n",
    "如果词表中没有 `s`，那么 `'a', 'p', 'p', 'l', 'e', '<UNK>'`\n",
    "\n",
    "解决办法是, 在中文语料里独立训练一个新词表，融合到已有词表中。\n",
    "\n",
    "规则是唯一的，编码结果是唯一的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bd161-f4fb-4a59-aabd-c6c6e54c580a",
   "metadata": {},
   "source": [
    "## 分词实例 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d681ebb-2b45-4f03-a74a-9da35a90d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原文本: \n",
      "### **Love Story**  \n",
      "*By Taylor Swift*  \n",
      "\n",
      "**[Verse 1]**  \n",
      "We were both young when I first saw you  \n",
      "\n",
      "分词: ['\\n', '#', '#', '#', ' ', '*', '*', 'Love', ' ', 'Story', '*', '*', ' ', ' ', '\\n', '*', 'By', ' ', 'Taylor', ' ', 'Swift', '*', ' ', ' ', '\\n\\n', '*', '*', '[', 'Verse', ' ', '1', ']', '*', '*', ' ', ' ', '\\nWe', ' ', 'were', ' ', 'both', ' ', 'young', ' ', 'when', ' ', 'I', ' ', 'first', ' ', 'saw', ' ', 'you', ' ', ' ', '\\nI', ' ', 'close', ' ', 'my', ' ', 'eyes', ' ', 'and', ' ', 'the', ' ', 'flashback', ' ', 'starts', ' ', ' ', '\\nI', \"'\", 'm', ' ', 'standing', ' ', 'there', ' ', 'on', ' ', 'a', ' ', 'balcony', ' ', 'in', ' ', 'summer', ' ', 'air', ' ', ' ', '\\n\\nSee', ' ', 'the', ' ', 'lights', ',', ' ']\n",
      "\n",
      "token ids: [0, 88, 88, 88, 1, 95, 95, 39, 16, 23, 6, 1, 46, 21, 16, 19, 26, 95, 95, 1, 1, 0, 95, 29, 26, 1, 47, 2, 26, 13, 16, 19, 1, 46, 24, 10, 7, 21, 95, 1, 1, 117, 95, 95, 108, 49, 6, 19, 20, 6, 1, 55, 109, 95, 95, 1, 1, 0, 50, 6, 1, 24, 6, 19, 6, 1, 3, 16, 21, 9, 1, 26, 16, 22, 15, 8, 1, 24, 9, 6, 15, 1, 36, 1, 7, 10, 19, 20, 21, 1, 20, 2, 24, 1, 26, 16, 22, 1, 1, 0]\n",
      "\n",
      "decode ids: ['\\n', '#', '#', '#', ' ', '*', '*', 'L', 'o', 'v', 'e', ' ', 'S', 't', 'o', 'r', 'y', '*', '*', ' ', ' ', '\\n', '*', 'B', 'y', ' ', 'T', 'a', 'y', 'l', 'o', 'r', ' ', 'S', 'w', 'i', 'f', 't', '*', ' ', ' ', '\\n\\n', '*', '*', '[', 'V', 'e', 'r', 's', 'e', ' ', '1', ']', '*', '*', ' ', ' ', '\\n', 'W', 'e', ' ', 'w', 'e', 'r', 'e', ' ', 'b', 'o', 't', 'h', ' ', 'y', 'o', 'u', 'n', 'g', ' ', 'w', 'h', 'e', 'n', ' ', 'I', ' ', 'f', 'i', 'r', 's', 't', ' ', 's', 'a', 'w', ' ', 'y', 'o', 'u', ' ', ' ', '\\n']\n",
      "\n",
      "解码文本 \n",
      "### **Love Story**  \n",
      "*By Taylor Swift*  \n",
      "\n",
      "**[Verse 1]**  \n",
      "We were both young when I first saw you  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 英文文本\n",
    "\n",
    "english_text =\"\"\"\n",
    "### **Love Story**  \n",
    "*By Taylor Swift*  \n",
    "\n",
    "**[Verse 1]**  \n",
    "We were both young when I first saw you  \n",
    "I close my eyes and the flashback starts  \n",
    "I'm standing there on a balcony in summer air  \n",
    "\n",
    "See the lights, see the party, the ball gowns  \n",
    "See you make your way through the crowd  \n",
    "And say, \"Hello, little did I know\"  \n",
    "\n",
    "**[Pre-Chorus]**  \n",
    "That you were Romeo, you were throwing pebbles  \n",
    "And my daddy said, \"Stay away from Juliet\"  \n",
    "And I was crying on the staircase  \n",
    "Begging you, \"Please don't go\"  \n",
    "\n",
    "**[Chorus]**  \n",
    "And I said,  \n",
    "\"Romeo, take me somewhere we can be alone  \n",
    "I'll be waiting, all there's left to do is run  \n",
    "You'll be the prince and I'll be the princess  \n",
    "It's a love story, baby, just say 'Yes'\"  \n",
    "\n",
    "**[Verse 2]**  \n",
    "So I sneak out to the garden to see you  \n",
    "We keep quiet, 'cause we're dead if they knew  \n",
    "So close your eyes, escape this town for a little while  \n",
    "\n",
    "**[Pre-Chorus]**  \n",
    "'Cause you were Romeo, I was a scarlet letter  \n",
    "And my daddy said, \"Stay away from Juliet\"  \n",
    "But you were everything to me  \n",
    "I was begging you, \"Please don't go\"  \n",
    "\n",
    "**[Chorus]**  \n",
    "And I said,  \n",
    "\"Romeo, take me somewhere we can be alone  \n",
    "I'll be waiting, all there's left to do is run  \n",
    "You'll be the prince and I'll be the princess  \n",
    "It's a love story, baby, just say 'Yes'  \n",
    "Romeo, save me, they're trying to tell me how to feel  \n",
    "This love is difficult, but it's real  \n",
    "Don't be afraid, we'll make it out of this mess  \n",
    "It's a love story, baby, just say 'Yes'\"  \n",
    "\n",
    "**[Bridge]**  \n",
    "I got tired of waiting  \n",
    "Wondering if you were ever coming around  \n",
    "My faith in you was fading  \n",
    "When I met you on the outskirts of town  \n",
    "\n",
    "And I said,  \n",
    "\"Romeo, save me, I've been feeling so alone  \n",
    "I keep waiting for you, but you never come  \n",
    "Is this in my head? I don't know what to think\"  \n",
    "He knelt to the ground and pulled out a ring  \n",
    "\n",
    "**[Final Chorus]**  \n",
    "And said,  \n",
    "\"Marry me, Juliet, you'll never have to be alone  \n",
    "I love you and that's all I really know  \n",
    "I talked to your dad, go pick out a white dress  \n",
    "It's a love story, baby, just say 'Yes'\"  \n",
    "\n",
    "**[Outro]**  \n",
    "Oh, oh, oh  \n",
    "Oh, oh, oh  \n",
    "'Cause we were both young when I first saw you  \n",
    "\"\"\"\n",
    "\n",
    "tokens, token_ids = encode_anything(vocab, pattern, english_text) # 新的编码函数\n",
    "print('\\n原文本:', english_text[:100])\n",
    "print('\\n分词:',tokens[:100])\n",
    "print('\\ntoken ids:',token_ids[:100])\n",
    "decode_token = decode(vocab_reverse, token_ids[:100])\n",
    "print('\\ndecode ids:',decode_token[:100])\n",
    "print('\\n解码文本',''.join(decode_token[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05160c8-4fd6-43e2-9899-347a7f4950b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原文本: \n",
      "\n",
      "[LeetCode 53. Maximum Subarray]\n",
      "\n",
      "from typing import List\n",
      "\n",
      "class Solution:\n",
      "    def maxSubArray(self\n",
      "\n",
      "分词: ['\\n\\n', '[', 'LeetCode', ' ', '5', '3', '.', ' ', 'Maximum', ' ', 'Subarray', ']', '\\n\\nfrom', ' ', 'typing', ' ', 'import', ' ', 'List\\n\\nclass', ' ', 'Solution', ':', '\\n', ' ', ' ', ' ', ' ', 'def', ' ', 'maxSubArray', '(', 'self', ',', ' ', 'nums', ':', ' ', 'List', '[', 'int', ']', ')', ' ', '-', '>', ' ', 'int', ':', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '#', ' ', '当', '前', '以', ' ', 'nums', '[', 'i', ']', ' ', '结', '尾', '的', '子', '数', '组', '最', '大', '和', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'cur', '_', 'max', ' ', '=', ' ', 'nums', '[', '0', ']', '\\n', ' ', ' ', ' ']\n",
      "\n",
      "token ids: [117, 108, 39, 6, 6, 21, 30, 16, 5, 6, 1, 59, 57, 99, 1, 40, 2, 25, 10, 14, 22, 14, 1, 46, 22, 3, 2, 19, 19, 2, 26, 109, 0, 0, 7, 19, 16, 14, 1, 21, 26, 17, 10, 15, 8, 1, 10, 14, 17, 16, 19, 21, 1, 39, 10, 20, 21, 0, 0, 4, 13, 2, 20, 20, 1, 46, 16, 13, 22, 21, 10, 16, 15, 101, 0, 1, 1, 1, 1, 5, 6, 7, 1, 14, 2, 25, 46, 22, 3, 28, 19, 19, 2, 26, 93, 137, 97, 1, 15, 22]\n",
      "\n",
      "decode ids: ['\\n\\n', '[', 'L', 'e', 'e', 't', 'C', 'o', 'd', 'e', ' ', '5', '3', '.', ' ', 'M', 'a', 'x', 'i', 'm', 'u', 'm', ' ', 'S', 'u', 'b', 'a', 'r', 'r', 'a', 'y', ']', '\\n', '\\n', 'f', 'r', 'o', 'm', ' ', 't', 'y', 'p', 'i', 'n', 'g', ' ', 'i', 'm', 'p', 'o', 'r', 't', ' ', 'L', 'i', 's', 't', '\\n', '\\n', 'c', 'l', 'a', 's', 's', ' ', 'S', 'o', 'l', 'u', 't', 'i', 'o', 'n', ':', '\\n', ' ', ' ', ' ', ' ', 'd', 'e', 'f', ' ', 'm', 'a', 'x', 'S', 'u', 'b', 'A', 'r', 'r', 'a', 'y', '(', 'self', ',', ' ', 'n', 'u']\n",
      "\n",
      "解码文本 \n",
      "\n",
      "[LeetCode 53. Maximum Subarray]\n",
      "\n",
      "from typing import List\n",
      "\n",
      "class Solution:\n",
      "    def maxSubArray(self, nums: List[int]) -> int:\n",
      "        # <UNK><UNK><UNK> nums[i] <UNK><UNK>的<UNK>数<UNK><UNK>大和\n",
      "        cur_max = nums[0]\n",
      "        # <UNK><UNK><UNK>大和\n",
      "        global_max = nums[0]\n",
      "\n",
      "        for i in range(1, len(nums)):\n",
      "            # <UNK><UNK><UNK><UNK><UNK><UNK>的<UNK>数<UNK><UNK><UNK>，<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "            cur_max = max(nums[i], cur_max + nums[i])\n",
      "            # <UNK><UNK><UNK><UNK><UNK>大<UNK>\n",
      "            global_max = max(global_max, cur_max)\n",
      "        return global_max\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 分词实例2:\n",
    "\n",
    "code_text = \"\"\"\n",
    "\n",
    "[LeetCode 53. Maximum Subarray]\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class Solution:\n",
    "    def maxSubArray(self, nums: List[int]) -> int:\n",
    "        # 当前以 nums[i] 结尾的子数组最大和\n",
    "        cur_max = nums[0]\n",
    "        # 全局最大和\n",
    "        global_max = nums[0]\n",
    "\n",
    "        for i in range(1, len(nums)):\n",
    "            # 要么接在前面的子数组后面，要么从当前位置重新开始\n",
    "            cur_max = max(nums[i], cur_max + nums[i])\n",
    "            # 更新全局最大值\n",
    "            global_max = max(global_max, cur_max)\n",
    "        return global_max\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokens, token_ids = encode_anything(vocab, pattern, code_text) # 新的编码函数\n",
    "print('\\n原文本:', code_text[:100])\n",
    "print('\\n分词:',tokens[:100])\n",
    "print('\\ntoken ids:',token_ids[:100])\n",
    "decode_token = decode(vocab_reverse, token_ids)\n",
    "print('\\ndecode ids:',decode_token[:100])\n",
    "print('\\n解码文本',''.join(decode_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392dbb3-44c6-421b-99f9-4c4e9c153cfd",
   "metadata": {},
   "source": [
    "## Batch Tokenize\n",
    "\n",
    "- pre-process\n",
    "- padding\n",
    "- truction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cb086846-ad2c-4813-b71b-00465f5a4f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sos token:  64 \n",
      "eos token:  65 \n",
      "pad token:  67 \n",
      "unk token:  66\n"
     ]
    }
   ],
   "source": [
    "# special_tokens = ['<SOS>', '<EOS>', '<PAD>', '<UNK>']\n",
    "# @dataclass\n",
    "class SpecialToken:\n",
    "    def __init__(self,):\n",
    "        self.sos_token = '<SOS>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "\n",
    "special_token = SpecialToken()\n",
    "\n",
    "\n",
    "print('\\nsos token: ', vocab[special_token.sos_token], \n",
    "      '\\neos token: ', vocab[special_token.eos_token], \n",
    "      '\\npad token: ', vocab[special_token.pad_token], \n",
    "      '\\nunk token: ', vocab[special_token.unk_token], )\n",
    "\n",
    "# example\n",
    "texts = [\"Large Language Models (LLMs) are advanced AI systems trained on vast datasets to understand, generate, and manipulate human-like text. \", \n",
    "         \"I have 12 apples!\", \n",
    "         \"large language model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8b7d2-6eea-4ebe-8bb0-4d257e46adfd",
   "metadata": {},
   "source": [
    "### 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e2df0717-a1d5-4fa5-b830-f309296fbd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>Large Language Models (LLMs) are advanced AI systems trained on vast datasets to understand, generate, and manipulate human-like text. <EOS>', '<SOS>I have 12 apples!<EOS>', '<SOS>large language model<EOS>']\n",
      "[64, 36, 1, 9, 2, 23, 6, 1, 55, 56, 1, 2, 17, 17, 13, 6, 20, 85, 65]\n",
      "[64, 36, 1, 9, 2, 23, 6, 1, 55, 56, 1, 2, 17, 17, 13, 6, 20, 85, 65]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "texts = [\"Large Language Models (LLMs) are advanced AI systems trained on vast datasets to understand, generate, and manipulate human-like text. \", \n",
    "         \"I have 12 apples!\", \n",
    "         \"large language model\"]\n",
    "\n",
    "# 预处理有两种方式，一种是在 text 层面加 special token， 一种是在 token 层面加 special token id\n",
    "# 1. pre process text\n",
    "def text_pre_process(text, sos = '', eos = ''):\n",
    "    return sos + text + eos\n",
    "\n",
    "texts_pre = [ text_pre_process(text, special_token.sos_token, special_token.eos_token) for text in texts ]\n",
    "token_ids_1 = [ encode_anything(vocab, pattern, text)[1] for text in texts_pre ]\n",
    "print(texts_pre)\n",
    "print(token_ids_1[1])\n",
    "\n",
    "# 2. pre process token\n",
    "def token_pre_process(tokens = tokens, sos = None, eos = None):\n",
    "    if sos is not None:\n",
    "        tokens = [sos] + tokens\n",
    "    if eos is not None:\n",
    "        tokens = tokens + [eos] \n",
    "    return tokens\n",
    "\n",
    "token_ids = [ encode_anything(vocab, pattern, text)[1] for text in texts ]\n",
    "# token_pre = [ token_pre_process(token, \n",
    "#                                sos = vocab[special_token.sos_token],\n",
    "#                                eos = vocab[special_token.eos_token]) for token in token_ids ]\n",
    "token_ids_2 = []\n",
    "for tokens in token_ids:\n",
    "    tmp = token_pre_process(tokens = tokens, sos = vocab[special_token.sos_token], eos = vocab[special_token.eos_token])\n",
    "    token_ids_2.append(tmp)\n",
    "print(token_ids_2[1])\n",
    "\n",
    "input_ids = token_ids_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3d76b-dabd-4014-ba95-8a05fb62e0be",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f774d6fc-2206-48a8-8c4e-4c1e2b1960b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 64, 117,   1, 118,   1, 119,   1,  93, 120,  94,   1,   2,  19,   6,\n",
      "           1,   2,   5,  23,   2,  15,   4,   6,   5,   1,  28,  36,   1,  20,\n",
      "          26,  20,  21,   6,  14,  20,   1,  21,  19,   2,  10,  15,   6,   5,\n",
      "           1,  16,  15,   1,  23,   2,  20,  21,   1, 170,   1, 154,   1,  22,\n",
      "          15,   5,   6,  19,  20,  21,   2,  15,   5,  97,   1,   8,   6,  15,\n",
      "           6,  19,   2,  21,   6,  97,   1, 126,   1,  14,   2,  15,  10,  17,\n",
      "          22,  13,   2,  21,   6,   1,   9,  22,  14,   2,  15,  98,  13,  10,\n",
      "          12,   6,   1,  21,   6,  25,  21,  99,   1,  65],\n",
      "        [ 67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  64,  36,   1,   9,   2,  23,   6,   1,  55,\n",
      "          56,   1,   2,  17,  17,  13,   6,  20,  85,  65],\n",
      "        [ 67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  64,  13,   2,  19,   8,   6,   1,  13,\n",
      "           2,  15,   8,  22,   2,   8,   6,   1, 244,  65]])\n",
      "tensor([[ 64, 117,   1, 118,   1, 119,   1,  93, 120,  94,   1,   2,  19,   6,\n",
      "           1,   2,   5,  23,   2,  15,   4,   6,   5,   1,  28,  36,   1,  20,\n",
      "          26,  20,  21,   6,  14,  20,   1,  21,  19,   2,  10,  15,   6,   5,\n",
      "           1,  16,  15,   1,  23,   2,  20,  21,   1, 170,   1, 154,   1,  22,\n",
      "          15,   5,   6,  19,  20,  21,   2,  15,   5,  97,   1,   8,   6,  15,\n",
      "           6,  19,   2,  21,   6,  97,   1, 126,   1,  14,   2,  15,  10,  17,\n",
      "          22,  13,   2,  21,   6,   1,   9,  22,  14,   2,  15,  98,  13,  10,\n",
      "          12,   6,   1,  21,   6,  25,  21,  99,   1,  65],\n",
      "        [ 64,  36,   1,   9,   2,  23,   6,   1,  55,  56,   1,   2,  17,  17,\n",
      "          13,   6,  20,  85,  65,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67],\n",
      "        [ 64,  13,   2,  19,   8,   6,   1,  13,   2,  15,   8,  22,   2,   8,\n",
      "           6,   1, 244,  65,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67,  67,  67,  67,  67,  67,  67]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 使用 longest padding 策略, 默认使用 right-padding\n",
    "# padding 后每条数据都是 等长的，适合使用 tensor 来存储\n",
    "# padding 是在 token 层面加的, text 加没有意义。\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "def padding(input_ids, pad_token_id = None, padding_side = 'RIGHT'):\n",
    "    if pad_token_id is None:\n",
    "        return\n",
    "    tokens_lens = [ len(ids) for ids in input_ids]\n",
    "    tokens_lens = torch.tensor(tokens_lens, dtype = torch.long)\n",
    "    tokens_max_len = torch.max(tokens_lens)\n",
    "    paddding_input_ids = torch.ones(len(input_ids), tokens_max_len, dtype = torch.long) * pad_token_id\n",
    "    \n",
    "    if padding_side == 'RIGHT':\n",
    "        for i in range(len(input_ids)):\n",
    "            paddding_input_ids[i, :tokens_lens[i]] = torch.tensor(input_ids[i], dtype = torch.long)\n",
    "    else: # left padding\n",
    "        for i in range(len(input_ids)):\n",
    "            paddding_input_ids[i, -tokens_lens[i]:] = torch.tensor(input_ids[i], dtype = torch.long)\n",
    "    return paddding_input_ids\n",
    "    \n",
    "\n",
    "pad_input_ids = padding(input_ids,\n",
    "            pad_token_id = vocab[ special_token.pad_token ],\n",
    "            padding_side = 'Left')\n",
    "print(pad_input_ids)\n",
    "\n",
    "\n",
    "pad_input_ids = padding(input_ids,\n",
    "            pad_token_id = vocab[ special_token.pad_token ],\n",
    "            padding_side = 'RIGHT')\n",
    "print(pad_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f00d7f-98c4-406d-b920-4eeea5f806d1",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "1. 可以在 padding 后好的 token 序列上按照长度裁剪\n",
    "2. 可以在 padding 前进行裁剪\n",
    "\n",
    "padding 方向 和 裁剪方向一致如： left padding: [p, p, 2, 3], 裁剪: left padding [p, 2, 3]\n",
    "\n",
    "trucation 哪个方向更为合理 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d03778d6-bdea-4479-88ac-e62107fa23ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 64, 117,   1, 118,   1, 119,   1,  93, 120,  94,   1,   2,  19,   6,\n",
      "           1,   2,   5,  23,   2,  15,   4,   6,   5,   1,  28,  36,   1,  20,\n",
      "          26,  20,  21,   6],\n",
      "        [ 64,  36,   1,   9,   2,  23,   6,   1,  55,  56,   1,   2,  17,  17,\n",
      "          13,   6,  20,  85,  65,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67],\n",
      "        [ 64,  13,   2,  19,   8,   6,   1,  13,   2,  15,   8,  22,   2,   8,\n",
      "           6,   1, 244,  65,  67,  67,  67,  67,  67,  67,  67,  67,  67,  67,\n",
      "          67,  67,  67,  67]])\n"
     ]
    }
   ],
   "source": [
    "# 展示方式 2\n",
    "\n",
    "# max_len = 4, max_seq_len = 5\n",
    "#                  cut\n",
    "# seq1: 1, 1, 1, 1, | 1\n",
    "# seq2: 1, 1, \n",
    "# seq3: 1, 1, 1, \n",
    "\n",
    "def padding_max_length(input_ids, \n",
    "                       max_len = 32, \n",
    "                       pad_token_id = None, \n",
    "                       padding_side = 'RIGHT',\n",
    "                      truction_side = 'RIGHT'):\n",
    "    if pad_token_id is None:\n",
    "        return\n",
    "    tokens_lens = [ len(ids) for ids in input_ids]\n",
    "    tokens_lens = torch.tensor(tokens_lens, dtype = torch.long)\n",
    "    tokens_max_len = torch.max(tokens_lens)\n",
    "\n",
    "    if tokens_max_len > max_len:\n",
    "        tokens_max_len = max_len\n",
    "    if truction_side == 'RIGHT':\n",
    "        input_ids = [ ids[ : min(len(ids), tokens_max_len)] for ids in input_ids]\n",
    "    else:\n",
    "        input_ids = [ ids[ -min(len(ids), tokens_max_len) : ] for ids in input_ids]\n",
    "\n",
    "    \n",
    "    paddding_input_ids = torch.ones(len(input_ids), tokens_max_len, dtype = torch.long) * pad_token_id\n",
    "    if padding_side == 'RIGHT':\n",
    "        for i in range(len(input_ids)):\n",
    "            paddding_input_ids[i, : len(input_ids[i])] = torch.tensor(input_ids[i], dtype = torch.long)\n",
    "    else: # left padding\n",
    "        for i in range(len(input_ids)):\n",
    "            paddding_input_ids[i, -len(input_ids[i]):] = torch.tensor(input_ids[i], dtype = torch.long)\n",
    "\n",
    "    \n",
    "    return paddding_input_ids\n",
    "    \n",
    "\n",
    "pad_input_ids = padding_max_length(input_ids,\n",
    "            max_len =  32,\n",
    "            pad_token_id = vocab[ special_token.pad_token ],\n",
    "            padding_side = 'RIGHT',\n",
    "            truction_side = 'RIGHT')\n",
    "print(pad_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffcca3-a80a-4877-a685-1895edb86bb9",
   "metadata": {},
   "source": [
    "## Tokenizer IO\n",
    "\n",
    "1. 需要存储词表\n",
    "2. 需要存储特殊词元\n",
    "3. 需要定义一个 config 字典来管理\n",
    "4. 加载时将 字典 转成 数据类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a118aebe-4d3d-4fdc-80e0-d25828ce6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenizerBaseConfig(vocab_size=-1, class_name='TokenizerBase', sos_token='<SOS>', sos_token_id=-1, eos_token='<EOS>', eos_token_id=-1, pad_token='<PAD>', pad_token_id=-1, unk_token='<UNK>', unk_token_id=-1, pattern='')\n",
      "TokenizerBaseConfig(vocab_size=302, class_name='TokenizerBaseConfig', sos_token='<SOS>', sos_token_id=64, eos_token='<EOS>', eos_token_id=65, pad_token='<PAD>', pad_token_id=67, unk_token='<UNK>', unk_token_id=66, pattern='(?:<SOS>|<EOS>|<PAD>|<UNK>|\\n)|[，。！？；：“”‘’【】（）《》、!\"\\\\\\\\\\\\#\\\\\\\\\\\\$%\\\\\\\\\\\\&\\'\\\\\\\\\\\\(\\\\\\\\\\\\)\\\\\\\\\\\\*\\\\\\\\\\\\+,\\\\\\\\\\\\-\\\\\\\\\\\\./:;<=>\\\\\\\\\\\\?@\\\\\\\\\\\\[\\\\\\\\\\\\\\\\\\\\\\\\\\\\]\\\\\\\\\\\\^_`\\\\\\\\\\\\{\\\\\\\\\\\\|\\\\\\\\\\\\}\\\\\\\\\\\\~\\\\ ]|\\\\d|[\\\\u4e00-\\\\u9fa5]|[^，。！？；：“”‘’【】（）《》、!\"\\\\\\\\\\\\#\\\\\\\\\\\\$%\\\\\\\\\\\\&\\'\\\\\\\\\\\\(\\\\\\\\\\\\)\\\\\\\\\\\\*\\\\\\\\\\\\+,\\\\\\\\\\\\-\\\\\\\\\\\\./:;<=>\\\\\\\\\\\\?@\\\\\\\\\\\\[\\\\\\\\\\\\\\\\\\\\\\\\\\\\]\\\\\\\\\\\\^_`\\\\\\\\\\\\{\\\\\\\\\\\\|\\\\\\\\\\\\}\\\\\\\\\\\\~\\\\ \\\\d\\\\u4e00-\\\\u9fa5<>]+')\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class TokenizerBaseConfig:\n",
    "    vocab_size: int = -1\n",
    "    class_name: str = 'TokenizerBase'\n",
    "    sos_token: str = '<SOS>'\n",
    "    sos_token_id: int = -1\n",
    "    eos_token: str = '<EOS>'\n",
    "    eos_token_id: int = -1\n",
    "    pad_token: str = '<PAD>'\n",
    "    pad_token_id: int = -1\n",
    "    unk_token: str = '<UNK>'\n",
    "    unk_token_id: int = -1\n",
    "    pattern: str = ''\n",
    "    \n",
    "\n",
    "config = TokenizerBaseConfig()\n",
    "print(config)\n",
    "\n",
    "config_dict = {\n",
    "    'vocab_size' : len(vocab),\n",
    "    'class_name' : 'TokenizerBaseConfig',\n",
    "    'sos_token' : special_token.sos_token,\n",
    "    'sos_token_id' : vocab[special_token.sos_token],\n",
    "    'eos_token' : special_token.eos_token,\n",
    "    'eos_token_id' : vocab[special_token.eos_token],\n",
    "    'pad_token' : special_token.pad_token,\n",
    "    'pad_token_id'  : vocab[special_token.pad_token],\n",
    "    'unk_token' : special_token.unk_token,\n",
    "    'unk_token_id' : vocab[special_token.unk_token],\n",
    "    'pattern' : pattern,\n",
    "}\n",
    "# print(config_dict)\n",
    "\n",
    "config = TokenizerBaseConfig(**config_dict)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e6945-a66c-4e7c-9f5c-3383e6fafd2d",
   "metadata": {},
   "source": [
    "### tokenizer save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7c660dc0-d6fc-457b-8ec0-fb1df052473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: output: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "55f87230-36dc-461c-9381-6443447d909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储函数：存 vocab, config\n",
    "from dataclasses import asdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_dict_to_json(filepath, data):\n",
    "    \"\"\"将字典保存为 JSON 文件\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=True, indent=4)\n",
    "    print(f\"字典已保存为 JSON 文件: {filepath}\")\n",
    "\n",
    "def save_pretrained(directory, vocab : Dict[str, int], config): \n",
    "    \"\"\"\n",
    "    保存 tokenizer, 包含词表, 分词规则, config\n",
    "    config 保存 分词器 类名, 分词器保存规则 \n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"目录 '{directory}' 已创建\")\n",
    "    else:\n",
    "        print(f\"目录 '{directory}' 已存在\")\n",
    "        # return False\n",
    "\n",
    "    vocab_path = os.path.join(directory, 'vocab.json')\n",
    "    config_path = os.path.join(directory, 'config.json')\n",
    "\n",
    "    config_dict = asdict(config)\n",
    "\n",
    "    save_dict_to_json(config_path, config_dict)\n",
    "    save_dict_to_json(vocab_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "301d9069-2921-4b61-9fcd-58454c59845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录 './output/tokenizer' 已存在\n",
      "字典已保存为 JSON 文件: ./output/tokenizer/config.json\n",
      "字典已保存为 JSON 文件: ./output/tokenizer/vocab.json\n"
     ]
    }
   ],
   "source": [
    "save_pretrained('./output/tokenizer', vocab, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bd3b38ef-8d92-478d-921b-af11617bddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"vocab_size\": 302,\n",
      "    \"class_name\": \"TokenizerBaseConfig\",\n",
      "    \"sos_token\": \"<SOS>\",\n",
      "    \"sos_token_id\": 64,\n",
      "    \"eos_token\": \"<EOS>\",\n",
      "    \"eos_token_id\": 65,\n",
      "    \"pad_token\": \"<PAD>\",\n",
      "    \"pad_token_id\": 67,\n",
      "    \"unk_token\": \"<UNK>\",\n",
      "    \"unk_token_id\": 66,\n",
      "    \"pattern\": \"(?:<SOS>|<EOS>|<PAD>|<UNK>|\\n)|[\\uff0c\\u3002\\uff01\\uff1f\\uff1b\\uff1a\\u201c\\u201d\\u2018\\u2019\\u3010\\u3011\\uff08\\uff09\\u300a\\u300b\\u3001!\\\"\\\\\\\\\\\\#\\\\\\\\\\\\$%\\\\\\\\\\\\&'\\\\\\\\\\\\(\\\\\\\\\\\\)\\\\\\\\\\\\*\\\\\\\\\\\\+,\\\\\\\\\\\\-\\\\\\\\\\\\./:;<=>\\\\\\\\\\\\?@\\\\\\\\\\\\[\\\\\\\\\\\\\\\\\\\\\\\\\\\\]\\\\\\\\\\\\^_`\\\\\\\\\\\\{\\\\\\\\\\\\|\\\\\\\\\\\\}\\\\\\\\\\\\~\\\\ ]|\\\\d|[\\\\u4e00-\\\\u9fa5]|[^\\uff0c\\u3002\\uff01\\uff1f\\uff1b\\uff1a\\u201c\\u201d\\u2018\\u2019\\u3010\\u3011\\uff08\\uff09\\u300a\\u300b\\u3001!\\\"\\\\\\\\\\\\#\\\\\\\\\\\\$%\\\\\\\\\\\\&'\\\\\\\\\\\\(\\\\\\\\\\\\)\\\\\\\\\\\\*\\\\\\\\\\\\+,\\\\\\\\\\\\-\\\\\\\\\\\\./:;<=>\\\\\\\\\\\\?@\\\\\\\\\\\\[\\\\\\\\\\\\\\\\\\\\\\\\\\\\]\\\\\\\\\\\\^_`\\\\\\\\\\\\{\\\\\\\\\\\\|\\\\\\\\\\\\}\\\\\\\\\\\\~\\\\ \\\\d\\\\u4e00-\\\\u9fa5<>]+\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ./output/tokenizer/config.json\n",
    "# !cat ./output/tokenizer/vocab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99497c-c9d8-4fd4-a6b1-e8a2af8f25a2",
   "metadata": {},
   "source": [
    "### tokenizer load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c8f00da2-bfca-4226-bf32-c9e8d91547bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(directory):\n",
    "    \n",
    "    vocab_path = os.path.join(directory, 'vocab.json')\n",
    "    config_path = os.path.join(directory, 'config.json')\n",
    "    \n",
    "    if os.path.isfile(config_path):\n",
    "        with open(config_path, encoding='utf-8') as f:\n",
    "            config = json.load(f) # loads 返回 dict\n",
    "            print('加载成功：')\n",
    "    else:\n",
    "        print(f'[错误] 文件不存在：{config_path}')\n",
    "\n",
    "        \n",
    "    if 'class_name' in config:\n",
    "        cls = globals()[config['class_name']]  # 获取类对象\n",
    "        config = cls(**config)\n",
    "    else:\n",
    "        print('not specified tokenizer class name')\n",
    "\n",
    "    \n",
    "    if os.path.isfile(vocab_path):\n",
    "        with open(vocab_path, encoding='utf-8') as f:\n",
    "            vocab = json.load(f) # loads 返回 dict\n",
    "            print('加载成功：')\n",
    "    else:\n",
    "        print(f'[错误] 文件不存在：{vocab_path}')\n",
    "\n",
    "    return config, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4bd9dc2e-9e7e-451d-b59c-348a9f0ca689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载成功：\n",
      "加载成功：\n",
      "TokenizerBaseConfig(vocab_size=302, class_name='TokenizerBaseConfig', sos_token='<SOS>', sos_token_id=64, eos_token='<EOS>', eos_token_id=65, pad_token='<PAD>', pad_token_id=67, unk_token='<UNK>', unk_token_id=66, pattern='(?:<SOS>|<EOS>|<PAD>|<UNK>|\\n)|[，。！？；：“”‘’【】（）《》、!\"\\\\\\\\\\\\#\\\\\\\\\\\\$%\\\\\\\\\\\\&\\'\\\\\\\\\\\\(\\\\\\\\\\\\)\\\\\\\\\\\\*\\\\\\\\\\\\+,\\\\\\\\\\\\-\\\\\\\\\\\\./:;<=>\\\\\\\\\\\\?@\\\\\\\\\\\\[\\\\\\\\\\\\\\\\\\\\\\\\\\\\]\\\\\\\\\\\\^_`\\\\\\\\\\\\{\\\\\\\\\\\\|\\\\\\\\\\\\}\\\\\\\\\\\\~\\\\ ]|\\\\d|[\\\\u4e00-\\\\u9fa5]|[^，。！？；：“”‘’【】（）《》、!\"\\\\\\\\\\\\#\\\\\\\\\\\\$%\\\\\\\\\\\\&\\'\\\\\\\\\\\\(\\\\\\\\\\\\)\\\\\\\\\\\\*\\\\\\\\\\\\+,\\\\\\\\\\\\-\\\\\\\\\\\\./:;<=>\\\\\\\\\\\\?@\\\\\\\\\\\\[\\\\\\\\\\\\\\\\\\\\\\\\\\\\]\\\\\\\\\\\\^_`\\\\\\\\\\\\{\\\\\\\\\\\\|\\\\\\\\\\\\}\\\\\\\\\\\\~\\\\ \\\\d\\\\u4e00-\\\\u9fa5<>]+')\n",
      "{'\\n': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '0': 54, '1': 55, '2': 56, '3': 57, '4': 58, '5': 59, '6': 60, '7': 61, '8': 62, '9': 63, '<SOS>': 64, '<EOS>': 65, '<UNK>': 66, '<PAD>': 67, '，': 68, '。': 69, '！': 70, '？': 71, '；': 72, '：': 73, '“': 74, '”': 75, '‘': 76, '’': 77, '【': 78, '】': 79, '（': 80, '）': 81, '《': 82, '》': 83, '、': 84, '!': 85, '\"': 86, '\\\\': 87, '#': 88, '$': 89, '%': 90, '&': 91, \"'\": 92, '(': 93, ')': 94, '*': 95, '+': 96, ',': 97, '-': 98, '.': 99, '/': 100, ':': 101, ';': 102, '<': 103, '=': 104, '>': 105, '?': 106, '@': 107, '[': 108, ']': 109, '^': 110, '_': 111, '`': 112, '{': 113, '|': 114, '}': 115, '~': 116, 'Large': 117, 'Language': 118, 'Models': 119, 'LLMs': 120, 'Modern': 121, 'such': 122, 'as': 123, 'OpenAI': 124, 'GPT': 125, 'and': 126, 'Meta': 127, 'LLaMA': 128, 'leverage': 129, 'transformer': 130, 'architectures': 131, 'Vaswani': 132, 'et': 133, 'al': 134, 'with': 135, 'self': 136, 'attention': 137, 'mechanisms': 138, '\\x08egin': 139, 'equation': 140, '\\text': 141, 'Attention': 142, 'softmax': 143, 'left': 144, '\\x0crac': 145, 'QK': 146, 'sqrt': 147, '\\right': 148, 'V\\n': 149, 'end': 150, 'These': 151, 'models': 152, 'scale': 153, 'to': 154, 'hundreds': 155, 'of': 156, 'billions': 157, 'parameters': 158, 'enabling': 159, 'state': 160, 'the': 161, 'art': 162, 'performance': 163, 'in': 164, 'NLP': 165, 'tasks': 166, 'Training': 167, 'requires': 168, 'massive': 169, 'datasets': 170, 'tokens': 171, 'distributed': 172, 'computing': 173, 'frameworks': 174, '大': 175, '语': 176, '言': 177, '模': 178, '型': 179, 'LLM': 180, '技': 181, '术': 182, '现': 183, '代': 184, '如': 185, '的': 186, '和': 187, '基': 188, '于': 189, 'Transformer': 190, '架': 191, '构': 192, '等': 193, '其': 194, '自': 195, '注': 196, '意': 197, '力': 198, '机': 199, '制': 200, '公': 201, '式': 202, '为': 203, '此': 204, '类': 205, '参': 206, '数': 207, '量': 208, '已': 209, '达': 210, '千': 211, '亿': 212, '约': 213, '万': 214, '依': 215, '赖': 216, '超': 217, '规': 218, '训': 219, '练': 220, '据': 221, '词': 222, '元': 223, '分': 224, '布': 225, '计': 226, '算': 227, '框': 228, '推': 229, '动': 230, '任': 231, '务': 232, '性': 233, '能': 234, '突': 235, '破': 236, 'Scaling': 237, 'Laws': 238, 'Trends': 239, 'Empirical': 240, 'studies': 241, 'Kaplan': 242, 'show': 243, 'model': 244, 'scales': 245, 'power': 246, 'law': 247, 'compute': 248, 'budget': 249, 'dataset': 250, 'size': 251, '\\x07pprox': 252, '\\x07lpha': 253, 'infty\\n': 254, 'For': 255, 'example': 256, 'Google': 257, 'PaLM': 258, 'params': 259, 'achieved': 260, 'multilingual': 261, 'accuracy': 262, 'while': 263, 'smaller': 264, 'Mistral': 265, 'optimize': 266, 'efficiency': 267, 'via': 268, 'sparse': 269, '扩': 270, '展': 271, '定': 272, '律': 273, '实': 274, '证': 275, '研': 276, '究': 277, '表': 278, '明': 279, '随': 280, '呈': 281, '幂': 282, '增': 283, '长': 284, '例': 285, '谷': 286, '歌': 287, '了': 288, '多': 289, '准': 290, '确': 291, '率': 292, '而': 293, '小': 294, '通': 295, '过': 296, '稀': 297, '疏': 298, '提': 299, '升': 300, '效': 301}\n"
     ]
    }
   ],
   "source": [
    "load_path = './output/tokenizer'\n",
    "config_load, vocab_load =load_tokenizer(load_path)\n",
    "print(config_load) # 数据类对象\n",
    "print(vocab_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f4410-960b-4b74-bbc5-6fdbc5fe4c40",
   "metadata": {},
   "source": [
    "## Tokenizer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "bca3e365-3e54-43a6-886c-555e43dfd012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Tuple, Union, str\n",
    "# from abc import ABC, abstractmethod\n",
    "\n",
    "# class TokenizerBase(ABC):\n",
    "#     # @abstractmethod\n",
    "#     def __init__(self):\n",
    "#         self.vocab : Dict[str, int] = {}\n",
    "#         self.vocab_reverse : Dict[int, str] = {}\n",
    "#         self.vocab_size : int = 0\n",
    "#         self.special_token : Dict[str, int] = {}\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def init_vocab(self, vocab: Dict[str, int] ) : -> None\n",
    "#         \"\"\"\n",
    "#         初始化词表, 可以用现成的字典, 也可以默认使用基础字符来创建\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     @abstractmethod\n",
    "#     def train(self, text: Union[str, List[str])): -> None\n",
    "#         \"\"\"\n",
    "#         输入语料\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "        \n",
    "\n",
    "#     @abstractmethod\n",
    "#     def add_special_token(self, token: Dict[str, str]): -> None\n",
    "#         \"\"\"\n",
    "#         添加特殊 token，存入 特殊的 tokenizer 表中\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def encode(self, \n",
    "#                input_list: List[str],\n",
    "#                padding = False : str,\n",
    "#                padding_side = 'right' : str, \n",
    "#                max_length = 'long' : Union[int, str] \n",
    "#                add_bos_token = False : bool,\n",
    "#                add_eos_token = False : bool,\n",
    "#                add_pad_token = False : bool,\n",
    "#                return_type = None : str, # pt: pytorch tensor\n",
    "#                 ): -> Union[torch.tensor, List[List[int]]]\n",
    "#         \"\"\"\n",
    "#         批量编码\n",
    "#         input: [\"I have 12 apples!\"] \n",
    "#         output: 9 numbers list  # -> ['I', ' ' ,'have', ' ', '1', '2', ' ', 'apple', '\"']\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def decode(self, token_ids : list[list[int]],\n",
    "#                     skip_special_token = True : bool,\n",
    "#                        return_string = True : bool\n",
    "#               ): -> Union[List[str], List[List[str]]]\n",
    "#         \"\"\"\n",
    "#         批量解码\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "\n",
    "    \n",
    "#     @abstractmethod\n",
    "#     def from_pretrained(self, filepath = './tokenizer' :str ): -> None\n",
    "#         pass\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def save_pretrained(self, filepath = './tokenizer' :str ): -> None\n",
    "#         pass\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def chat_template(self, \n",
    "#                       prompt = None : Union[str, List[str]), \n",
    "#                       response = None : Union[str, List[str]), \n",
    "#                         messages = None : List[Dict[str, Any]], \n",
    "#                       tokenize = False : bool,\n",
    "#                       add_response_prompt=  False : bool,): -> Union[str, List[str], List[int], List[List[int]]]\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699ef261-f74a-4544-94f7-c8eafb7e27d9",
   "metadata": {},
   "source": [
    "# Transformer \n",
    "\n",
    "逐步实现 Transformer 完整架构：\n",
    "\n",
    "1. 实现主体网络, 用 embedding 层 和 Linear 层代替关键组件\n",
    "2. 实现输入层\n",
    "3. 实现 encoder\n",
    "4. 实现 decoder\n",
    "5. 实现 输出层\n",
    "6. 数据集制作\n",
    "7. 训练代码, 收敛\n",
    "8. 推理代码\n",
    "9. 模型保存\n",
    "10. 模型加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c51120-3e49-4ac3-bd6e-d8c2ea9db21b",
   "metadata": {},
   "source": [
    "## Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85dfc7be-4cdf-41af-96f6-a1589cee058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug \n",
    "\n",
    "dim = 512 \n",
    "num_layers = 6\n",
    "heads = 8\n",
    "\n",
    "batch_size = 2\n",
    "src_len = 256\n",
    "trg_len = 128\n",
    "max_len = 512\n",
    "\n",
    "src_vocab_size = 100\n",
    "trg_vocab_size = 200\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "PAD_TOKEN_ID = 0\n",
    "SOS_TOKEN_ID = 1\n",
    "EOS_TOKEN_ID = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232684b8-021b-4368-b597-444ed1ac4afa",
   "metadata": {},
   "source": [
    "## Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4aa675-acde-4120-82ad-21a0479bff85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11efb8eb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84145898-4c7c-480d-91b2-56cb64c41758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBasic(\n",
      "  (encoder_input): Embedding(100, 512)\n",
      "  (encoder): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (decoder_input): Embedding(200, 512)\n",
      "  (decoder): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n",
      "encode input shape:  torch.Size([2, 256])\n",
      "decode output shape:  torch.Size([2, 128])\n",
      "encoder output:\t torch.Size([2, 256, 512])\n",
      "decoder output:\t torch.Size([2, 128, 512])\n",
      "transformer output:\t torch.Size([2, 128, 200])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBasic(nn.Module):\n",
    "    \"\"\"\n",
    "    仅通过 Embedding 和 Lienar 实现 Transformer 计算逻辑\n",
    "    输入:[bs, src_seq_len]\n",
    "    输出:[bs, trg_seq_len, vocab_size]\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size = 100, trg_vocab_size = 200, dim = 512, num_layers = 6, heads = 8, max_len = 512):\n",
    "        super().__init__()\n",
    "        self.encoder_input = nn.Embedding(src_vocab_size, dim)\n",
    "        self.encoder = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.decoder_input = nn.Embedding(trg_vocab_size, dim)\n",
    "        self.decoder = nn.Linear(dim, dim)\n",
    "\n",
    "        self.output_layer = nn.Linear(dim, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src_ids, trg_ids, src_mask = None, trg_mask = None, src_trg_mask = None):\n",
    "        X = self.encoder_input(src_ids)\n",
    "        X_src = self.encoder(X)\n",
    "        print('encoder output:\\t', X_src.shape)\n",
    "\n",
    "        Y = self.decoder_input(trg_ids)\n",
    "        Y = self.decoder(Y) + X_src.mean(dim = 1, keepdim = True)\n",
    "        print('decoder output:\\t', Y.shape)\n",
    "\n",
    "        logits = self.output_layer(Y) \n",
    "        prob = F.softmax(logits, dim = -1)\n",
    "        \n",
    "        return logits, prob\n",
    "    \n",
    "model = TransformerBasic()\n",
    "print(model)\n",
    "\n",
    "\n",
    "src_ids = torch.randint(src_vocab_size, (batch_size, src_len))\n",
    "trg_ids = torch.randint(trg_vocab_size, (batch_size, trg_len))\n",
    "print('encode input shape: ', src_ids.shape)\n",
    "print('decode output shape: ', trg_ids.shape)\n",
    "\n",
    "logits, _ = model(src_ids, trg_ids)\n",
    "print('transformer output:\\t', logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94280d14-dbc7-440f-875c-e7c456af44b1",
   "metadata": {},
   "source": [
    "- Decoder 输入 `bs x trg_len` 与 输出 ` bs x trg_len x trg_vocab_size`, 一个序列输出 `trg_len` 个 概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f06813-d86c-4c28-b533-6a352c02bf0f",
   "metadata": {},
   "source": [
    "## Transformer Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a4925f-231b-4101-93eb-78252d153c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2],\n",
       "        [4, 4, 4],\n",
       "        [6, 6, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, 2)\n",
    "a = torch.tensor([1,2,3]) # a is row\n",
    "b = torch.tensor([2,2,2]) # b is col\n",
    "torch.outer(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf826788-d1ab-42c3-a359-16ab330a51e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoPE:  tensor([[[-0.5031, -0.2963, -1.6388, -1.1080,  0.0778,  0.0641],\n",
      "         [ 0.2451,  0.6976, -0.2715,  0.9356,  1.8059, -0.5141],\n",
      "         [ 0.0044,  0.1593,  0.0037, -0.5692, -0.6155, -0.1125]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Constant:  tensor([[[-0.5031, -0.2963, -1.6388, -1.1080,  0.0778,  0.0641],\n",
      "         [ 0.2461,  0.6986, -0.2705,  0.9366,  1.8068, -0.5131],\n",
      "         [ 0.0063,  0.1613,  0.0056, -0.5672, -0.6135, -0.1106]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Learnable:  tensor([[[ 0.3895,  0.2742, -0.8089, -1.9987, -0.0772,  1.6185],\n",
      "         [-1.3515,  0.7869,  1.7763,  2.6277,  1.2768, -0.4698],\n",
      "         [ 0.9530,  0.7793, -1.3005, -2.0253, -0.2677, -2.4765]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Sin-cos-PE:  tensor([[[-0.5031, -0.2963, -1.6388, -1.1080,  0.0778,  0.0641],\n",
      "         [ 1.0866,  1.5391, -0.2251,  0.9820,  1.8080, -0.5119],\n",
      "         [ 0.9137,  1.0686,  0.0964, -0.4765, -0.6112, -0.1082]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransformerInputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    词向量 + 位置编码\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size = 100, dim = 512, max_len = 1024, base = 10000.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.position_encoding = nn.Parameter( torch.randn(max_len, dim) ) # learnable\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # sin-cos position encoding\n",
    "        # 人工设计的 位置编码\n",
    "        group = dim // 2\n",
    "        theta_ids = torch.arange(0, dim, 2) # 0, 2, 4, ..., 512\n",
    "        theta =  1 / ( base ** ( theta_ids / dim ) )\n",
    "        pe = torch.zeros(dim) # 512, sin( theta_0 ),cos( theta_0), ...\n",
    "        pe[theta_ids] = theta\n",
    "        pe[theta_ids+1] = theta\n",
    "\n",
    "        position_ids = torch.arange(0, max_len) # 0, 1, 2, ..., 1024\n",
    "        self.PE = torch.outer(position_ids, pe) # 1024 x 512\n",
    "        \n",
    "        self.PE[:, theta_ids] = torch.sin(self.PE[:, theta_ids])\n",
    "        self.PE[:, theta_ids+1] = torch.sin(self.PE[:, theta_ids+1])\n",
    "\n",
    "    def forward_NoPE(self, input_ids):\n",
    "        \"\"\"\n",
    "        嵌入向量 + 无位置编码\n",
    "        \"\"\"\n",
    "        X = self.embedding(input_ids)\n",
    "        return X\n",
    "\n",
    "    def forward_basic(self, input_ids):\n",
    "        \"\"\"\n",
    "        嵌入向量 + 常数向量位置编码\n",
    "        \"\"\"\n",
    "        bs, seq_len = input_ids.shape\n",
    "        X = self.embedding(input_ids)\n",
    "        PE = torch.arange(seq_len).unsqueeze(dim = 0).unsqueeze(dim = 2)\n",
    "        X_ = X + PE / self.max_len\n",
    "        return X_\n",
    "\n",
    "    def forward_learn(self, input_ids):\n",
    "        \"\"\"\n",
    "        嵌入向量 + 可学习位置编码\n",
    "        \"\"\"\n",
    "        bs, seq_len = input_ids.shape\n",
    "        X = self.embedding(input_ids)\n",
    "        PE = self.position_encoding[:seq_len, :]\n",
    "        X_ = X + PE\n",
    "        return X_\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        嵌入向量 + 绝对位置编码(标准实现)\n",
    "        \"\"\"\n",
    "        bs, seq_len = input_ids.shape\n",
    "        X = self.embedding(input_ids)\n",
    "        PE = self.PE[:seq_len, :]\n",
    "        X_ = X + PE\n",
    "        return X_\n",
    "\n",
    "input_layer = TransformerInputLayer(vocab_size = src_vocab_size, dim = 6)\n",
    "print('NoPE: ', input_layer.forward_NoPE(src_ids[:1, :3]))\n",
    "print('Constant: ', input_layer.forward_basic(src_ids[:1, :3]))\n",
    "print('Learnable: ', input_layer.forward_learn(src_ids[:1, :3]))\n",
    "print('sin-cos-PE: ', input_layer.forward(src_ids[:1, :3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883deea3-eb89-4903-8c90-9d955c02712d",
   "metadata": {},
   "source": [
    "## Transformer Encoder\n",
    "\n",
    "- 归一化层\n",
    "- 多头注意力层\n",
    "- 前馈层\n",
    "- 残差链接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de344e0c-9a96-4c11-8c81-09a4e8118e4b",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d19512-266a-4786-b59e-2ab6812e3d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm()\n",
      "tensor([[[-0.6895,  0.2339, -0.8599,  1.3155],\n",
      "         [-0.9774, -0.7341,  0.9996,  0.7119],\n",
      "         [ 1.4324, -0.8155, -0.0928, -0.5241]],\n",
      "\n",
      "        [[ 0.4565,  0.5562, -1.4987,  0.4860],\n",
      "         [ 0.5503,  1.1316, -0.8761, -0.8057],\n",
      "         [ 1.4116, -0.2015, -0.2627, -0.9475]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, ):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        self.epsilon = 1e-8\n",
    "    def forward(self, X, ):\n",
    "        mu = X.mean( dim = -1, keepdim = True)\n",
    "        var = X.var( dim = -1, keepdim = True)\n",
    "        X_hat = ( X - mu ) / torch.sqrt( var + self.epsilon)\n",
    "        Y = X_hat * self.gamma + self.beta\n",
    "        return Y\n",
    "        \n",
    "tmp_dim = 4\n",
    "LN = LayerNorm(dim = tmp_dim)\n",
    "print(LN)\n",
    "X = torch.randn(2,3,4)\n",
    "print(LN(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38989faf-2269-40a7-8c5f-fb4b884ca647",
   "metadata": {},
   "source": [
    "### Feed Forward Network (FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea0ac9a-525d-4abf-9a09-262fb2f7c44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNetwork(\n",
      "  (W_up): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (ReLU): ReLU()\n",
      "  (W_down): Linear(in_features=16, out_features=4, bias=True)\n",
      ")\n",
      "tensor([[[-0.3446,  0.1762, -0.4142, -0.0142],\n",
      "         [-0.4503,  0.0020, -0.2082,  0.2073],\n",
      "         [-0.4478,  0.4252, -0.6522, -0.2090]],\n",
      "\n",
      "        [[-0.5538, -0.0942, -0.1867,  0.0850],\n",
      "         [-0.0023, -0.1210, -0.1883,  0.1941],\n",
      "         [-0.0823, -0.0515, -0.1285,  0.3077]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, dim, ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.W_up = nn.Linear(self.dim, 4 * self.dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.W_down = nn.Linear(4 * self.dim, self.dim)\n",
    "    def forward(self, X):\n",
    "        X_ = self.ReLU(self.W_up(X))\n",
    "        Y = self.W_down(X_)\n",
    "        return Y\n",
    "        \n",
    "FFN = FeedForwardNetwork(dim = 4)\n",
    "X = torch.randn(2,3,4)\n",
    "print(FFN)\n",
    "print(FFN(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111103a4-1969-409e-ae28-8ff584dbb8db",
   "metadata": {},
   "source": [
    "### Multi Heads Attention\n",
    "\n",
    "self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d6d2c80-5df4-46a9-aa47-a8f696a59b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, heads = 8):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(dim_in, dim_out)\n",
    "        self.WK = nn.Linear(dim_in, dim_out)\n",
    "        self.WV = nn.Linear(dim_in, dim_out)\n",
    "        self.WO = nn.Linear(dim_in, dim_out)\n",
    "        self.heads = 8\n",
    "        self.head_dim = dim_out // self.heads\n",
    "        \n",
    "    def forward(self, X_Q, X_K, X_V, mask = None):\n",
    "        bs, seq_len, dim = X_Q.shape\n",
    "        bs, seq_K_len, dim = X_K.shape\n",
    "        bs, seq_V_len, dim = X_V.shape\n",
    "        Q = self.WQ(X_Q)\n",
    "        K = self.WK(X_K)\n",
    "        V = self.WV(X_V)\n",
    "\n",
    "        # 拆分维度\n",
    "        Q_h = Q.view(bs, seq_len, self.heads, self.head_dim).transpose(1,2)\n",
    "        K_h = K.view(bs, seq_K_len, self.heads, self.head_dim).transpose(1,2) # KV len 可以不等同于 Q len\n",
    "        V_h = V.view(bs, seq_V_len, self.heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # 多个 q_i 计算注意力特征\n",
    "        S = Q_h @ K_h.transpose(2,3) / math.sqrt(self.head_dim) # 1. 为什么要除于 \\sqrt{d}\n",
    "\n",
    "        if mask is not None:\n",
    "            idx = torch.where(mask == 0)\n",
    "            # 维度 “：” 表示头并行共享 mask 矩阵\n",
    "            S[idx[0], :,idx[1],idx[2]] = -10000.0 \n",
    "        \n",
    "        P = torch.softmax(S, dim = -1) # 行 softmax\n",
    "        Z = P @ V_h\n",
    "\n",
    "        # 恢复维度\n",
    "        Z = Z.transpose(1,2).reshape(bs, seq_len, dim)\n",
    "        \n",
    "        output = self.WO(Z)\n",
    "        \n",
    "        return output\n",
    "tmp_dim = 16\n",
    "X = torch.randn(2, 4, tmp_dim)\n",
    "mask = torch.ones(2, 4, tmp_dim)\n",
    "model = MultiHeadScaleDotProductAttention(tmp_dim, tmp_dim, 8)\n",
    "Y = model(X, X, X, mask)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d935c-590d-4b8f-a5b0-51539b13e1fe",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe764f3-f890-4348-932a-6782bc0c2781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim = 512, heads = 8):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadScaleDotProductAttention(dim, dim, heads)\n",
    "        self.ln1 = LayerNorm(dim)\n",
    "        self.ffn = FeedForwardNetwork(dim)\n",
    "        self.ln2 = LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, X, src_mask = None):\n",
    "        X_attn = self.attn(X, X, X, mask = src_mask)\n",
    "        X_ln = self.ln1(X_attn)\n",
    "        X = X + X_ln\n",
    "\n",
    "        X_ffn = self.ffn(X)\n",
    "        X_ln = self.ln2(X_ffn)\n",
    "        X = X + X_ln\n",
    "\n",
    "        return X\n",
    "\n",
    "tmp_dim = 16\n",
    "X = torch.randn(2, 4, tmp_dim)\n",
    "mask = torch.ones(2, 4, tmp_dim)\n",
    "model = TransformerEncoderBlock(tmp_dim, 8)\n",
    "Y = model(X, mask)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc5c732-130e-4a5b-9607-da94b22b71a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    输入 原文本序列，输出 token 序列的编码表征\n",
    "    输入:[bs, src_seq_len, dim]\n",
    "    输出:[bs, src_seq_len, dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, dim = 512, num_layers = 6, heads = 8):\n",
    "        super().__init__()\n",
    "        # self.encoder = nn.Linear(dim, dim) \n",
    "        self.encoder = nn.ModuleList(\n",
    "            [TransformerEncoderBlock(dim, heads) for i in range(num_layers)]\n",
    "        )\n",
    "    def forward(self, X, mask = None):\n",
    "        for encode_block in self.encoder:\n",
    "            X = encode_block(X, mask)\n",
    "        return X\n",
    "        \n",
    "tmp_dim = 16\n",
    "X = torch.randn(2, 4, tmp_dim)\n",
    "mask = torch.ones(2, 4, tmp_dim)\n",
    "model = TransformerEncoder(tmp_dim, tmp_dim, 8)\n",
    "Y = model(X, mask)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff277ad-0e6a-4e6a-b488-fe2afcfd50ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (encoder): ModuleList(\n",
      "    (0-15): 16 x TransformerEncoderBlock(\n",
      "      (attn): MultiHeadScaleDotProductAttention(\n",
      "        (WQ): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (WK): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (WV): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (WO): Linear(in_features=16, out_features=16, bias=True)\n",
      "      )\n",
      "      (ln1): LayerNorm()\n",
      "      (ffn): FeedForwardNetwork(\n",
      "        (W_up): Linear(in_features=16, out_features=64, bias=True)\n",
      "        (ReLU): ReLU()\n",
      "        (W_down): Linear(in_features=64, out_features=16, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b543f4-4989-4ae8-9848-484e6b1b3ac9",
   "metadata": {},
   "source": [
    "### Encoder Mask detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a6764e-8bc2-4af4-92e1-b7aae96e6901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5355,  1.0730,  0.4873, -0.0000,  0.0000],\n",
       "         [ 0.4597, -0.0714,  1.8693,  0.0000,  0.0000],\n",
       "         [-1.1784, -0.6136, -0.9517, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[ 1.5798, -1.2041,  0.0000, -0.0000, -0.0000],\n",
       "         [-2.3888,  0.3134, -0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_src_mask(input_ids, pad_token_id = 0):\n",
    "    bs, seq_len = input_ids.shape\n",
    "    mask = torch.ones(bs, seq_len, seq_len)\n",
    "    for i in range(bs):\n",
    "        pad_idx =  torch.where(input_ids[i, :]  == pad_token_id)[0]\n",
    "        mask[i, pad_idx, :] = 0\n",
    "        mask[i, :, pad_idx] = 0\n",
    "    return mask\n",
    "    \n",
    "input_ids = torch.tensor([[1, 2, 3, 0, 0],\n",
    "                          [1, 2, 0, 0, 0]], dtype = torch.long) # 0 is pad\n",
    "bs, seq_len = input_ids.shape\n",
    "mask = get_src_mask(input_ids, pad_token_id = PAD_TOKEN_ID)\n",
    "print(mask)\n",
    "\n",
    "score = torch.randn(bs, seq_len, seq_len)\n",
    "score * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425d6a65-d1ed-4366-bb54-eeba63e28cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.0993e+00, -6.1178e-01,  1.5540e+00,  0.0000e+00, -0.0000e+00],\n",
       "          [ 4.3575e-01,  8.6956e-01, -1.0213e-01, -0.0000e+00, -0.0000e+00],\n",
       "          [ 4.4461e-01,  1.5828e+00,  2.7130e-01,  0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 8.1156e-01,  9.3808e-02,  4.4381e-01,  0.0000e+00, -0.0000e+00],\n",
       "          [-1.0390e+00,  1.7692e-01,  1.1105e+00, -0.0000e+00,  0.0000e+00],\n",
       "          [ 9.5825e-01, -3.1649e-04,  7.1094e-01,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-7.9658e-01, -1.1100e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
       "          [-3.5486e-01,  1.4551e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[-7.7056e-01, -1.4599e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
       "          [-9.4509e-01, -1.0617e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_score = torch.randn(bs, 2, seq_len, seq_len) # multi-head score\n",
    "multi_head_score * mask.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef86f9-e112-484f-ab1e-49f69bfeebb4",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "276c07d6-ce47-4d6f-b50b-e0bd9f36fb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, dim = 512, heads = 8):\n",
    "        super().__init__()\n",
    "        self.masked_attn = MultiHeadScaleDotProductAttention(dim, dim, heads)\n",
    "        self.ln1 = LayerNorm(dim)\n",
    "    \n",
    "        self.cross_attn = MultiHeadScaleDotProductAttention(dim, dim, heads)\n",
    "        self.ln2 = LayerNorm(dim)\n",
    "        \n",
    "        self.ffn = FeedForwardNetwork(dim)\n",
    "        self.ln3 = LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, X, X_src, trg_mask = None, src_trg_mask = None):\n",
    "        X_attn = self.masked_attn(X, X, X, trg_mask)\n",
    "        X_ln = self.ln1(X_attn)\n",
    "        X = X + X_ln\n",
    "        \n",
    "        X_attn = self.cross_attn(X, X_src, X_src, src_trg_mask)\n",
    "        X_ln = self.ln2(X_attn)\n",
    "        X = X + X_ln\n",
    "\n",
    "        X_ffn = self.ffn(X)\n",
    "        X_ln = self.ln3(X_ffn)\n",
    "        X = X + X_ln\n",
    "\n",
    "        return X\n",
    "\n",
    "tmp_dim = 16\n",
    "X = torch.randn(2, 4, tmp_dim)\n",
    "X_src = torch.randn(2, 8, tmp_dim)\n",
    "mask = torch.ones(2, 4, tmp_dim)\n",
    "model = TransformerDecoderBlock(tmp_dim, 8)\n",
    "Y = model(X, X_src)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "316e7f04-fb94-44cc-9820-a7ab3547fb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    输入:[bs, trg_seq_len, dim]\n",
    "    输出:[bs, trc_seq_len, dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, dim = 512, num_layers = 6, heads = 8):\n",
    "        super().__init__()\n",
    "        # self.decoder = nn.Linear(dim, dim) \n",
    "        self.decoder = nn.ModuleList(\n",
    "            [TransformerDecoderBlock(dim, heads) for i in range(num_layers)]\n",
    "        )\n",
    "    def forward(self, X, X_src, trg_mask = None, src_trg_mask = None):\n",
    "        for decoder_block in self.decoder:\n",
    "            X = decoder_block(X, X_src, trg_mask, src_trg_mask)\n",
    "        return X\n",
    "        \n",
    "tmp_dim = 16\n",
    "X = torch.randn(2, 4, tmp_dim)\n",
    "X_src = torch.randn(2, 3, tmp_dim)\n",
    "model = TransformerDecoder(tmp_dim)\n",
    "Y = model(X, X_src, )\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e11e84-16ee-4cfb-8147-b317cf838df5",
   "metadata": {},
   "source": [
    "### masked-self-attention mask detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e3eaa2-6a23-4ad3-b59d-3038a33f48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3775, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "         [-0.2391,  0.4032, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.2447,  0.4989,  1.8628,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[ 1.7257, -0.0000, -0.0000,  0.0000,  0.0000],\n",
       "         [ 0.3218, -1.6551,  0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trg_mask(input_ids, pad_token_id = 0):\n",
    "    bs, seq_len = input_ids.shape\n",
    "    mask = torch.tril(torch.ones(bs, seq_len, seq_len)) # tril\n",
    "    for i in range(bs):\n",
    "        pad_idx =  torch.where(input_ids[i, :]  == pad_token_id)[0]\n",
    "        mask[i, pad_idx, :] = 0\n",
    "        mask[i, :, pad_idx] = 0\n",
    "    return mask\n",
    "    \n",
    "input_ids = torch.tensor([[1, 2, 3, 0, 0],\n",
    "                          [1, 2, 0, 0, 0]], dtype = torch.long) # 0 is pad\n",
    "mask = get_trg_mask(input_ids, pad_token_id = PAD_TOKEN_ID)\n",
    "print(mask)\n",
    "\n",
    "score = torch.randn(bs, seq_len, seq_len)\n",
    "score * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3842fae7-d792-4d05-b8e1-97dc0dff7af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5273,  0.0000,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.3483,  1.0282,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.1696, -2.3923,  1.0051, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.3363,  0.0000,  0.0000, -0.0000,  0.0000],\n",
       "          [-0.2155, -0.9674,  0.0000,  0.0000, -0.0000],\n",
       "          [ 0.2109, -0.2376,  0.9523, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1990, -0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.7963, -0.1189, -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.9682, -0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.6906,  0.7477, -0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_score = torch.randn(bs, 2, seq_len, seq_len) # multi-head score\n",
    "multi_head_score * mask.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f274dd-3b8d-4fe3-b9ba-9b65874c9dfd",
   "metadata": {},
   "source": [
    "### cross-attention mask detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "658ca1c5-fde6-4244-aa5b-a01c55d33975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0457,  0.9644, -0.8429, -0.0000,  0.0000],\n",
       "         [-0.8643, -1.7688, -0.9666, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.3906, -0.6990,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.5454, -1.3998,  0.0000, -0.0000, -0.0000],\n",
       "         [ 0.8878,  0.9592,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_src_trg_mask(src_ids, trg_ids, pad_token_id = 0):\n",
    "    bs, src_seq_len = src_ids.shape\n",
    "    bs, trg_seq_len = trg_ids.shape\n",
    "    \n",
    "    mask = torch.ones(bs, trg_seq_len, src_seq_len) # tril\n",
    "    for i in range(bs):\n",
    "        src_pad_idx =  torch.where(src_ids[i, :]  == pad_token_id)[0]\n",
    "        trg_pad_idx =  torch.where(trg_ids[i, :]  == pad_token_id)[0]\n",
    "        mask[i, trg_pad_idx, :] = 0\n",
    "        mask[i, :, src_pad_idx] = 0\n",
    "    return mask\n",
    "    \n",
    "src_ids = torch.tensor([[1, 2, 3, 0, 0],\n",
    "                          [1, 2, 0, 0, 0]], dtype = torch.long) # 0 is pad\n",
    "\n",
    "trg_ids = torch.tensor([[4, 5, 0, 0, ],\n",
    "                          [4, 5, 6, 0, ]], dtype = torch.long) # 0 is pad\n",
    "\n",
    "mask = get_src_trg_mask(src_ids, trg_ids, PAD_TOKEN_ID)\n",
    "print(mask)\n",
    "\n",
    "score = torch.randn(bs, 4, 5)\n",
    "score * mask\n",
    "\n",
    "# 头并行同理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdee52-d47e-4dc5-8bda-c122b482b273",
   "metadata": {},
   "source": [
    "## Transformer output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee64acc0-cf7e-45ce-93ff-bbd98d21ad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerOutputLayer(\n",
      "  (lm_head): Linear(in_features=16, out_features=200, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "torch.Size([2, 8, 16]) torch.Size([2, 8, 200])\n"
     ]
    }
   ],
   "source": [
    "class TransformerOutputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size = 100, dim = 512):\n",
    "        super().__init__()\n",
    "        self.lm_head = nn.Linear(dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        logits = self.lm_head(X)\n",
    "        prob = self.softmax(logits)\n",
    "        return logits\n",
    "\n",
    "X = torch.randn(2, 8, tmp_dim)\n",
    "model = TransformerOutputLayer(trg_vocab_size, tmp_dim)\n",
    "print(model)\n",
    "Y = model(X)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057b5c7-20dd-4184-8056-b2e8f1297c6d",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "855d2398-35db-4acc-ada8-e2bc49e0e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1794, -1.7957, -0.6651],\n",
      "         [-0.2517, -0.2236, -1.9435],\n",
      "         [-1.0218,  0.3721,  1.4052]],\n",
      "\n",
      "        [[-2.0408,  0.0177,  0.4979],\n",
      "         [-0.0796, -1.9046,  0.4364],\n",
      "         [ 1.5434,  0.7333, -0.1994]]])\n",
      "tensor([[[-2.2118],\n",
      "         [-2.4860],\n",
      "         [-1.4007]],\n",
      "\n",
      "        [[-1.0094],\n",
      "         [-1.0426],\n",
      "         [-0.4823]]])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.],\n",
      "         [0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-1.1847)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor([[1, 2, 1],\n",
    "                          [1, 0, 0]], dtype = torch.long) # 0 is pad\n",
    "logits = torch.randn(2, 3, 3)\n",
    "\n",
    "print(logits)\n",
    "logprob = torch.log_softmax(logits, dim = -1) # why use `log_softmax()` instead `softmax()`\n",
    "logprob = torch.gather(logprob, dim = -1, index = labels.unsqueeze(dim = -1))\n",
    "print(logprob)\n",
    "\n",
    "mask = torch.ones(2, 3, 1)\n",
    "mask[ torch.where(labels == PAD_TOKEN_ID) ] = 0\n",
    "print(mask)\n",
    "\n",
    "torch.mean(logprob * mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11dbcd7c-6933-4521-9024-ee3e24946abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 200])\n",
      "tensor([[5, 0, 0],\n",
      "        [5, 6, 0]])\n",
      "tensor([[ 1.1350,  0.4015, -0.9549],\n",
      "        [ 1.2343, -1.5474,  0.1226]])\n",
      "tensor(-3.6103)\n"
     ]
    }
   ],
   "source": [
    "def CrossEntropyLoss(logits, labels, ignore_index = 0):\n",
    "    \"\"\"\n",
    "    logits: bs x seq_len x num_classes\n",
    "    label: bs x seq_len\n",
    "    return: loss\n",
    "    \"\"\"\n",
    "    bs, seq_len, num_classes = logits.shape\n",
    "    log_prob = F.log_softmax(logits, dim = -1) # vec\n",
    "    log_prob = torch.gather(log_prob, dim = -1, index = labels.unsqueeze(dim = -1)) # point\n",
    "    \n",
    "    if ignore_index is not None:\n",
    "        mask = torch.ones(bs, seq_len, 1)\n",
    "        mask[torch.where(labels == ignore_index)] = 0\n",
    "    loss = torch.mean(log_prob * mask)\n",
    "    return loss\n",
    "\n",
    "trg_ids = torch.tensor([[4, 5, 0, 0, ],\n",
    "                          [4, 5, 6, 0, ]], dtype = torch.long) # 0 is pad\n",
    "\n",
    "logits = torch.randn(2, 4, trg_vocab_size)\n",
    "print(logits.shape)\n",
    "\n",
    "print(trg_ids[:, 1:])\n",
    "print(logits[:, :-1, 0])\n",
    "\n",
    "loss = CrossEntropyLoss(logits, labels = trg_ids, ignore_index = 0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c2dd6-da98-47da-b142-6d14d80de563",
   "metadata": {},
   "source": [
    "# Transformer New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62390c60-34cd-4a6d-88b1-ceb472f2aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder_input): TransformerInputLayer(\n",
      "    (embedding): Embedding(100, 512)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (encoder): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderBlock(\n",
      "        (attn): MultiHeadScaleDotProductAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln1): LayerNorm()\n",
      "        (ffn): FeedForwardNetwork(\n",
      "          (W_up): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (ReLU): ReLU()\n",
      "          (W_down): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln2): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_input): TransformerInputLayer(\n",
      "    (embedding): Embedding(200, 512)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (decoder): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderBlock(\n",
      "        (masked_attn): MultiHeadScaleDotProductAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln1): LayerNorm()\n",
      "        (cross_attn): MultiHeadScaleDotProductAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln2): LayerNorm()\n",
      "        (ffn): FeedForwardNetwork(\n",
      "          (W_up): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (ReLU): ReLU()\n",
      "          (W_down): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln3): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): TransformerOutputLayer(\n",
      "    (lm_head): Linear(in_features=512, out_features=200, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    输入:[bs, src_seq_len]\n",
    "    输出:[bs, trg_seq_len, vocab_size]\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size = 100, trg_vocab_size = 200, dim = 512, num_layers = 6, heads = 8, max_len = 512):\n",
    "        super().__init__()\n",
    "        #self.encoder_input = nn.Embedding(src_vocab_size, dim)\n",
    "        self.encoder_input = TransformerInputLayer(vocab_size = src_vocab_size, \n",
    "                                                   dim = dim, \n",
    "                                                   max_len = max_len, )\n",
    "        # self.encoder = nn.Linear(dim, dim)\n",
    "        self.encoder = TransformerEncoder(dim = dim, \n",
    "                                          num_layers = num_layers, \n",
    "                                          heads = heads)\n",
    "        \n",
    "        # self.decoder_input = nn.Embedding(trg_vocab_size, dim)\n",
    "        self.decoder_input = TransformerInputLayer(vocab_size = trg_vocab_size, \n",
    "                                                   dim = dim, \n",
    "                                                   max_len = max_len, )\n",
    "        # self.decoder = nn.Linear(dim, dim)\n",
    "        self.decoder = TransformerDecoder(dim = dim, \n",
    "                                          num_layers = num_layers, \n",
    "                                          heads = heads)\n",
    "\n",
    "        # self.output_layer = nn.Linear(dim, trg_vocab_size)\n",
    "        self.output_layer = TransformerOutputLayer(vocab_size = trg_vocab_size, \n",
    "                                                   dim = dim)\n",
    "        \n",
    "    def forward(self, src_ids, trg_ids, src_mask = None, trg_mask = None, src_trg_mask = None):\n",
    "        X = self.encoder_input(src_ids)\n",
    "        X_src = self.encoder(X, src_mask)\n",
    "        # print('encoder output:\\t', X_src.shape)\n",
    "\n",
    "        Y = self.decoder_input(trg_ids)\n",
    "        # Y = self.decoder(Y) + X_src.mean(dim = 1, keepdim = True)\n",
    "        Y = self.decoder(Y, X_src, trg_mask = trg_mask, src_trg_mask = src_trg_mask)\n",
    "        # print('decoder output:\\t', Y.shape)\n",
    "\n",
    "        logits = self.output_layer(Y) \n",
    "        prob = F.softmax(logits, dim = -1)\n",
    "        \n",
    "        return logits, prob\n",
    "    \n",
    "model = Transformer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6baf55f9-ff1f-4ca2-88cb-d1943789fa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode input shape:  torch.Size([2, 256])\n",
      "decode output shape:  torch.Size([2, 128])\n",
      "transformer output:\t torch.Size([2, 128, 200])\n"
     ]
    }
   ],
   "source": [
    "src_ids = torch.randint(src_vocab_size, (batch_size, src_len))\n",
    "trg_ids = torch.randint(trg_vocab_size, (batch_size, trg_len))\n",
    "print('encode input shape: ', src_ids.shape)\n",
    "print('decode output shape: ', trg_ids.shape)\n",
    "\n",
    "src_mask = get_src_mask(src_ids)\n",
    "trg_mask = get_trg_mask(trg_ids)\n",
    "src_trg_mask = get_src_trg_mask(src_ids, trg_ids)\n",
    "\n",
    "logits, _ = model(src_ids, trg_ids, src_mask = src_mask, trg_mask = trg_mask, src_trg_mask = src_trg_mask)\n",
    "print('transformer output:\\t', logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03971f-77b8-4760-b926-71f8fd83d490",
   "metadata": {},
   "source": [
    "## Train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab04017b-8893-49c6-a889-fd29ba56c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 512 \n",
    "num_layers = 6\n",
    "heads = 8\n",
    "\n",
    "batch_size = 2\n",
    "src_len = 256\n",
    "trg_len = 128\n",
    "max_len = 512\n",
    "\n",
    "src_vocab_size = 100\n",
    "trg_vocab_size = 200\n",
    "\n",
    "N_train = 1024 # dataset\n",
    "N_test = 128\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "PAD_TOKEN_ID = 0\n",
    "SOS_TOKEN_ID = 1\n",
    "EOS_TOKEN_ID = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73dd44a-9a4a-4cf6-be50-682aee9a6296",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3903a84d-c752-4d4b-b629-d8457e34e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx,:], self.Y[idx,:]\n",
    "\n",
    "def create_dataset(N, src_len,trg_len,src_vocab_size,trg_vocab_size):\n",
    "    X = torch.randint(2, src_vocab_size-1, (N, src_len), dtype = torch.long) \n",
    "    Y = torch.randint(2, trg_vocab_size-1, (N, trg_len), dtype = torch.long) \n",
    "    X[:, 0] = SOS_TOKEN_ID # <SOS>\n",
    "    X[:, src_len - 1] = EOS_TOKEN_ID # <PAD> or <EOS>\n",
    "    Y[:, 0] = SOS_TOKEN_ID # <SOS>\n",
    "    Y[:, trg_len - 1] = EOS_TOKEN_ID # <PAD> or <EOS>\n",
    "    dataset = Seq2SeqDataset(X, Y)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(N = N_train, \n",
    "              src_len = src_len, \n",
    "              trg_len = trg_len,  \n",
    "              src_vocab_size = src_vocab_size, \n",
    "              trg_vocab_size = trg_vocab_size)\n",
    "\n",
    "\n",
    "test_dataset = create_dataset(N = N_test, \n",
    "              src_len = src_len, \n",
    "              trg_len = trg_len,  \n",
    "              src_vocab_size =src_vocab_size, \n",
    "              trg_vocab_size = trg_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5198b17-cfbe-4a2a-a5fc-ee048a07de2c",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ebd33d0-0667-43af-8852-2a9f589f74b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1, 47,  3,  ..., 62, 61,  2],\n",
      "        [ 1, 77, 25,  ..., 69, 59,  2],\n",
      "        [ 1, 45, 97,  ...,  4, 92,  2],\n",
      "        ...,\n",
      "        [ 1, 44, 53,  ..., 76, 44,  2],\n",
      "        [ 1, 15,  4,  ..., 96, 85,  2],\n",
      "        [ 1, 96, 38,  ..., 37, 84,  2]]), tensor([[  1,  62, 137,  ...,  40,  93,   2],\n",
      "        [  1, 196, 155,  ...,  20, 171,   2],\n",
      "        [  1, 137,   3,  ...,  64, 194,   2],\n",
      "        ...,\n",
      "        [  1, 116,  85,  ...,  83, 111,   2],\n",
      "        [  1, 197, 111,  ..., 150, 146,   2],\n",
      "        [  1, 153, 116,  ..., 175, 171,   2]])]\n",
      "torch.Size([8, 256])\n",
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                    batch_size = 8 , \n",
    "                    # collate_fn=collate_fn,\n",
    "                    pin_memory=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                    batch_size= 32, \n",
    "                    # collate_fn=collate_fn,\n",
    "                    pin_memory=True)\n",
    "\n",
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed6849f-b7e2-410b-a644-e618a762cfa3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2113f879-07df-41c1-886d-d189396d2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:0, step:10, train_loss: 6.925608158111572\n",
      "epochs:0, step:20, train_loss: 6.485151290893555\n",
      "epochs:0, step:30, train_loss: 6.398404598236084\n",
      "epochs:0, step:40, train_loss: 6.281773567199707\n",
      "epochs:0, step:50, train_loss: 6.337588787078857\n",
      "epochs:0, step:60, train_loss: 6.179869651794434\n",
      "epochs:0, step:70, train_loss: 6.144567012786865\n",
      "epochs:0, step:80, train_loss: 6.168476581573486\n",
      "epochs:0, step:90, train_loss: 6.039062023162842\n",
      "epochs:0, step:100, train_loss: 6.064135551452637\n",
      "epochs:0, step:110, train_loss: 6.0511794090271\n",
      "epochs:0, step:120, train_loss: 6.056873321533203\n",
      "epochs:1, step:130, train_loss: 5.989947319030762\n",
      "epochs:1, step:140, train_loss: 6.0201592445373535\n",
      "epochs:1, step:150, train_loss: 5.9559736251831055\n",
      "epochs:1, step:160, train_loss: 5.962271213531494\n",
      "epochs:1, step:170, train_loss: 6.026018142700195\n",
      "epochs:1, step:180, train_loss: 5.931738376617432\n",
      "epochs:1, step:190, train_loss: 5.9036102294921875\n",
      "epochs:1, step:200, train_loss: 5.962732791900635\n",
      "epochs:1, step:210, train_loss: 5.944289207458496\n",
      "epochs:1, step:220, train_loss: 5.908456325531006\n",
      "epochs:1, step:230, train_loss: 5.93869161605835\n",
      "epochs:1, step:240, train_loss: 5.90361213684082\n",
      "epochs:1, step:250, train_loss: 5.916235446929932\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Transformer(src_vocab_size = src_vocab_size, \n",
    "                    trg_vocab_size = trg_vocab_size, \n",
    "                    dim = dim, \n",
    "                    num_layers = num_layers, \n",
    "                    heads = heads, \n",
    "                    max_len = max_len)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = -100)\n",
    "\n",
    "epochs = 2\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "# PPL = []\n",
    "total_step = 0\n",
    "\n",
    "\n",
    "# train\n",
    "for i in range(epochs):\n",
    "    # train\n",
    "    for k, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        X = batch[0]\n",
    "        # trg_ids : <SOS>, t1, t2, t3, <EOS>\n",
    "        # trg_ids[:,:-1]: <SOS>, t1, t2, t3\n",
    "        # trg_ids[:, 1:]:    t1, t2, t3, <EOS>\n",
    "        Y = batch[1][:, :-1] \n",
    "\n",
    "        src_mask = get_src_mask(X)\n",
    "        trg_mask = get_trg_mask(Y)\n",
    "        src_trg_mask = get_src_trg_mask(X, Y)\n",
    "\n",
    "        logits, _ = model(X, Y, src_mask, trg_mask, src_trg_mask)\n",
    "\n",
    "        label = batch[1][:, 1:] \n",
    "        bs, tmp_trg_len = label.shape\n",
    "        loss = loss_fn(logits.reshape( bs*tmp_trg_len, trg_vocab_size) , label.reshape( bs * tmp_trg_len) )\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_step = total_step + 1\n",
    "        \n",
    "        if total_step % 10 == 0: \n",
    "            print(f\"epochs:{i}, step:{total_step}, train_loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595f697-0e3a-4ab2-8d51-7a7c6799637b",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68744343-972c-4e4e-b016-b084779afb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 10\n",
    "\n",
    "src_len = 5\n",
    "\n",
    "src_ids = torch.randint(0, src_vocab_size, (1, src_len))\n",
    "src_mask = get_src_mask(src_ids, pad_token_id = PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10580b4c-b7ce-4589-909d-8a08849e8259",
   "metadata": {},
   "source": [
    "拆分 forward 为两部分\n",
    "\n",
    "```txt\n",
    "    def forward(self, src_ids, trg_ids, src_mask = None, trg_mask = None, src_trg_mask = None):\n",
    "\n",
    "        # --------------------------Stage 1--------------------------------\n",
    "        X = self.encoder_input(src_ids)\n",
    "        X_src = self.encoder(X, src_mask)\n",
    "\n",
    "\n",
    "        # --------------------------Stage 2--------------------------------\n",
    "        Y = self.decoder_input(trg_ids)\n",
    "        Y = self.decoder(Y, X_src, trg_mask = trg_mask, src_trg_mask = src_trg_mask)\n",
    "\n",
    "        logits = self.output_layer(Y) \n",
    "        prob = F.softmax(logits, dim = -1)\n",
    "        \n",
    "        return logits, prob\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b806bd0d-a84a-4238-ac26-979f871d0040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "### Stage1: Encode\n",
    "\n",
    "with torch.no_grad():\n",
    "    X = model.encoder_input(src_ids)\n",
    "    X_src = model.encoder(X, mask = src_mask)\n",
    "    print(X_src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a69293b-1877-42c2-bb55-b162468ca657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n",
      "tensor([[  1, 146]])\n",
      "tensor([[  1, 146,  14]])\n",
      "tensor([[  1, 146,  14,  14]])\n",
      "tensor([[  1, 146,  14,  14,  14]])\n",
      "tensor([[  1, 146,  14,  14,  14,  14]])\n",
      "tensor([[  1, 146,  14,  14,  14,  14,  14]])\n",
      "tensor([[  1, 146,  14,  14,  14,  14,  14,  14]])\n",
      "tensor([[  1, 146,  14,  14,  14,  14,  14,  14,  14]])\n",
      "tensor([[  1, 146,  14,  14,  14,  14,  14,  14,  14,  14]])\n",
      "tensor([[  1, 146,  14,  14,  14,  14,  14,  14,  14,  14,  14]])\n",
      "final: tensor([[  1, 146,  14,  14,  14,  14,  14,  14,  14,  14,  14]])\n"
     ]
    }
   ],
   "source": [
    "### Stage2: Decode\n",
    "trg_ids = torch.randint(0, src_vocab_size, (1, 1)) # 仅有 1 个 token\n",
    "trg_ids[:,0] = SOS_TOKEN_ID # trg 一定要 <SOS> \n",
    "\n",
    "\n",
    "trg_mask = get_src_mask(trg_ids, pad_token_id = PAD_TOKEN_ID)\n",
    "src_trg_mask = get_src_trg_mask(src_ids, trg_ids, pad_token_id = PAD_TOKEN_ID)\n",
    "\n",
    "print(trg_ids)\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "        Y = model.decoder_input(trg_ids)\n",
    "        Y = model.decoder(Y, X_src, trg_mask = None, src_trg_mask = None)\n",
    "        logits = model.output_layer(Y) # [bs, seq_len, trg_vocab_size], 并行多个 next token logits 预测\n",
    "        next_token_logits = logits[:, -1, :] # [bs, trg_vocab_size]\n",
    "        next_token_prob = F.softmax(next_token_logits, dim = -1)\n",
    "        next_token = torch.argmax(next_token_prob, dim = -1, keepdim = True)\n",
    "        trg_ids = torch.concat((trg_ids, next_token), dim = 1)\n",
    "        print(trg_ids)\n",
    "print('final:', trg_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2ccf9-e7cb-43e3-a312-4c1266a346fc",
   "metadata": {},
   "source": [
    "## Model Save\n",
    "\n",
    "在训练过程中，要保存模型，具体包含：\n",
    "\n",
    "- 保存网络权重参数, torch 用字典数据类型管理权重\n",
    "- 保存模型超参数配置: config，可以由独立的文件来管理，如 json, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "334ae248-67e1-4961-8edf-60537820fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./output/model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def model_save(filepath, model):\n",
    "    # save_path = os.path(filepath)\n",
    "    save_dict = {'model_state_dict': model.state_dict()}\n",
    "    # torch.save(model, save_path)\n",
    "    torch.save(save_dict, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "model_save('./output/model.pth', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd4c38-90c5-4d57-ab2f-31e1e39509ec",
   "metadata": {},
   "source": [
    "## Model Load\n",
    "\n",
    "权重加载是容易的（权重不等于一个模型类对象），但是会遇到 config 超参数加载问题。\n",
    "\n",
    "1. 加载 config 文件，初始化 model, 此时参数是 random 的。\n",
    "2. 加载 权重文件，并将参数 copy 到 model 对象中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4cd2fc8-5d13-415a-be5e-680ceec691b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, full_model=True):\n",
    "    # load_path = os.path(filename)\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"No model found at {filename}\")\n",
    "    \n",
    "    if full_model:\n",
    "        model = torch.load(filename, weights_only = False)\n",
    "    else:\n",
    "        checkpoint = torch.load(filename)\n",
    "        \n",
    "        # 加载模型参数\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=strict_load)\n",
    "        \n",
    "        # 返回其他保存的信息\n",
    "        return {k: v for k, v in checkpoint.items() if k != 'model_state_dict'}\n",
    "    return model\n",
    "    \n",
    "new_model = load_model('./output/model.pth', )\n",
    "# print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "406e1ffe-3cb4-4b1e-965b-98ad6e1bf560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model = Transformer(src_vocab_size = src_vocab_size, \n",
    "                    trg_vocab_size = trg_vocab_size, \n",
    "                    dim = dim, \n",
    "                    num_layers = num_layers, \n",
    "                    heads = heads, \n",
    "                    max_len = max_len)\n",
    "init_model.load_state_dict(new_model['model_state_dict'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9be8fba5-4411-44fd-b5a7-cdc30dfc29b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4045,  0.6274,  0.3160,  ..., -0.3810,  1.5396,  0.7373],\n",
      "        [ 0.6353,  1.3252,  0.8607,  ..., -1.2796, -0.7702,  0.1237],\n",
      "        [-0.2740, -0.2623,  1.5734,  ..., -0.8170, -1.2125,  0.3567],\n",
      "        ...,\n",
      "        [ 0.4068, -0.2399, -0.3577,  ..., -1.3916, -1.0082, -1.3997],\n",
      "        [ 0.4731,  0.9009,  1.5134,  ...,  0.5403,  0.0127,  0.2677],\n",
      "        [-1.8452,  0.3997, -0.2465,  ...,  0.3447, -0.8801, -0.4935]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(init_model.encoder_input.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3036db51-f750-44cb-a051-bd355aea26f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder_input): TransformerInputLayer(\n",
      "    (embedding): Embedding(100, 512)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (encoder): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderBlock(\n",
      "        (attn): MultiHeadScaleDotProductAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln1): LayerNorm()\n",
      "        (ffn): FeedForwardNetwork(\n",
      "          (W_up): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (ReLU): ReLU()\n",
      "          (W_down): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln2): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_input): TransformerInputLayer(\n",
      "    (embedding): Embedding(200, 512)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (decoder): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderBlock(\n",
      "        (masked_attn): MultiHeadScaleDotProductAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln1): LayerNorm()\n",
      "        (cross_attn): MultiHeadScaleDotProductAttention(\n",
      "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln2): LayerNorm()\n",
      "        (ffn): FeedForwardNetwork(\n",
      "          (W_up): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (ReLU): ReLU()\n",
      "          (W_down): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln3): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): TransformerOutputLayer(\n",
      "    (lm_head): Linear(in_features=512, out_features=200, bias=True)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4d1d5-861c-449c-983a-0736ce09236a",
   "metadata": {},
   "source": [
    "将在 `./model_io.ipynb` 中描写一个完整的模型 IO 类"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa36295d-1acf-4ebe-9d7c-436d85ef8625",
   "metadata": {},
   "source": [
    "# Transformer Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd868534-2522-4c7c-9aee-8ced706c0ef1",
   "metadata": {},
   "source": [
    "“小冬瓜有两把刷子”，中的“刷子”的含义，需要分析上下文语境，才能识别其具体的语义。\n",
    "\n",
    "这个语义仍有模糊的地方，它不是绝对的表示实体的“刷子”或者“能力”。 “一语双关”正是自然语言分析的复杂之处\n",
    "\n",
    "语言模型正是要建模出这种模糊的语义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c7b91-d35e-4c95-81b4-0813d3a26afe",
   "metadata": {},
   "source": [
    "## 序列建模\n",
    "\n",
    "word2vec 通过假设窗口局部词汇的关联性，从而构建无监督学习任务来学习 token 的表示。\n",
    "\n",
    "给定 “刷子” token\n",
    "\n",
    "- text1: “小冬瓜有两把**刷子**”\n",
    "- text2: “它用**刷子**画了一只小猫”\n",
    "\n",
    "在两种语境中， `刷子` 有不同的语义\n",
    "\n",
    "我们从 embedding 出发, 两句话的 `刷子` 的表示 $E_{刷子}$ 是一样的。\n",
    "\n",
    "而我们需要通过 **变换** $\\mathcal{F}(\\cdot):\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ 实现:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{F}(E_{刷子}) &\\rightarrow X_{text1:刷子},\\\\\n",
    "\\mathcal{F}(E_{刷子}) &\\rightarrow X_{text2:刷子},\\\\\n",
    "X_\\text{text1:刷子} &\\neq X_\\text{text2:刷子}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489060dc-6ec7-434b-aa3c-a89414d738a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 0, 2, 3, 7, 5, 5, 8])\n",
      "tensor([0, 5, 5, 4, 8, 7, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vocab_size = 10\n",
    "seq_len = 8\n",
    "batch_size = 2\n",
    "dim = 4\n",
    "\n",
    "\n",
    "text1 = torch.randint(0, vocab_size - 1, (1, seq_len))[0]\n",
    "text2 = torch.randint(0, vocab_size - 1, (1, seq_len))[0]\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e9bff6-7da6-4b2d-9225-34793fcbbdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 0, 2, 3, 7, 5, 5, 8])\n",
      "tensor([9, 5, 5, 4, 8, 7, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "text1[idx] = 9\n",
    "text2[idx] = 9\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52fbc97-fc13-4ac8-9ee9-2a00b771bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = nn.Embedding(vocab_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7210538f-e4cb-4511-ae20-947281933f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5181, -0.7648, -0.5946, -0.5700], grad_fn=<SelectBackward0>)\n",
      "tensor([ 1.5181, -0.7648, -0.5946, -0.5700], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_embd_1 = E(text1)\n",
    "seq_embd_2 = E(text2)\n",
    "\n",
    "print(seq_embd_1[0])\n",
    "print(seq_embd_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8f89e8-7221-4df4-9ef8-e600865a8b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.1499, -0.5838, -0.8918, -0.6797], grad_fn=<SelectBackward0>)\n",
      "tensor([ 2.2314, -0.6370, -0.8608, -0.9149], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def operation(X):\n",
    "    X_global = X.mean(dim = 0)\n",
    "    X = X_global + X\n",
    "    return X\n",
    "\n",
    "f1 = operation( seq_embd_1 )[ idx ]\n",
    "f2 = operation( seq_embd_2 )[ idx ]\n",
    "print( f1 )\n",
    "print( f2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eab669-e36e-4dc4-81ac-c263bc0fa1c0",
   "metadata": {},
   "source": [
    "以上，对于位置 $t=0$ 的 context 融合后的特征，是有差别的。\n",
    "\n",
    "从context-level特征视角：`X = X_global + X` ,  而 X_global 为 序列性的整体表征\n",
    "\n",
    "词的语义受上下文影响。\n",
    "\n",
    "序列建模的目标是：找到一种能够高效组合全局信息的方式，来表示词元在语境中的语义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ba958-1251-496a-917e-2487848b879a",
   "metadata": {},
   "source": [
    "## 高效序列建模\n",
    "\n",
    "对于文本中 “小冬瓜有两把刷子“， `刷子` 的语义是根据 “语法” 关系表示出来的。\n",
    "\n",
    "- 语法规则：我们可以通过提取主谓宾定等词性，并分析词与词（或整体）之间的联系，从而表示 `刷子`含义\n",
    "- 自动化规则：语法规则在于它是复杂的，比如“定语重置”或“倒装句”，中英文语法关系也有区别，期望能够**自动化** 表示 token 在  context 语义：\n",
    "\n",
    "在复杂的语言模式中，通用性的语言表示，实际上是要自动化的刻画 token 在 context 中的表示。\n",
    "\n",
    "我们定义权重，来刻画词元之间的语法关系\n",
    "\n",
    "|                | 小   | 冬   | 瓜   | 有   | 两   | 把   | 刷   | 子   |\n",
    "| -------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| $\\overline{w}$ | 1/8  | 1/8  | 1/8  | 1/8  | 1/8  | 1/8  | 1/8  | 1/8  |\n",
    "| $w$            | 0.1  | 0.1  | 0.5  | 0.01 | 0.01 | 0.08 | 0.1  | 0.1  |\n",
    "| $w_刷$         | 0.0  | 0.1  | 0.1  | 0.4  | 0.0  | 0.0  | 0.3  | 0.1  |\n",
    "\n",
    "- $\\overline{w}$：每个 token 对全局特征的 重要性 相同\n",
    "- $w$: 每个 token 对全局特征 重要性 不同。\n",
    "- $w_刷$: `刷` token 视角，它与其他词元之间的关系。其精细化的表示：一个特异的语法规则联系。 对于其他token也有独立的权重来表示自动化语法规则\n",
    "\n",
    "再者，对于较长的小说，也许只有关键的情节才能推动故事的发展，即 故事 与 情节有强关联，与其他描写无关或关联小。\n",
    "\n",
    "\n",
    "|                | 小   | 冬   | 瓜   | 有   | 两   | 把   | 刷   | 子   |\n",
    "| -------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| $w_刷$         | 0.0  | 0.0  | 0.0  | 0.0  | 0.2  | 0.4  | 0.3  | 0.1  |\n",
    "\n",
    "如果我们刻画了一个错误的权重, `刷子` 就难以表示在 context 中的语义。\n",
    "\n",
    "$$\n",
    "S_i = \\sum_j w_{ij} X_j\n",
    "$$\n",
    "\n",
    "所以难点其实在于 如何建模 “自动化语法规则” 的权重$w_i$，从而提高语义理解。\n",
    "\n",
    "如下我们定义向量内积，来计算特征向量之间的关联程度。（内积，只是度量相关性的一种实现）\n",
    "\n",
    "$$\n",
    "w_{ij} = X_i X_j^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6f4561-14ae-42dd-a147-8055925beb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 1.0000, -0.5625, -0.2787,  0.9354,  0.2386,  0.7529,  0.7529, -0.4618],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[ 19.6897, -12.4808, -14.2707,   2.0402]], grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 8])\n",
      "tensor([[ 19.6897, -12.4808, -14.2707,   2.0402]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = seq_embd_1\n",
    "i = 0\n",
    "X_0 = seq_embd_1[i,:].unsqueeze(dim = 0)\n",
    "print(X.shape)\n",
    "print(X_0.shape)\n",
    "\n",
    "## 循环实现\n",
    "X_weight_i = torch.zeros(1, dim)\n",
    "w_ij = torch.zeros(seq_len)\n",
    "for j in range(seq_len):\n",
    "    w_ij[j] = X[i,:] @ X[j,:].t()\n",
    "    X_weight_i += w_ij[j] * X[j,:] # weight * feature\n",
    "print(w_ij/w_ij.max())\n",
    "print(X_weight_i)\n",
    "\n",
    "\n",
    "# 矩阵操作实现\n",
    "s = X_0 @ X.t()\n",
    "print(s.shape)\n",
    "X_weight_i = s @ X\n",
    "print(X_weight_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b4286-1840-4800-b9ea-096c9dff2a45",
   "metadata": {},
   "source": [
    "## 注意力机制\n",
    "\n",
    "上述，已经找出变换 $\\mathcal{F}(\\cdot):\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "\n",
    "本质上，我们是在找 “词在序列中的表征”， \n",
    "\n",
    "\\begin{align}\n",
    "S_i = \\sum_j w_{ij} X_j,\\\\\n",
    "w_{ij} = X_i X_j^T\n",
    "\\end{align}\n",
    "\n",
    "展开式子\n",
    "\n",
    "\\begin{align}\n",
    "S_i = w_{i1} X_1 + w_{i2} X_2 + \\ldots + w_{iN} X_N \n",
    "\\end{align}\n",
    "\n",
    "其中有两部分表示 $w_{ij}$ 权重项用于 衡量各 token 的贡献，$X_i$ 即是常规的特征表示。\n",
    "\n",
    "进一步展开\n",
    "\n",
    "\\begin{align}\n",
    "S_i = (X_i X_1^T)\\cdot X_1 + (X_i X_2^T)\\cdot X_2 + \\ldots + (X_i X_N^T)\\cdot X_N, (X_i X_N^T)\\in\\mathbb{R}\n",
    "\\end{align}\n",
    "\n",
    "其中，$X_i X_N^T$ 是标量。我们将每一项都进行线性特征变换\n",
    "\n",
    "\n",
    "| $(X_i$    | $ X_j^T )$    | $\\cdot$ | $X_j$    |\n",
    "| --------- | ------------- | ------- | -------- |\n",
    "| $(X_iW_q$ | $(X_jW_k)^T)$ | $\\cdot$ | $X_jW_v$ |\n",
    "| $(Q_i$    | $K^T_j)$      | $\\cdot$ | $V_j$    |\n",
    "| 查询      | 键            | $\\cdot$ | 值       |\n",
    "\n",
    "- query：查询向量，即 词元$i$ 想要知道，他在这个 context 中的表示。即查询词元 $i$ 拿了一把钥匙\n",
    "- key：键向量，即 context 中的每个词元 就是一个门，所以两个词元之间的相关程度，需要 查询词元拿着钥匙$Q_i$ **访问** 每一道门$K_j$\n",
    "- value：值向量， 即是门里面内容$V_j$\n",
    "\n",
    "即一个查询词元$Q_i$，敲开所有的键门$K_j$并访问了内容$V_j$, 综合了所有信息后才有 **融合** 的表示\n",
    "\n",
    "\\begin{align}\n",
    "S_i &= \\sum_j (Q_iK^T_j) V_j,\\\\\n",
    "Q_i &= X_iW_Q \\\\\n",
    "K_j &= X_jW_K \\\\\n",
    "V_j &= X_jW_V \\\\\n",
    "\\end{align}\n",
    "\n",
    "其中 $W_Q,W_K,W_V \\in \\mathbb{R}^{d \\times d}$ , 对各个词特征进行投影变换，实现查询、键、值的表示功能。上式即为**注意力机制**，其计算输出称之为 **注意力特征**\n",
    "\n",
    "那么，原始的计算形式**特征$(x_{i})$加权$(w_{ij})$组合$(\\sum)$**，同样是**注意力机制**， \n",
    "\n",
    "\\begin{align}\n",
    "S_i = \\sum_j w_{ij} X_j,\\\\\n",
    "\\end{align}\n",
    "\n",
    "**既然原始的输入向量能够做 注意力计算，为什么还需要投影？**\n",
    "\n",
    "考虑$w_{ij}$ 为 $w_{刷:}$\n",
    "\n",
    "|                | 小   | 冬   | 瓜   | 有   | 两   | 把   | 刷   | 子   |\n",
    "| -------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| $w_刷$         | 0.0  | 0.0  | 0.0  | 0.0  | 0.2  | 0.4  | 0.3  | 0.1  |\n",
    "\n",
    "$w_\\text{刷,瓜} = Q_\\text{刷}K_\\text{瓜} = 0.0 $ 由于这种表示，忽略了 主语“小冬瓜”， 那么就会造成`刷`的语义错误，使得任务出错.\n",
    "\n",
    "反向传播时调整$W_Q',W_K'$, 从而调整权重 $w_\\text{刷,瓜} = Q_\\text{刷}'K_\\text{瓜}' = 0.2 $，改变注意力特征关系，而$W_v'$ 同理。\n",
    "\n",
    "引入参数对表征进行投影，实际是希望投影后的特征，**让查询能够高效注意到对预测任务有重要贡献的词元**。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7cd426a-1fbb-4bcb-b399-84c7cb78b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0310,  0.4020, -0.9460, -1.0811]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.0453e-02, -2.2811e-03,  7.2466e-01,  2.6776e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 13.9701, -18.9218,  24.5574,  30.4336]])\n",
      "tensor([[ -1.8335,  16.1551, -15.2250, -24.2852]])\n"
     ]
    }
   ],
   "source": [
    "class AttentionSimplest(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(dim_in, dim_out)\n",
    "        self.WK = nn.Linear(dim_in, dim_out)\n",
    "        self.WV = nn.Linear(dim_in, dim_out)\n",
    "        # W_O 对输出做一层投影，对齐到外部空间\n",
    "        self.WO = nn.Linear(dim_in, dim_out) \n",
    "        \n",
    "    def forward(self, X, i):\n",
    "        seq_len, dim = X.shape\n",
    "        query_i = X[i, :].unsqueeze(dim = 0)\n",
    "        qi = self.WQ(query_i)\n",
    "        K = self.WK(X)\n",
    "        V = self.WV(X)\n",
    "\n",
    "        attn_i = torch.zeros(1, dim)\n",
    "        for j in range(seq_len):\n",
    "            w_ij = qi @ K[j,:].t()\n",
    "            attn_i += w_ij * V[j,:]\n",
    "\n",
    "        output = self.WO(attn_i)\n",
    "        return output\n",
    "\n",
    "    def forward_basic(self, X, i):\n",
    "        \"\"\"\n",
    "        此版本不做投影变换，仍然能算注意力\n",
    "        \"\"\"\n",
    "        seq_len, dim = X.shape\n",
    "        qi = X[i, :].unsqueeze(dim = 0)\n",
    "        K = X\n",
    "        V = X\n",
    "        attn_i = torch.zeros(1, dim)\n",
    "        for j in range(seq_len):\n",
    "            w_ij = qi @ K[j,:].t()\n",
    "            attn_i += w_ij * V[j,:]\n",
    "        return attn_i\n",
    "\n",
    "X = torch.randn(seq_len, dim)\n",
    "attn = AttentionSimplest(dim, dim)\n",
    "\n",
    "\n",
    "# 我们对 每个查询, 都能获得 token_i 在 context 中的语义表示\n",
    "O_0 = attn(X, 0)\n",
    "O_1 = attn(X, 1)\n",
    "print(O_0)\n",
    "print(O_1)\n",
    "\n",
    "\n",
    "# 我们对 每个查询, 不经过投影变换\n",
    "O_0 = attn.forward_basic(X, 0)\n",
    "O_1 = attn.forward_basic(X, 1)\n",
    "print(O_0)\n",
    "print(O_1)\n",
    "\n",
    "# 其输出，不带 grad_fn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5fe41c-ee27-44ba-bcff-4b120cc49edc",
   "metadata": {},
   "source": [
    "## 注意力分数归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a08220d-f3cd-4425-93c5-afc9d67cfaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3505, -1.6537,  2.7238,  0.4472,  1.1096, -0.1954, -2.0742,  0.7644]])\n",
      "tensor([[0.0578, 0.0078, 0.6209, 0.0637, 0.1236, 0.0335, 0.0051, 0.0875]])\n",
      "tensor(1.)\n",
      "tensor([0.0578, 0.0078, 0.6209, 0.0637, 0.1236, 0.0335, 0.0051, 0.0875])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "Q_i = torch.randn(1, dim)\n",
    "K = torch.randn(seq_len, dim)\n",
    "V = torch.randn(seq_len, dim)\n",
    "\n",
    "S = Q_i @ K.t()\n",
    "print(S)\n",
    "\n",
    "# 归一化处理\n",
    "P = F.softmax(S, dim = -1)\n",
    "print(P)\n",
    "print(P.sum())\n",
    "\n",
    "O = P @ V\n",
    "\n",
    "# 归一化处理 softmax\n",
    "def softmax(X):\n",
    "    m = torch.max(X)\n",
    "    X_exp = torch.exp(X - m)\n",
    "    L = torch.sum(X_exp)\n",
    "    P = X_exp / L\n",
    "    return P\n",
    "\n",
    "P = softmax(S[0,:])\n",
    "print(P)\n",
    "print(P.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7aec3-017d-4b7e-aa38-8774b2c60d5d",
   "metadata": {},
   "source": [
    "## 掩码注意力\n",
    "\n",
    "我们在之前的例子中，有注意力分数：\n",
    "\n",
    "|                | 小   | 冬   | 瓜   | 有   | 两   | 把   | 刷   | 子   |\n",
    "| -------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| $w_刷$         | 0.0  | 0.1  | 0.1  | 0.4  | 0.0  | 0.0  | 0.3  | 0.1  |\n",
    "\n",
    "如果我们人为定义规则，要求不能访问“奇数位置”的键门，即对“K_j, j%2 = 1” 的门进行上锁。\n",
    "\n",
    "|                | 小   | 冬   | 瓜   | 有   | 两   | 把   | 刷   | 子   |\n",
    "| -------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| $w_刷$         | 0.0  | 0.1  | 0.1  | 0.4  | 0.0  | 0.0  | 0.3  | 0.1  |\n",
    "| $\\text{mask}_刷$         | 1  | 0  | 1  | 0 | 1  | 0  | 1  | 0  |\n",
    "\n",
    "如果不能开键（key）门, 那么也不能访问内容（value）, 同理我们对每个 token 都有独立的 $\\text{mask}_i$ 锁\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eff67-5dd7-463f-9416-0b2083c96bf8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "S_i = \\sum_j \\textcolor{red}{\\text{mask}_{ij}} w_{ij} X_j,\\\\\n",
    "w_{ij} = X_i X_j^T\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1944a0f8-bb2c-488f-a0b6-29ece515e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 5, 7])\n",
      "tensor([[0., 1., 0., 1., 0., 1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.zeros(1,seq_len)\n",
    "idx = torch.arange(1, seq_len, 2)\n",
    "print(idx)\n",
    "mask[0, idx] = 1\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d3afbc-269b-4637-8855-f4c914096903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0627,  0.9994,  0.7831,  0.2163, -2.1494,  1.6317,  1.8986,  1.0182]])\n",
      "tensor([[-0.0627,  0.9994,  0.7831,  0.2163, -2.1494,  1.6317,  1.8986,  1.0182]])\n",
      "tensor([[0.0631, 0.1715, 0.0631, 0.0784, 0.0631, 0.3228, 0.0631, 0.1748]])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 掩码注意力\n",
    "Q_i = torch.randn(1, dim)\n",
    "K = torch.randn(seq_len, dim)\n",
    "V = torch.randn(seq_len, dim)\n",
    "\n",
    "S = Q_i @ K.t()\n",
    "print(S)\n",
    "\n",
    "# 增加mask\n",
    "S_mask = S * mask\n",
    "print(S)\n",
    "\n",
    "# 归一化处理\n",
    "P = F.softmax(S_mask, dim = -1)\n",
    "print(P) # 有问题, mask 掉的 token 仍有 访问权重\n",
    "print(P.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287b23d1-05ee-4e81-aada-904700709642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6])\n",
      "tensor([[-1.0000e+04,  9.9941e-01, -1.0000e+04,  2.1627e-01, -1.0000e+04,\n",
      "          1.6317e+00, -1.0000e+04,  1.0182e+00]])\n",
      "tensor([[0.0000, 0.2295, 0.0000, 0.1049, 0.0000, 0.4318, 0.0000, 0.2338]])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "idx = torch.arange(0, seq_len, 2)\n",
    "print(idx)\n",
    "\n",
    "# 增加mask\n",
    "S_inf_mask = S.clone()\n",
    "S_inf_mask[0,idx] = -10000.0 # 在 mask 掉的分数置为 负无穷($-\\infty$)\n",
    "print(S_inf_mask)\n",
    "\n",
    "# 归一化处理\n",
    "P = F.softmax(S_inf_mask, dim = -1)\n",
    "print(P) # 没问题\n",
    "print(P.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce07f0-4d21-4508-aad0-74db8df4f44f",
   "metadata": {},
   "source": [
    "## 点积缩放注意力(ScaledDotProductAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9944e923-a937-4672-8a0d-5e36935e7cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 3]),\n",
       " tensor([1, 2, 3, 4, 2, 3, 4, 3, 4, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(1,5,5))\n",
    "print(mask)\n",
    "torch.where(mask==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b2606c-0dbe-4bf6-a08a-39df379150ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0664,  1.7814,  2.5556, -0.5928, -0.5491])\n",
      "tensor([[ 1.9017,  0.1047,  3.4827, -2.1984, -0.7388],\n",
      "        [-0.8771,  0.1031, -1.4690, -0.4169,  0.3927],\n",
      "        [ 2.2289, -1.5629, -4.0477, -0.0432,  2.9467],\n",
      "        [-0.4769,  0.6820, -0.7752, -0.0132,  0.5000],\n",
      "        [ 0.0664,  1.7814,  2.5556, -0.5928, -0.5491]])\n",
      "tensor([[[ 1.9017,  0.1047,  3.4827, -2.1984, -0.7388],\n",
      "         [-0.8771,  0.1031, -1.4690, -0.4169,  0.3927],\n",
      "         [ 2.2289, -1.5629, -4.0477, -0.0432,  2.9467],\n",
      "         [-0.4769,  0.6820, -0.7752, -0.0132,  0.5000],\n",
      "         [ 0.0664,  1.7814,  2.5556, -0.5928, -0.5491]],\n",
      "\n",
      "        [[ 4.3100,  2.1476, -2.4445, -0.3642, -0.8997],\n",
      "         [ 2.0166,  0.8142, -0.0494,  0.3950,  0.1620],\n",
      "         [-2.8694, -4.4273, -0.4835, -1.7465, -1.0318],\n",
      "         [-1.9456, -2.2504, -0.9859, -1.3685, -0.8618],\n",
      "         [ 3.3065,  5.8300, -1.3517,  1.0775,  0.5839]]])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.randn(2, 5, dim) # 1 is batchsize\n",
    "K = torch.randn(2, 5, dim)\n",
    "V = torch.randn(2, 5, dim)\n",
    "\n",
    "S = Q[0, -1, :] @ K[0, :, :].transpose(0, 1) # batchsize 1, 最后一个 token 作为检索\n",
    "print(S)\n",
    "\n",
    "S = Q[0, :, :] @ K[0, :, :].transpose(0, 1) # 序列做 attention,\n",
    "print(S)\n",
    "\n",
    "S = Q @ K.transpose(1, 2) # 序列做 attention, batch 并行\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d10e869a-8abe-4460-80e1-ce740d046f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "## 完整注意力\n",
    "import math\n",
    "\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(dim_in, dim_out)\n",
    "        self.WK = nn.Linear(dim_in, dim_out)\n",
    "        self.WV = nn.Linear(dim_in, dim_out)\n",
    "        self.WO = nn.Linear(dim_in, dim_out) \n",
    "        \n",
    "    def forward(self, X, mask = None):\n",
    "        batch_size, seq_len, dim = X.shape\n",
    "        Q = self.WQ(X)\n",
    "        K = self.WK(X)\n",
    "        V = self.WV(X)\n",
    "\n",
    "        # 多个 q_i 计算注意力特征\n",
    "        S = Q @ K.transpose(1,2) / math.sqrt(dim) # 1. 为什么要除于 \\sqrt{d}\n",
    "\n",
    "        if mask is not None:\n",
    "            idx =torch.where(mask==0)\n",
    "            S[idx[0],idx[1],idx[2]] = -10000.0\n",
    "        \n",
    "        P = torch.softmax(S, dim = -1) # 行 softmax\n",
    "        Z = P @ V\n",
    "        output = self.WO(Z)\n",
    "        \n",
    "        return output\n",
    "\n",
    "X = torch.randn(64, seq_len, dim)\n",
    "# mask = torch.tril(torch.ones(64, seq_len, dim))\n",
    "mask = torch.ones(64, seq_len, dim)\n",
    "model = ScaleDotProductAttention(dim, dim)\n",
    "Y = model(X, mask)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d6ddfce-d54d-4f76-8c1e-5d6fc48f7e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionClassifyModel(\n",
      "  (E): Embedding(26, 512)\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (head): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 实例：基于 Attention 的文本分类\n",
    "import torch.optim as optim\n",
    "\n",
    "# 数据\n",
    "bs = 32\n",
    "seq_len = 100\n",
    "dim = 512\n",
    "vocab_size = 26\n",
    "class_num = 3 # 情感分类: negative, neutral, positive\n",
    "\n",
    "\n",
    "class AttentionClassifyModel(nn.Module):\n",
    "    def __init__(self, dim = 512, vocab_size = 100, class_num = 2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.class_num = class_num\n",
    "        self.E = nn.Embedding(vocab_size, dim)\n",
    "        self.attention = ScaleDotProductAttention(dim, dim)\n",
    "        self.head = nn.Linear(dim, class_num)\n",
    "        \n",
    "    def forward(self, X, mask):\n",
    "        bs,seq_len = X.shape\n",
    "        X = self.E(X)\n",
    "        X = self.attention(X, mask)\n",
    "        h = X.mean(dim = 1) # sequence dimention, mean pooling\n",
    "        Y = self.head(h)\n",
    "        return Y # logits\n",
    "        \n",
    "input_ids = torch.randint(0, vocab_size, size=(bs, seq_len)) # 语料\n",
    "Y = torch.randint(0, class_num, size=(1, bs))[0]\n",
    "\n",
    "model = AttentionClassifyModel(dim, vocab_size, class_num)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60a2fa4d-dffb-4aa5-8948-afb9919d34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0038, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9579, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8850, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7633, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4109, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2699, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1797, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1213, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    Y_pred = model(input_ids, None)\n",
    "\n",
    "    loss = loss_fn(Y_pred, Y)   \n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64112625-048a-4cfb-a716-6fedc7683b28",
   "metadata": {},
   "source": [
    "## 实例2: 基于 Attention 的词元预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2985ceaa-af0c-4262-ba3c-db66cb1944f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18,  8,  7,  ...,  7, 12,  9],\n",
      "        [21, 12,  2,  ..., 10,  2, 24],\n",
      "        [ 0, 23, 17,  ..., 20, 20,  1],\n",
      "        ...,\n",
      "        [ 2,  1,  3,  ...,  7,  2, 22],\n",
      "        [ 9, 23, 15,  ..., 24, 16, 13],\n",
      "        [16, 16, 21,  ...,  4, 11,  6]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8,  7, 11,  ..., 12,  9, 21],\n",
       "        [12,  2, 22,  ...,  2, 24,  0],\n",
       "        [23, 17,  4,  ..., 20,  1, 23],\n",
       "        ...,\n",
       "        [ 1,  3, 20,  ...,  2, 22,  9],\n",
       "        [23, 15, 22,  ..., 16, 13, 16],\n",
       "        [16, 21, 23,  ..., 11,  6, 18]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(0, vocab_size, size=(bs, seq_len)) # 语料\n",
    "print(input_ids)\n",
    "torch.roll(input_ids, shifts = -1) # input seq维度左移动一位作为 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce1f8606-662d-47d8-b110-d3690a4732c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionLanguageModel(\n",
      "  (E): Embedding(26, 512)\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (head): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 实例2: 基于 Attention 的词元预测\n",
    "import torch.optim as optim\n",
    "\n",
    "# 数据\n",
    "bs = 32\n",
    "seq_len = 100\n",
    "dim = 512\n",
    "vocab_size = 26\n",
    "class_num = 3 # 情感分类: negative, neutral, positive\n",
    "\n",
    "\n",
    "class AttentionLanguageModel(nn.Module):\n",
    "    def __init__(self, dim = 512, vocab_size = 100):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.class_num = class_num\n",
    "        self.E = nn.Embedding(vocab_size, dim)\n",
    "        self.attention = ScaleDotProductAttention(dim, dim)\n",
    "        self.head = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "    def forward(self, X, mask):\n",
    "        bs,seq_len = X.shape\n",
    "        X = self.E(X)\n",
    "        X = self.attention(X, mask)\n",
    "        # h = X.mean(dim = 1) # sequence dimention, mean pooling\n",
    "        Y = self.head(X)\n",
    "        return Y # logits\n",
    "        \n",
    "input_ids = torch.randint(0, vocab_size, size=(bs, seq_len)) # 语料\n",
    "Y = torch.roll(input_ids.clone(), shifts = -1)\n",
    "\n",
    "model = AttentionLanguageModel(dim, vocab_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf6f6bdd-8caf-4190-9db9-ab4a4bc98fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100, 26])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(bs, seq_len, seq_len))\n",
    "Y_pred = model(input_ids, mask)\n",
    "print(input_ids.shape)\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d12a41b-1628-4351-969a-2f4a72b185a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2611, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2556, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2516, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2482, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2452, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2423, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2394, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2364, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2332, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2298, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    Y_pred = model(input_ids, mask)\n",
    "\n",
    "    loss = loss_fn(Y_pred.view(bs*seq_len, vocab_size), \n",
    "                   Y.view(bs*seq_len), ) \n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7421b7a-e86e-46b2-84ce-2ea4746699d8",
   "metadata": {},
   "source": [
    "## 多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42dc7d-47df-4bd4-9a7b-159e6fa7b55a",
   "metadata": {},
   "source": [
    "注意力权重可能是稀疏的（如 seq_len 为 1024）, 可能有极少数的 token 有 重要权重\n",
    "\n",
    "这会导致有信息发生遗漏\n",
    "\n",
    "如何可以更多样的注意力关系？\n",
    "\n",
    "- 注意力权重，是关心分类任务的\n",
    "- 注意力权重，是关心语法分析的\n",
    "\n",
    "以下示例，就扩展了 **单头** 注意力范围\n",
    "\n",
    "|                | 小   | 冬   | 瓜   | 有   | 两   | 把   | 刷   | 子   |\n",
    "| -------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| $w^{(1)}_刷$    | 0.0  | 0.0  | 0.0  | 0.0  | 0.2  | 0.4  | 0.3  | 0.1  |\n",
    "| $w^{(2)}_刷$    | 0.1  | 0.05  | 0.4.5  | 0.1  | 0.2  | 0.0  | 0.1  | 0.0  |\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "S^{(h)}_i = \\sum_j w^{(h)}_{ij} {V_j^{(h)}}^T,\\\\\n",
    "w^{(h)}_{ij} = Q^{(h)}_i {K^{(h)}_j}^T\n",
    "\\end{align}\n",
    "\n",
    "其中, $ \\texttt{Split}(Q) \\rightarrow Q^{(1)},Q^{(2)}\\ldots Q^{(H)} \\in \\mathbb {R} ^ {N \\times d'}, d' = d / H $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d9687c-6069-4897-9e14-3096420a4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 5\n",
    "dim = 512\n",
    "heads = 8\n",
    "head_dim = dim // heads\n",
    "\n",
    "Q = torch.randn(bs, seq_len, dim)\n",
    "K = torch.randn(bs, seq_len, dim)\n",
    "V = torch.randn(bs, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e749fc1-8197-4128-a900-b491aed860ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# single head\n",
    "\n",
    "S = Q @ K.transpose(1,2) / math.sqrt(dim)\n",
    "P = torch.softmax(S, dim = -1)\n",
    "Z = P @ V\n",
    "\n",
    "print(S.shape)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7510751d-db88-4cc4-adc6-479999fc84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n",
      "64\n",
      "torch.Size([2, 5, 8, 64])\n",
      "torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 5])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# multi head 切分，尝试单头版本\n",
    "print(Q.shape)\n",
    "Q_h = Q.view(bs, seq_len, heads, head_dim)\n",
    "K_h = K.view(bs, seq_len, heads, head_dim)\n",
    "V_h = V.view(bs, seq_len, heads, head_dim)\n",
    "print(head_dim)\n",
    "print(Q_h.shape)\n",
    "\n",
    "\n",
    "Q_0 = Q_h[:, :, 0, :] # head 0\n",
    "K_0 = K_h[:, :, 0, :] # head 0\n",
    "V_0 = V_h[:, :, 0, :] # head 0\n",
    "print(Q_0.shape)\n",
    "\n",
    "S = Q_0 @ K_0.transpose(1,2) / math.sqrt( head_dim ) # 注意除单头的维度\n",
    "P = torch.softmax(S, dim = -1)\n",
    "Z_0 = P @ V_0\n",
    "print(S.shape)\n",
    "print(Z.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "210a22a7-dba2-4398-b86a-33cd82e09e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8, 64])\n",
      "torch.Size([2, 8, 5, 64])\n",
      "torch.Size([2, 8, 64, 5])\n",
      "torch.Size([2, 8, 5, 5])\n",
      "torch.Size([2, 8, 5, 64])\n",
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# multi head, 头并行\n",
    "print(Q_h.shape)\n",
    "print(Q_h.transpose(1,2).shape) # 这个变换后两个维度 为 seq, dim, 要把 head 提前处理\n",
    "print(K_h.transpose(1,2).transpose(2,3).shape)\n",
    "\n",
    "Q_h = Q_h.transpose(1,2)\n",
    "K_h = K_h.transpose(1,2)\n",
    "V_h = V_h.transpose(1,2)\n",
    "\n",
    "S = Q_h @ K_h.transpose(2,3) / math.sqrt( head_dim ) # 注意除单头的维度\n",
    "P = torch.softmax(S, dim = -1)\n",
    "Z = P @ V_h\n",
    "print(S.shape)\n",
    "print(Z.shape)\n",
    "\n",
    "\n",
    "Z = Z.transpose(1,2).reshape(bs, seq_len, dim)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de294485-fd3d-4d2d-96b0-8fe665e899d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "## 多头注意力实现\n",
    "import math\n",
    "\n",
    "class MultiHeadScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, heads = 8):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(dim_in, dim_out)\n",
    "        self.WK = nn.Linear(dim_in, dim_out)\n",
    "        self.WV = nn.Linear(dim_in, dim_out)\n",
    "        self.WO = nn.Linear(dim_in, dim_out)\n",
    "        self.heads = 8\n",
    "        self.head_dim = dim_out // self.heads\n",
    "        \n",
    "    def forward(self, X, mask = None):\n",
    "        batch_size, seq_len, dim = X.shape\n",
    "        Q = self.WQ(X)\n",
    "        K = self.WK(X)\n",
    "        V = self.WV(X)\n",
    "\n",
    "        # 拆分维度\n",
    "        Q_h = Q.view(bs, seq_len, self.heads, self.head_dim).transpose(1,2)\n",
    "        K_h = K.view(bs, seq_len, self.heads, self.head_dim).transpose(1,2)\n",
    "        V_h = V.view(bs, seq_len, self.heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # 多个 q_i 计算注意力特征\n",
    "        S = Q_h @ K_h.transpose(2,3) / math.sqrt(self.head_dim) # 1. 为什么要除于 \\sqrt{d}\n",
    "\n",
    "        # 请判断以下 mask 代码是否正确?\n",
    "        if mask is not None:\n",
    "            idx = torch.where(mask==0)\n",
    "            S[:, idx[0],idx[1],idx[2]] = -10000.0\n",
    "        \n",
    "        P = torch.softmax(S, dim = -1) # 行 softmax\n",
    "        Z = P @ V_h\n",
    "\n",
    "        # 恢复维度\n",
    "        Z = Z.transpose(1,2).reshape(bs, seq_len, dim)\n",
    "        \n",
    "        output = self.WO(Z)\n",
    "        \n",
    "        return output\n",
    "\n",
    "X = torch.randn(2, seq_len, dim)\n",
    "mask = torch.ones(2, seq_len, dim)\n",
    "model = MultiHeadScaleDotProductAttention(dim, dim, 8)\n",
    "Y = model(X, mask)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b9937ab-0fcb-40a4-9927-cf4ac4eb8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3580, -2.1688,  0.4910,  0.0000,  0.0000],\n",
       "         [ 1.6233,  0.3894, -0.1632, -0.0000,  0.0000],\n",
       "         [-0.2142, -0.2808, -0.7808,  0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.2749, -1.1813, -0.0000, -0.0000, -0.0000],\n",
       "         [-0.4358,  1.2782, -0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([[1, 2, 3, 0, 0],\n",
    "                          [1, 2, 0, 0, 0]], dtype = torch.long) # 0 is pad\n",
    "bs, seq_len = input_ids.shape\n",
    "mask = torch.ones(bs, seq_len, seq_len)\n",
    "\n",
    "for i in range(bs):\n",
    "    pad_idx =  torch.where(input_ids[i, :]  == 0)[0]\n",
    "    mask[i, pad_idx, :] = 0\n",
    "    mask[i, :, pad_idx] = 0\n",
    "print(mask)\n",
    "\n",
    "score = torch.randn(bs, seq_len, seq_len)\n",
    "score * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f9818-2793-40dc-9c83-d6482c6bef67",
   "metadata": {},
   "source": [
    "## 实例3: 基于multi-head-Attention 的词元预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03983ef0-e562-4ee5-adeb-c0221962b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionLanguageModel(\n",
      "  (E): Embedding(26, 512)\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WK): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WV): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (WO): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (head): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 数据\n",
    "bs = 32\n",
    "seq_len = 100\n",
    "dim = 512\n",
    "vocab_size = 26\n",
    "class_num = 3 # 情感分类: negative, neutral, positive\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLanguageModel(nn.Module):\n",
    "    def __init__(self, dim = 512, vocab_size = 100):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.class_num = class_num\n",
    "        self.E = nn.Embedding(vocab_size, dim)\n",
    "        self.attention = MultiHeadScaleDotProductAttention(dim, dim) # 仅改变这个函数\n",
    "        self.head = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "    def forward(self, X, mask):\n",
    "        bs,seq_len = X.shape\n",
    "        X = self.E(X)\n",
    "        X = self.attention(X, mask)\n",
    "        Y = self.head(X)\n",
    "        return Y # logits\n",
    "\n",
    "input_ids = torch.randint(0, vocab_size, size=(bs, seq_len)) # 语料\n",
    "Y = torch.roll(input_ids, shifts = -1) # input seq维度左移动一位作为 label\n",
    "\n",
    "model = AttentionLanguageModel(dim, vocab_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "569c668b-3044-4fb3-a08e-523659d8219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2587, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2563, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2544, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2528, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2514, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2501, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2488, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2474, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2459, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2444, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    Y_pred = model(input_ids, mask)\n",
    "\n",
    "    loss = loss_fn(Y_pred.view(bs*seq_len, vocab_size), \n",
    "                   Y.view(bs*seq_len), ) \n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb437508-9a95-4b2e-a3f0-2e4dae5a6f7b",
   "metadata": {},
   "source": [
    "## MultiHeadAttention Backward(*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc02c59-7f61-4342-add4-fea5209e158f",
   "metadata": {},
   "source": [
    "### 单头版本 pytorch 自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3125b2f7-2026-4f8c-9664-fdc084b64521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(2, 3, requires_grad = True) #  seq_len, dim \n",
    "X.retain_grad()\n",
    "\n",
    "Wq = torch.randn(3, 4, requires_grad = True)\n",
    "Wk = torch.randn(3, 4, requires_grad = True)\n",
    "Wv = torch.randn(3, 4, requires_grad = True)\n",
    "Wo = torch.randn(4, 3, requires_grad = True)\n",
    "\n",
    "Q = X @ Wq\n",
    "K = X @ Wk\n",
    "V = X @ Wv\n",
    "Q.retain_grad()\n",
    "K.retain_grad()\n",
    "V.retain_grad()\n",
    "\n",
    "S = Q @ K.t() / math.sqrt(4)\n",
    "S.retain_grad()\n",
    "\n",
    "P = F.softmax(S, dim = -1)\n",
    "P.retain_grad()\n",
    "\n",
    "Z = P @ V\n",
    "Z.retain_grad()\n",
    "print(Z.shape)\n",
    "\n",
    "O = Z @ Wo\n",
    "O.retain_grad()\n",
    "print(O.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f36eed64-de3a-434a-9dcb-690102b69fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1211, grad_fn=<MseLossBackward0>)\n",
      "tensor([[-2.1932, -2.0092, -3.4629],\n",
      "        [ 9.5022,  1.1678, -5.1825]])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.randn(2, 3)\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(O, Y)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "724a6cac-6794-45be-ae0a-584974eb7b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3]) torch.Size([2, 3]) torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "## 单头版本 手动 求导\n",
    "\n",
    "dO = (1/O.numel()) * 2 * (O - Y) # 2, 3, 4\n",
    "print(Wo.shape, dO.shape, Z.shape)\n",
    "# O = Z @ Wo\n",
    "\n",
    "dWo = (dO.t() @ Z)\n",
    "dZ = dO @ Wo.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "263fb0d2-2531-457b-94ea-ba64001252f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dP = dZ @ V.t()\n",
    "dV = P.t() @ dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3205c54c-eb09-49dd-99eb-82774bc21008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[ 0.0178, -0.0178],\n",
      "        [ 3.0729, -3.0729]], grad_fn=<CopySlices>)\n",
      "tensor([[ 0.0178, -0.0178],\n",
      "        [ 3.0729, -3.0729]])\n"
     ]
    }
   ],
   "source": [
    "dS = torch.zeros_like(dP)\n",
    "\n",
    "for i in range(2):\n",
    "    dP_dS_i = torch.diag(P[i,:]) - torch.outer(P[i,:] , P[i,:])\n",
    "    dS[i,:] = dP[i,:] @ dP_dS_i\n",
    "print(dS.shape)\n",
    "\n",
    "dS = dS \n",
    "print(dS)\n",
    "print(S.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "160012e0-756d-4e2a-b25e-1ba3df1ac6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q @ K.t() = 2,4\n",
    "# Q 2x3\n",
    "# K 4x3, K.t()= 3x4\n",
    "# S = 2x4\n",
    "\n",
    "dQ = dS @ K / math.sqrt(4)\n",
    "dK = dS.t() @ Q /math.sqrt(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a1b8abc-eb9a-4df7-8875-dc664948ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0132, -0.0165, -0.0147,  0.0300],\n",
      "        [ 2.2869, -2.8550, -2.5510,  5.1993]], grad_fn=<DivBackward0>) tensor([[ 0.0132, -0.0165, -0.0147,  0.0300],\n",
      "        [ 2.2869, -2.8550, -2.5510,  5.1993]])\n",
      "tensor([[ 0.5142, -0.0055, -0.3759, -0.1285],\n",
      "        [-0.5142,  0.0055,  0.3759,  0.1285]], grad_fn=<DivBackward0>) tensor([[ 0.5142, -0.0055, -0.3759, -0.1285],\n",
      "        [-0.5142,  0.0055,  0.3759,  0.1285]])\n",
      "tensor([[-1.8593,  0.9742, -1.8910,  1.3738],\n",
      "        [-2.4568,  1.3461, -0.9550,  0.2883]], grad_fn=<MmBackward0>) tensor([[-1.8593,  0.9742, -1.8910,  1.3738],\n",
      "        [-2.4568,  1.3461, -0.9550,  0.2883]])\n"
     ]
    }
   ],
   "source": [
    "print(dQ, Q.grad)\n",
    "print(dK, K.grad)\n",
    "print(dV, V.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a612d68-03cb-4dc8-89a7-578bb5604d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1932, -2.0092, -3.4629],\n",
      "        [ 9.5022,  1.1678, -5.1825]], grad_fn=<AddBackward0>)\n",
      "tensor([[-2.1932, -2.0092, -3.4629],\n",
      "        [ 9.5022,  1.1678, -5.1825]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dX_dQ = dQ @ Wq.t()\n",
    "dX_dK = dK @ Wk.t()\n",
    "dX_dV = dV @ Wv.t()\n",
    "dX = dX_dQ + dX_dK + dX_dV\n",
    "print(dX)\n",
    "print(dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da156560-587e-438c-9f14-18e4221cd3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1109,  0.1385,  0.1237, -0.2522],\n",
      "        [ 1.1945, -1.4913, -1.3325,  2.7158],\n",
      "        [-0.2443,  0.3049,  0.2725, -0.5553]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.1109,  0.1385,  0.1237, -0.2522],\n",
      "        [ 1.1945, -1.4913, -1.3325,  2.7158],\n",
      "        [-0.2443,  0.3049,  0.2725, -0.5553]])\n"
     ]
    }
   ],
   "source": [
    "dWq = X.t() @ dQ\n",
    "dWk = X.t() @ dK\n",
    "dWv = X.t() @ dV\n",
    "print(dWq)\n",
    "print(Wq.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b27180-3df9-4388-b43d-b8394ccabc4d",
   "metadata": {},
   "source": [
    "### 多头版本多batch，注意力梯度求导（*）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4138295d-d2d8-43d6-b97e-129b34fc130c",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "\n",
    "仿照 `Trasformers` 库实现数据处理流程，具体包含：\n",
    "\n",
    "原始数据 是用 `list` 或 `dict` 组织的:\n",
    "\n",
    "```text\n",
    "List{\n",
    "    {'input': '天气很好', 'label': 1},\n",
    "    {'input': '你好, 'label': 0},\n",
    "    ...\n",
    "}\n",
    "```\n",
    "或\n",
    "```text\n",
    "Dict{\n",
    "    'input': ['天气很好', '你好', ...],\n",
    "    'label': [1, 0, ...],\n",
    "}\n",
    "```\n",
    "\n",
    "Dataset 的目标 是为了能够获取 batch token-id 序列：\n",
    "\n",
    "1. 一批数据里的 token 序列是不等长的\n",
    "2. 需要截断数据或填充 `<PAD>`\n",
    "3. 考虑 shuffle 时，动态 padding\n",
    "4. 在训练过程中，获取批量数据的 `input_ids`, `attention_masks` 和 `label`\n",
    "5. Transformer 训练较为特殊的一点是：`label` 也是模型的输入，而不仅仅是用于 求 Loss。\n",
    "6. `Dict` 版本数据存储数据会更优化, `List`版本 会重复存储 多次 `input` 和 `label` 字段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff93f4-8d64-4c8a-8f74-1b2922e5176f",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "567b4122-6ea9-4566-85ce-bb2e59babf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_len = 6\n",
    "trg_len = 12\n",
    "src_vocab_size = 26\n",
    "trg_vocab_size = 26\n",
    "\n",
    "PAD_TOKEN_ID = 0\n",
    "SOS_TOKEN_ID = 1\n",
    "EOS_TOKEN_ID = 2\n",
    "UNK_TOKEN_ID = 3\n",
    "\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6ba2d-c21d-43f5-a92e-c57f0aa19f83",
   "metadata": {},
   "source": [
    "## 等长 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7abd96-e4e4-46c4-8d2e-0ff6fd90570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids':self.X[idx,:], 'label': self.Y[idx,:]}\n",
    "\n",
    "def create_dataset(N, src_len, trg_len, src_vocab_size, trg_vocab_size):\n",
    "    X = torch.randint(3, src_vocab_size, (N, src_len), dtype = torch.long) # id 0,1,2,3 为 special token\n",
    "    Y = torch.randint(3, trg_vocab_size, (N, trg_len), dtype = torch.long) \n",
    "    X[:, 0] = SOS_TOKEN_ID # <SOS>\n",
    "    X[:, src_len - 1] = EOS_TOKEN_ID # <PAD> or <EOS>\n",
    "    Y[:, 0] = SOS_TOKEN_ID # <SOS>\n",
    "    Y[:, trg_len - 1] = EOS_TOKEN_ID # <PAD> or <EOS>\n",
    "    dataset = Seq2SeqDataset(X, Y)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(N = 100, \n",
    "              src_len = src_len, \n",
    "              trg_len = trg_len,  \n",
    "              src_vocab_size = src_vocab_size, \n",
    "              trg_vocab_size = trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5136e799-c0bb-40b1-8ca1-f43757a3e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取 getitem 数据\n",
      "tensor([ 1, 17,  3, 24, 19,  2])\n",
      "tensor([ 1, 20, 16,  6, 12, 15, 24, 20, 14,  9, 11,  2])\n",
      "获取运算符 [] 数据\n",
      "tensor([ 1, 17,  3, 24, 19,  2])\n",
      "tensor([ 1, 20, 16,  6, 12, 15, 24, 20, 14,  9, 11,  2])\n",
      "获取批量数据\n",
      "tensor([[ 1, 17,  3, 24, 19,  2],\n",
      "        [ 1, 14,  4, 15, 24,  2],\n",
      "        [ 1, 25,  7, 12, 21,  2]])\n",
      "tensor([[ 1, 20, 16,  6, 12, 15, 24, 20, 14,  9, 11,  2],\n",
      "        [ 1,  8, 24, 18,  3, 23, 19, 10, 11,  7, 14,  2],\n",
      "        [ 1, 19, 22, 16, 13, 24, 22, 19, 15, 13,  4,  2]])\n"
     ]
    }
   ],
   "source": [
    "# 在处理过程中并没有 padding\n",
    "print('获取 getitem 数据')\n",
    "print(train_dataset.__getitem__(0)['input_ids'])\n",
    "print(train_dataset.__getitem__(0)['label'])\n",
    "\n",
    "# `[]` 运算符获取\n",
    "print('获取运算符 [] 数据')\n",
    "print(train_dataset[0]['input_ids'])\n",
    "print(train_dataset[0]['label'])\n",
    "\n",
    "# 批量获取 数据\n",
    "print('获取批量数据')\n",
    "print(train_dataset[0:3]['input_ids'])\n",
    "print(train_dataset[0:3]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d54cd4-80e7-4ab5-881f-1e3afbd5b2ec",
   "metadata": {},
   "source": [
    "## 变长序列 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f474e6-da08-4861-825c-eccce125c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Seq2SeqTransformersDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.data['input_ids'][idx], \n",
    "                'labels': self.data['labels'][idx]}\n",
    "\n",
    "def create_transformers_dataset(N, src_len, trg_len, src_vocab_size, trg_vocab_size):\n",
    "\n",
    "    raw_data = {'input_ids':[], 'labels':[]}\n",
    "    for i in range(N):\n",
    "        tmp_src_len = random.randint(3, src_len)\n",
    "        tmp_trg_len = random.randint(3, trg_len)\n",
    "        x = torch.randint(3, src_vocab_size, (1, tmp_src_len), dtype = torch.long)\n",
    "        y = torch.randint(3, trg_vocab_size, (1, tmp_trg_len), dtype = torch.long)\n",
    "        \n",
    "        x[:, 0] = SOS_TOKEN_ID # <SOS>\n",
    "        x[:, tmp_src_len - 1] = EOS_TOKEN_ID # <PAD> or <EOS>\n",
    "        y[:, 0] = SOS_TOKEN_ID # <SOS>\n",
    "        y[:, tmp_trg_len - 1] = EOS_TOKEN_ID # <PAD> or <EOS>\n",
    "        \n",
    "        raw_data['input_ids'].append(x)\n",
    "        raw_data['labels'].append(y)\n",
    "    \n",
    "    dataset = Seq2SeqTransformersDataset(raw_data)\n",
    "    return dataset\n",
    "\n",
    "src_len = 10\n",
    "trg_len = 20\n",
    "src_vocab_size = 26\n",
    "trg_vocab_size = 26\n",
    "\n",
    "train_dataset = create_transformers_dataset(N = 100, \n",
    "              src_len = src_len, \n",
    "              trg_len = trg_len,  \n",
    "              src_vocab_size = src_vocab_size, \n",
    "              trg_vocab_size = trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62415c25-3261-4fa2-8df1-051aa3c23781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取 getitem 数据\n",
      "{'input_ids': tensor([[ 1, 24, 16, 13, 12, 11,  2]]), 'labels': tensor([[ 1,  5, 19, 16,  2]])}\n",
      "获取运算符 [] 数据\n",
      "tensor([[ 1, 24, 16, 13, 12, 11,  2]])\n",
      "tensor([[ 1,  5, 19, 16,  2]])\n",
      "获取批量数据\n",
      "[tensor([[ 1, 24, 16, 13, 12, 11,  2]]), tensor([[ 1, 22, 11, 17, 20,  6,  2]]), tensor([[ 1, 11,  8, 18,  7,  2]])]\n",
      "[tensor([[ 1,  5, 19, 16,  2]]), tensor([[ 1, 22, 16, 11,  5, 15, 20, 24,  8, 10, 20,  2]]), tensor([[ 1,  8,  3, 11,  6, 18, 22, 24,  2]])]\n"
     ]
    }
   ],
   "source": [
    "# 在处理过程中并没有 padding\n",
    "print('获取 getitem 数据')\n",
    "print(train_dataset.__getitem__(0))\n",
    "\n",
    "# `[]` 运算符获取\n",
    "print('获取运算符 [] 数据')\n",
    "print(train_dataset[0]['input_ids'])\n",
    "print(train_dataset[0]['labels'])\n",
    "\n",
    "# 批量获取 数据\n",
    "print('获取批量数据')\n",
    "print(train_dataset[0:3]['input_ids'])\n",
    "print(train_dataset[0:3]['labels']) \n",
    "\n",
    "# src, trg 长度都不等长"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08ae87-8075-469f-803d-1e4f203694b6",
   "metadata": {},
   "source": [
    "## Dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53825b10-1e8c-4081-8c13-7d95fd80f68c",
   "metadata": {},
   "source": [
    "在训练过程中，从完整的数据集里，随机 shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f8bf62-c3b0-4f7e-adec-f39b90299e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[ 1,  8, 12,  6, 22,  3, 18, 11, 10,  2]]]), 'labels': tensor([[[ 1, 15, 12,  3, 13, 10,  7, 11, 11, 21, 20, 15, 16,  7, 25,  6, 18,\n",
      "          24,  2]]])}\n",
      "torch.Size([1, 1, 10])\n",
      "torch.Size([1, 1, 19])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                    batch_size = 1 ,       # 当调整 batch_size 就报错\n",
    "                    # collate_fn=collate_fn,\n",
    "                    pin_memory = True,\n",
    "                    shuffle = True)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd717983-eca4-4097-a85f-7938a0e62f23",
   "metadata": {},
   "source": [
    "## DataCollate\n",
    "\n",
    "将批数据 padding 处理成等长度，有策略\n",
    "\n",
    "- 'max_len' : 预设最大长度\n",
    "- 'longest' : 批量数据最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f87e6b-65c0-4c6c-9c8a-3ef6df809ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[ 1, 13,  2]]), 'labels': tensor([[ 1, 12, 17, 19, 12, 14, 24,  8, 11,  2]])}, {'input_ids': tensor([[1, 7, 2]]), 'labels': tensor([[ 1,  5,  6,  8, 19, 23,  3,  5, 20,  8, 16, 13, 25, 19,  6,  2]])}]\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch_data):\n",
    "    # print(batch_data)\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                    batch_size = 2 ,       # 当调整 batch_size 就报错\n",
    "                    collate_fn=collate_fn,\n",
    "                    pin_memory = True,\n",
    "                    shuffle = True)\n",
    "    \n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa64188-b6cf-47df-b2b9-42bbec6555cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1, 10, 22,  3,  4,  9,  2,  0,  0,  0],\n",
      "        [ 1,  5, 10, 15,  6,  7,  2,  0,  0,  0],\n",
      "        [ 1, 14,  5, 22, 11, 22, 16, 16, 21,  2],\n",
      "        [ 1, 20, 15, 18, 14, 23, 24,  2,  0,  0]]), 'input_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'label_ids': tensor([[ 1, 14,  3, 10, 14,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 14,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 17, 12,  4,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  8,  7, 25, 11, 15,  7, 12,  7, 19, 18,  4, 23,  3,  9,  2]]), 'label_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "def paddding_collate_fn(batch_data):\n",
    "\n",
    "    input_lens = []\n",
    "    label_lens = []\n",
    "    bs = len(batch_data)\n",
    "    for data in batch_data:\n",
    "        input_lens.append( data['input_ids'].shape[1] )\n",
    "        label_lens.append( data['labels'].shape[1] )\n",
    "\n",
    "    max_input_len = torch.max(torch.tensor(input_lens, dtype = torch.long))\n",
    "    max_label_len = torch.max(torch.tensor(label_lens, dtype = torch.long))\n",
    "\n",
    "    input_ids = torch.ones(bs, max_input_len, dtype = torch.long) * PAD_TOKEN_ID\n",
    "    input_attention_masks = torch.zeros(bs, max_input_len, dtype = torch.long) \n",
    "    label_ids = torch.ones(bs, max_label_len, dtype = torch.long) * PAD_TOKEN_ID\n",
    "    label_attention_masks = torch.zeros(bs, max_label_len, dtype = torch.long) \n",
    "\n",
    "    for i in range(bs):\n",
    "        input_ids[i, :input_lens[i]] = batch_data[i]['input_ids'][0, :input_lens[i]]\n",
    "        input_attention_masks[i, :input_lens[i]] = 1\n",
    "        \n",
    "        label_ids[i, :label_lens[i]] = batch_data[i]['labels'][0, :label_lens[i]]\n",
    "        label_attention_masks[i, :label_lens[i]] = 1\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'input_attention_mask': input_attention_masks,\n",
    "        'label_ids': label_ids,\n",
    "        'label_attention_mask': label_attention_masks,\n",
    "    }\n",
    "    \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                    batch_size = 4 ,       # 当调整 batch_size 就报错\n",
    "                    collate_fn=paddding_collate_fn,\n",
    "                    pin_memory = True,\n",
    "                    shuffle = True)\n",
    "    \n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04ba0d-d593-411c-a2fe-97ff3a7ccb40",
   "metadata": {},
   "source": [
    "## 训练细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2bc1ee-41d4-4fe8-945d-6dc3cd4c0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   7,    5,   18,    4,   22,   25,    5,    5,    3,   25,   13,   15,\n",
      "           16,    2, -100, -100],\n",
      "        [  10,    8,   18,   12,   23,   13,    5,    3,   20,    4,   12,   19,\n",
      "           17,    9,    5,    2],\n",
      "        [   6,   20,   14,   24,    3,   17,   10,   24,   20,    9,   14,    4,\n",
      "           17,    2, -100, -100],\n",
      "        [   6,   18,   16,   24,   25,    2, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    # 训练输入为\n",
    "    decoder_input_ids = batch['label_ids'][:, :-1]\n",
    "    decoder_attention_mask = batch['label_attention_mask'][:, :-1]\n",
    "\n",
    "    \n",
    "    label_for_loss = batch['label_ids'][:, 1:]\n",
    "    label_for_loss[ torch.where(label_for_loss == PAD_TOKEN_ID )  ] = IGNORE_INDEX\n",
    "\n",
    "    print(label_for_loss)\n",
    "    # loss_fn(logits, label_for_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd566f-fc92-41c0-87e8-7c3f8a0dba3d",
   "metadata": {},
   "source": [
    "## Attention Mask\n",
    "\n",
    "在 `paddding_collate_fn` 里，实际上存储了 mask 序列而不是矩阵，好处在于：\n",
    "\n",
    "1. 不占用过多显存\n",
    "2. 可以在计算 attention 前，即算即释放\n",
    "\n",
    "以下实现基于序列 mask 的 cross-attention-mask, 贴近 `Transformers` 库的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d7ac0e-f736-4d3d-b649-3d614637b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "def get_src_trg_mask(src_mask, trg_mask):\n",
    "    \"\"\"\n",
    "    trg 为检索, dim = 0\n",
    "    \"\"\"\n",
    "    bs, src_len = src_mask.shape\n",
    "    bs, trg_len = trg_mask.shape\n",
    "    \n",
    "    mask = torch.zeros(bs, trg_len, src_len)\n",
    "\n",
    "    for i in range(bs):\n",
    "        mask[i, :, :] = torch.outer(trg_mask[i,:], src_mask[i,:])\n",
    "    return mask\n",
    "\n",
    "src_mask = torch.tensor([[1, 1, 0, 0, 0],[1, 1, 1, 0, 0]])\n",
    "trg_mask = torch.tensor([[1, 1, 0,],[1, 1, 0]])\n",
    "\n",
    "mask = get_src_trg_mask(src_mask, trg_mask)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2953447d-d749-4967-b3fe-9976866f2402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18, 10])\n",
      "torch.Size([4, 8, 18, 10])\n",
      "torch.Size([4, 8, 18, 10])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    encoder_attention_mask = batch['input_attention_mask']\n",
    "    decoder_attention_mask = batch['label_attention_mask'][:, :-1]\n",
    "\n",
    "    # mask\n",
    "    mask = get_src_trg_mask(encoder_attention_mask, decoder_attention_mask)\n",
    "    print(mask.shape)\n",
    "    # print(mask)\n",
    "\n",
    "    bs, src_len = encoder_attention_mask.shape\n",
    "    bs, trg_len = decoder_attention_mask.shape\n",
    "\n",
    "    # multi-head attention score\n",
    "    heads = 8\n",
    "    dim = 64\n",
    "    K = torch.randn( bs, heads, src_len, dim)\n",
    "    Q = torch.randn( bs, heads, trg_len, dim)\n",
    "\n",
    "    S = Q @ K.transpose(2,3) / math.sqrt(dim)\n",
    "    print(S.shape)\n",
    "\n",
    "    multi_head_mask = mask.unsqueeze(dim = 1) # 扩展头维度，实现头并行 mask\n",
    "    S_mask = S * multi_head_mask\n",
    "    print(S_mask.shape)\n",
    "    \n",
    "    break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

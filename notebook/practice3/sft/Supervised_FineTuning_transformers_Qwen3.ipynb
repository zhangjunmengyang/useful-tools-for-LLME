{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dff2b5e-c3b0-467d-865d-3af3c890af49",
   "metadata": {},
   "source": [
    "# SFT Qwen3\n",
    "\n",
    "调包微调 Qwen3\n",
    "\n",
    "- model: Qwen3-0.6B\n",
    "- data: Alpaca\n",
    "- method: Full / QLoRA\n",
    "- platform: Colab\n",
    "\n",
    "基于微调后的模型做文本生成。\n",
    "\n",
    "huggingface 常用库：\n",
    "\n",
    "1. transformers: 提供基础的 model 封装、tokenizer、trainer，另外集成了第三方库便于做分布式训练（deepspeed、fsdp）、推理（vllm）等； huggingface 生态包含丰富的主流开源模型和数据集，用户可以上传下载模型和数据集。\n",
    "2. datasets: 封装数据集预处理方法、如用于 pretrained 的处理方法\n",
    "3. TRL：包含 post-train 相关的 trainer，SFT 也是 post-train 中的一种\n",
    "4. PEFT：参数高效微调，将常规的模型做一次封装，隐藏方法细节。使用起来与常规的model一样。\n",
    "5. accelerate：集成 deepspeed、fsdp、vllm 等分布式框架，隐藏分布式细节、缺点是难以改动\n",
    "\n",
    "使用库能加快开发效率，对于本 lecture 目标则是，掌握这些框架的实现方法，使得个人具备从零开发应用框架或 infra 框架的能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5d51c-8688-40dd-9937-942a8ca04156",
   "metadata": {},
   "source": [
    "## 官方 SFT 训练\n",
    "\n",
    "[TRL::SFT](https://huggingface.co/docs/trl/sft_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee898dd-500b-4916-a616-2f209f4782bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.1211803436279295, metrics={'train_runtime': 12.6362, 'train_samples_per_second': 1.583, 'train_steps_per_second': 0.791, 'total_flos': 12505752010752.0, 'train_loss': 2.1211803436279295})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from  trl import SFTConfig #https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig\n",
    "\n",
    "config = SFTConfig(\n",
    "    output_dir=\"output/qwen3_sft\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    max_length = 256,\n",
    "    max_steps = 10\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=\"Qwen/Qwen3-0.6B\",\n",
    "    args=config,\n",
    "    train_dataset=load_dataset(\"trl-lib/Capybara\", split=\"train\"),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83c2f9-4f66-4d47-ad4c-ca451addb292",
   "metadata": {},
   "source": [
    "## 手动 SFT 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2669b957-e343-4c1d-a200-54a4eba5c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-0.6B', \n",
    "                                          local_dir='~/.cache/huggingface/', # 如果可以直连 huggingface, 去除此行.\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen3-0.6B',\n",
    "                                             local_files_only=True, \n",
    "                                            dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8a91a-47c8-4afe-8459-4f11c0e9817a",
   "metadata": {},
   "source": [
    "# 数据类型\n",
    "\n",
    "1. Prompt\n",
    "2. Prompt-Completiont: 最常用\n",
    "3. Messages: 最通用\n",
    "\n",
    "掌握以下方法可以将常规 python 数据类型转化为 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4ac4e9-ce51-4b76-a712-490f4784df99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversation'],\n",
      "    num_rows: 3\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversation'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['conversation'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## dataset 初始化\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DEFINIED_SYSTEM_PROMPT='你是小冬瓜智能体,请安全详细回答用户 USER 的问题'\n",
    "messages_1=[    \n",
    "    {'role':'system', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'user', 'content':'什么是人工智能?'},\n",
    "    {'role':'assistant', 'content':'人工智能是让机器模拟人类思维的技术。'},\n",
    "]\n",
    "messages_2=[    \n",
    "    {'role':'system', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'user', 'content':'如何计算复利?'},\n",
    "    {'role':'assistant', 'content':'复利计算公式：本息和 = 本金 × (1 + 利率)^期数。'},\n",
    "]\n",
    "messages_3=[    \n",
    "    {'role':'system', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'user', 'content':'“哈基米”翻译成英文'},\n",
    "    {'role':'assistant', 'content':'“哈基米”翻译成英文通常是 \"Hakimi\"（人名音译）。'},\n",
    "]\n",
    "messages_list = [messages_1, messages_2, messages_3]\n",
    "\n",
    "hf_dataset = Dataset.from_dict(\n",
    "    {'conversation': messages_list}\n",
    ")\n",
    "print(hf_dataset)\n",
    "\n",
    "my_datasets = DatasetDict({\n",
    "    'train': hf_dataset,\n",
    "    'test': hf_dataset,\n",
    "})\n",
    "print(my_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8b325-aea9-43b0-90d6-cf7eed2ef38c",
   "metadata": {},
   "source": [
    "## 手动处理公开数据集\n",
    "\n",
    "alpaca 是公开数据集，它是 prompt-completion 类型数据，我们将利用库的函数将其转化为 SFT 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5cbc30-1feb-4e4d-98e1-0d5788209750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('tatsu-lab/alpaca',\n",
    "                      cache_dir=\"~/.cache/huggingface\",)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8c7a53-0cf5-4902-aab9-9666b0c88248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.'} \n",
      "\n",
      "instruction: What are the three primary colors? \n",
      "\n",
      "instruction: What are the three primary colors?\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][1],'\\n')\n",
    "print('instruction:', dataset['train'][1]['instruction'], '\\n')\n",
    "print('instruction:', dataset['train']['instruction'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ba132-957b-4707-8742-a3a675ca20bb",
   "metadata": {},
   "source": [
    "## 预处理流程\n",
    "\n",
    "本版本处理过程，主要遵循官方 chat_template 进行开发，使用公版的对话模版，好处在于大部分训练推理框架，都适配官方公版对话模版。\n",
    "\n",
    "1. 拼接 `instruction` 和 `input`\n",
    "2. 格式化处理数据\n",
    "3. tokenize 单条数据\n",
    "4. 手写 collate 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d72caa-86f0-4d7d-be33-79a70e5ba2d9",
   "metadata": {},
   "source": [
    "### 拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa05e0d1-d8de-4f50-8f9f-3f5181fc1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cat_inst_input(example):\n",
    "    example['prompt'] = example['instruction'] + example['input']\n",
    "    example['completion'] = example['output']\n",
    "    return example\n",
    "\n",
    "dataset_prompt_completion = dataset.map(map_cat_inst_input,\n",
    "                                        remove_columns=[\"instruction\", \"input\", \"output\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8a40af-626d-4212-a62a-72f659e97991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据: {'instruction': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)} \n",
      "\n",
      "map 数据: {'prompt': Value(dtype='string', id=None), 'completion': Value(dtype='string', id=None)} \n",
      "\n",
      "prompt: What are the three primary colors? \n",
      "\n",
      "completion: The three primary colors are red, blue, and yellow.\n"
     ]
    }
   ],
   "source": [
    "print('原数据:',dataset['train'].features,'\\n')\n",
    "print('map 数据:',dataset_prompt_completion['train'].features,'\\n')\n",
    "\n",
    "print('prompt:', dataset_prompt_completion['train'][1]['prompt'], '\\n')\n",
    "print('completion:', dataset_prompt_completion['train']['completion'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df26f29-94d6-468e-aa77-3b35c5f105dc",
   "metadata": {},
   "source": [
    "## 格式化\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13c692c-2820-4bf7-b3ff-68df6b884bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是小冬瓜智能体,请安全详细回答用户 USER 的问题<|im_end|>\\n<|im_start|>user\\n如何计算复利?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n复利计算公式：本息和 = 本金 × (1 + 利率)^期数。<|im_end|>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(hf_dataset[1]['conversation'], \n",
    "                              tokenize=False,\n",
    "                              add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f3f539-cbe9-4f3d-a0b6-82d0d32ef294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151644, 8948, 198, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 151645, 198, 151644, 872, 198, 100007, 100768, 58364, 59532, 30, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 58364, 59532, 100768, 110322, 5122, 21894, 22226, 33108, 284, 220, 114664, 24768, 320, 16, 488, 19468, 102, 95355, 29776, 22704, 8863, 1773, 151645, 198]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(hf_dataset[1]['conversation'], \n",
    "                              tokenize=True,\n",
    "                              add_generation_prompt=False)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2e6aba-b884-4cc8-8708-3e32c2226c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [151668, 271], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [151645], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('</think>\\n\\n'))\n",
    "print(tokenizer('<|im_end|>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909824a-23a4-47fb-8a19-27e4bc6e9b8d",
   "metadata": {},
   "source": [
    "## prompt-completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e82c0c-28d4-4e5e-9367-32ab17f4b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码无法运行, 可以重写函数映射字典\n",
    "# tokenizer.apply_chat_template(dataset_prompt_completion['train'][1], \n",
    "#                               tokenize=False,\n",
    "#                               add_generation_prompt=False)\n",
    "\n",
    "def map_apply_chat_template(example):\n",
    "    tmp_messages = [\n",
    "        {'role':'system', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "        {'role':'user', 'content':example['prompt']},\n",
    "        {'role':'assistant', 'content':example['completion']},\n",
    "    ]\n",
    "    example['text'] = tokenizer.apply_chat_template(tmp_messages,\n",
    "                                                   tokenize=False,)\n",
    "    return example\n",
    "\n",
    "dataset_chat = dataset_prompt_completion.map(map_apply_chat_template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e297368f-02ca-4e1d-a7aa-0122e120597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Give three tips for staying healthy.', 'completion': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n",
      "{'prompt': 'Give three tips for staying healthy.', 'completion': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': '<|im_start|>system\\n你是小冬瓜智能体,请安全详细回答用户 USER 的问题<|im_end|>\\n<|im_start|>user\\nGive three tips for staying healthy.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.<|im_end|>\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_prompt_completion['train'][0])\n",
    "print(dataset_chat['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e575e-6133-4695-8ae3-0ba8915f8eb7",
   "metadata": {},
   "source": [
    "### token_id datasets\n",
    "\n",
    "本 notebook 仅展示单条数据 tokenize 化，如何通过 map 函数对 batch 数据做 tokenize 提高编码效率？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa970766-d7e3-46cd-9884-fdfabcbed0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_token(example):\n",
    "    example['input_ids'] = tokenizer.encode(\n",
    "        example['text'],\n",
    "        return_tensors='pt',\n",
    "        # padding='longest',\n",
    "        # padding_side='left',\n",
    "        # max_length=1024,\n",
    "        # truction=True,\n",
    "    )[0]\n",
    "\n",
    "    seq_len = example['input_ids'].shape[0]\n",
    "\n",
    "    example['attention_mask'] = torch.ones(seq_len, dtype=torch.long)\n",
    "    \n",
    "    return example\n",
    "\n",
    "dataset_token = dataset_chat.map(map_to_token,\n",
    "                                 remove_columns=[\"prompt\", \"completion\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c63cbaf7-f619-440a-9822-fe02078e2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [151644, 8948, 198, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 151645, 198, 151644, 872, 198, 35127, 2326, 10414, 369, 19429, 9314, 13, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 16, 5142, 266, 264, 23831, 9968, 323, 1281, 2704, 311, 2924, 11260, 315, 25322, 323, 23880, 13, 715, 17, 13, 32818, 15502, 311, 2506, 697, 2487, 4541, 323, 3746, 13, 715, 18, 13, 2126, 3322, 6084, 323, 10306, 264, 12966, 6084, 9700, 13, 151645, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_token['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db1b35-6650-495e-a331-a062d557a92d",
   "metadata": {},
   "source": [
    "### 获取 label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1506ac-5a22-4778-b210-38159ae08a37",
   "metadata": {},
   "source": [
    "给定以下 assistant 方回复，思考 completion 的起始位置？\n",
    "```\n",
    "<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.<|im_end|>\\n'}\n",
    "```\n",
    "\n",
    "答案为:\n",
    "\n",
    "第一个 `<think>` 开始, 回答里包含的 `<think>\\n\\n</think>\\n\\n` 是一种特殊的回复内容，用于推理cot，RL章节涉及用法，初学不用深究，仅当成是一种回复模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa1b1dcf-2c0d-4686-9741-64b3136f9184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nA<|im_end|>\\n<|im_start|>assistant\\nB<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_messages=[\n",
    "    {'role':'system', 'content':'A'},\n",
    "    {'role':'assistant', 'content':'B'},\n",
    "]\n",
    "tokenizer.apply_chat_template(tmp_messages, \n",
    "                              tokenize=False, \n",
    "                              add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e526011-30ae-414d-b00b-70d256ed92aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [198, 151667, 271, 151668, 271], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [151667], 'attention_mask': [1]}\n",
      "{'input_ids': [151645], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('\\n<think>\\n\\n</think>\\n\\n'))\n",
    "print(tokenizer('<think>'))\n",
    "print(tokenizer('<|im_end|>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b90a194-6294-4ad6-87c1-5231de1ca206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "def find_completion_start_end(token_ids):\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for i in range(len(token_ids)-1, -1, -1):\n",
    "        if token_ids[i] == 151667:\n",
    "            start = i\n",
    "    end = len(token_ids)-1\n",
    "    return start, end\n",
    "\n",
    "tmp_token_ids = dataset_token['train'][0]['input_ids']\n",
    "start, end = find_completion_start_end( tmp_token_ids )\n",
    "print(tokenizer.decode(tmp_token_ids[start:end])) # 最后必须要有 <|im_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5d310b5-4fce-4c7b-8753-14e23422387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "def map_get_label(example):\n",
    "    example['input_ids'] = torch.tensor(example['input_ids'], dtype=torch.long)\n",
    "    example['attention_mask'] = torch.tensor(example['attention_mask'], dtype=torch.long)\n",
    "    seq_len = example['input_ids'].shape[0]\n",
    "    start, end = find_completion_start_end(example['input_ids'])\n",
    "    # example['labels'] = torch.ones(seq_len, dtype=torch.long) * -100\n",
    "    # example['labels'][start:end] = example['input_ids'][start:end]\n",
    "    \n",
    "    # example['labels'] = example['labels'].roll(shifts=-1) # label 左移一位\n",
    "    example['labels'] = torch.ones(seq_len, dtype=torch.long) * -100\n",
    "    example['labels'][start:end] = example['input_ids'][start:end]\n",
    "    \n",
    "    example['labels'] = example['labels'].roll(shifts=-1) # label 左移一位\n",
    "    return example\n",
    "\n",
    "dataset_sft = dataset_token.map(map_get_label,\n",
    "                                num_proc=32,# 多线程处理\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ae6412a-ea86-4d7b-b1b7-ac4462e3bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151644, 8948, 198, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 151645, 198, 151644, 872, 198, 35127, 2326, 10414, 369, 19429, 9314, 13, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 16, 5142, 266, 264, 23831, 9968, 323, 1281, 2704, 311, 2924, 11260, 315, 25322, 323, 23880, 13, 715, 17, 13, 32818, 15502, 311, 2506, 697, 2487, 4541, 323, 3746, 13, 715, 18, 13, 2126, 3322, 6084, 323, 10306, 264, 12966, 6084, 9700, 13, 151645, 198]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 271, 151668, 271, 16, 5142, 266, 264, 23831, 9968, 323, 1281, 2704, 311, 2924, 11260, 315, 25322, 323, 23880, 13, 715, 17, 13, 32818, 15502, 311, 2506, 697, 2487, 4541, 323, 3746, 13, 715, 18, 13, 2126, 3322, 6084, 323, 10306, 264, 12966, 6084, 9700, 13, 151645, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "input_ids = dataset_sft['train'][0]['input_ids']\n",
    "print(input_ids)\n",
    "attention_mask = dataset_sft['train'][0]['attention_mask']\n",
    "print(attention_mask)\n",
    "labels = dataset_sft['train'][0]['labels']\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5aa9cb-e175-472b-8df1-7f7282a0f58a",
   "metadata": {},
   "source": [
    "上述代码符合预期"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0c762-040a-4544-85e7-6dc85db11186",
   "metadata": {},
   "source": [
    "## filter\n",
    "\n",
    "去除过长的文本, 防止爆显存，同时可以避免过多 padding 导致，训练效率低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a495760c-95bf-4211-a53b-509d0ace6817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 50867\n",
      "    })\n",
      "})\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "config_max_len = 256\n",
    "dataset_sft_filter = dataset_sft.filter( lambda x: len(x[\"input_ids\"]) < config_max_len)\n",
    "print(dataset_sft)\n",
    "print(dataset_sft_filter)\n",
    "print(len(dataset_sft_filter['train'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad8aee-dae5-4246-836f-e763420ae947",
   "metadata": {},
   "source": [
    "### 使用 collate 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d83064-9efd-4717-9988-dfbe9e2dd597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198, 105043,  30709,  99949, 100857, 100168,  31914,\n",
      "             11,  14880,  99464, 100700, 102104,  20002,  13872,  43589,  86119,\n",
      "         151645,    198, 151644,    872,    198,  35127,   2326,  10414,    369,\n",
      "          19429,   9314,     13, 151645,    198, 151644,  77091,    198, 151667,\n",
      "            271, 151668,    271,     16,   5142,    266,    264,  23831,   9968,\n",
      "            323,   1281,   2704,    311,   2924,  11260,    315,  25322,    323,\n",
      "          23880,     13,    715,     17,     13,  32818,  15502,    311,   2506,\n",
      "            697,   2487,   4541,    323,   3746,     13,    715,     18,     13,\n",
      "           2126,   3322,   6084,    323,  10306,    264,  12966,   6084,   9700,\n",
      "             13, 151645,    198],\n",
      "        [151644,   8948,    198, 105043,  30709,  99949, 100857, 100168,  31914,\n",
      "             11,  14880,  99464, 100700, 102104,  20002,  13872,  43589,  86119,\n",
      "         151645,    198, 151644,    872,    198,   3838,    525,    279,   2326,\n",
      "           6028,   7987,     30, 151645,    198, 151644,  77091,    198, 151667,\n",
      "            271, 151668,    271,    785,   2326,   6028,   7987,    525,   2518,\n",
      "             11,   6303,     11,    323,  13753,     13, 151645,    198, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, TruncationStrategy\n",
    "\n",
    "# transfomrers 自带的 DataCollatorWithPadding 不适配 labels 的 padding\n",
    "# 解决方案 1: 去除 labels, 但是 loss 的计算仍要重新构造 labels\n",
    "# 解决方案 2: 继承 DataCollatorWithPadding 增加 labels 的 padding\n",
    "# 解决方案 3: 手动实现 Collator\n",
    "\n",
    "dataset_sft_not_labels = dataset_sft.remove_columns('labels')\n",
    "\n",
    "tokenizer.set_truncation_and_padding(\n",
    "    truncation_strategy=TruncationStrategy.ONLY_FIRST,\n",
    "    padding_strategy=PaddingStrategy.LONGEST,\n",
    "    max_length=512,\n",
    "    padding_side='right',\n",
    "    stride=1,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer, \n",
    "                                   return_tensors=\"pt\",\n",
    "                                   # max_length=512,\n",
    "                                   # padding=True,\n",
    "                                  )\n",
    "\n",
    "data_loader = DataLoader(dataset_sft_not_labels['train'], \n",
    "                    batch_size=2, \n",
    "                    collate_fn=collator, \n",
    "                    shuffle=False)\n",
    "\n",
    "for batch in data_loader:\n",
    "    print(batch['input_ids'])\n",
    "    # do train\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec0ab9c-f9ce-476e-9abc-d5d6586f56d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([151645])) # eos\n",
    "print(tokenizer.decode([151643])) # pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b3432-49de-4db0-8110-a8583cf6a8aa",
   "metadata": {},
   "source": [
    "### 手动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "189821c2-ed9b-4160-83fe-99630e7e3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    logits = model(input_ids = batch['input_ids'],\n",
    "          attention_mask = batch['attention_mask']).logits\n",
    "\n",
    "    # get labels\n",
    "\n",
    "    # get loss\n",
    "\n",
    "    # loss.backward\n",
    "    # optimizer.step\n",
    "    # optimizer.zero_grad()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27cee4-4e8d-4579-b1c9-cbcddff9e95e",
   "metadata": {},
   "source": [
    "## 基于 Trainer 训练\n",
    "\n",
    "以下代码运行出错，原因是\n",
    "\n",
    "1. 提前手动去除 `labels`, collator 才能采数据\n",
    "2. trainer 在训练时，并没有 `labels` 无法计算 loss, 导致无法执行训练\n",
    "\n",
    "在初学 transfomrers 时, 由于其封装，使得自定义功能实现非常麻烦，本例只是实现一个 SFT, 都要大费周章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "139144f0-05ce-4f20-a222-432e5cad6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/dkd7345140x57tz11tnwcrdm0000gn/T/ipykernel_64098/3364786665.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DISABLE_ACCELERATE\"] = \"1\" \n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output/qwen3_sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_steps=5000,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    # warmup_steps=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    # fp16=True,\n",
    "    push_to_hub=False,\n",
    "    \n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    deepspeed=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset_sft_not_labels[\"train\"],\n",
    "    eval_dataset=False,\n",
    ")\n",
    "\n",
    "# 执行以下代码会报错\n",
    "# trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf7b22-8885-4f68-87ab-435f3c3a13be",
   "metadata": {},
   "source": [
    "## 继承方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f1306b-a7c4-4a1d-8885-59d335931551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        # 分离输入特征和标签\n",
    "        labels = [feature.pop('labels') for feature in features] if 'labels' in features[0] else None\n",
    "        \n",
    "        # 调用父类方法处理输入特征\n",
    "        batch = super().__call__(features)\n",
    "\n",
    "        # padding \n",
    "        bsz, seq_len = batch['input_ids'].shape\n",
    "        padding_labels = torch.ones(bsz, seq_len, dtype=torch.long) * -100\n",
    "        for i in range(bsz):\n",
    "            if self.tokenizer.padding_side == 'right':\n",
    "                tmp_len = len(labels[i])\n",
    "                padding_labels[i, :tmp_len] = torch.tensor(labels[i], dtype=torch.long)\n",
    "        batch['labels'] = padding_labels\n",
    "                \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe42ce9f-8070-4704-8af0-770fdd648316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198, 105043,  30709,  99949, 100857, 100168,  31914,\n",
      "             11,  14880,  99464, 100700, 102104,  20002,  13872,  43589,  86119,\n",
      "         151645,    198, 151644,    872,    198,  35127,   2326,  10414,    369,\n",
      "          19429,   9314,     13, 151645,    198, 151644,  77091,    198, 151667,\n",
      "            271, 151668,    271,     16,   5142,    266,    264,  23831,   9968,\n",
      "            323,   1281,   2704,    311,   2924,  11260,    315,  25322,    323,\n",
      "          23880,     13,    715,     17,     13,  32818,  15502,    311,   2506,\n",
      "            697,   2487,   4541,    323,   3746,     13,    715,     18,     13,\n",
      "           2126,   3322,   6084,    323,  10306,    264,  12966,   6084,   9700,\n",
      "             13, 151645,    198],\n",
      "        [151644,   8948,    198, 105043,  30709,  99949, 100857, 100168,  31914,\n",
      "             11,  14880,  99464, 100700, 102104,  20002,  13872,  43589,  86119,\n",
      "         151645,    198, 151644,    872,    198,   3838,    525,    279,   2326,\n",
      "           6028,   7987,     30, 151645,    198, 151644,  77091,    198, 151667,\n",
      "            271, 151668,    271,    785,   2326,   6028,   7987,    525,   2518,\n",
      "             11,   6303,     11,    323,  13753,     13, 151645,    198, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643]])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100, 151667,    271,\n",
      "         151668,    271,     16,   5142,    266,    264,  23831,   9968,    323,\n",
      "           1281,   2704,    311,   2924,  11260,    315,  25322,    323,  23880,\n",
      "             13,    715,     17,     13,  32818,  15502,    311,   2506,    697,\n",
      "           2487,   4541,    323,   3746,     13,    715,     18,     13,   2126,\n",
      "           3322,   6084,    323,  10306,    264,  12966,   6084,   9700,     13,\n",
      "         151645,   -100,   -100],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100, 151667,    271,\n",
      "         151668,    271,    785,   2326,   6028,   7987,    525,   2518,     11,\n",
      "           6303,     11,    323,  13753,     13, 151645,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, TruncationStrategy\n",
    "\n",
    "my_collator = CustomDataCollator(tokenizer, \n",
    "                                   return_tensors=\"pt\",\n",
    "                                  )\n",
    "\n",
    "data_loader = DataLoader(dataset_sft['train'], # dataset_sft_not_labels\n",
    "                    batch_size=2, \n",
    "                    collate_fn=my_collator, \n",
    "                    shuffle=False)\n",
    "\n",
    "for batch in data_loader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['labels'])\n",
    "    # do train\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2e9f9a8-e82f-4232-8543-12e0aa0bbe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/dkd7345140x57tz11tnwcrdm0000gn/T/ipykernel_64098/4002118288.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>21.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>18.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.523900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=13.145537328720092, metrics={'train_runtime': 13.8239, 'train_samples_per_second': 2.894, 'train_steps_per_second': 0.723, 'total_flos': 15344124297216.0, 'train_loss': 13.145537328720092, 'epoch': 0.0007692011845698242})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"output/qwen3_sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_steps=5000,\n",
    "    logging_steps=1,\n",
    "    max_steps = 10,# for debug\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    # warmup_steps=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    # fp16=True,\n",
    "    push_to_hub=False,\n",
    "    \n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    deepspeed=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    # data_collator=collator,    \n",
    "    data_collator=my_collator,\n",
    "    # train_dataset=dataset_sft_not_labels[\"train\"],\n",
    "    train_dataset=dataset_sft[\"train\"],\n",
    "    eval_dataset=False,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0354ceb-2af0-4efc-b63a-b1103bbdd924",
   "metadata": {},
   "source": [
    "可自行调整训练参数，训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46637f82-715a-4bc1-8730-bf6c69e61b7f",
   "metadata": {},
   "source": [
    "## 使用 TRL 的 SFTTrainer\n",
    "\n",
    "根据官方例子, 所给的数据集 `trl-lib/Capybara` 是\n",
    "\n",
    "1. 多轮对话数据集\n",
    "2. messages 组织数据\n",
    "\n",
    "可以参考 `trl-lib/Capybara` 数据集, 处理 Alpaca 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32fb4e66-c920-427f-868c-27bd5800243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'messages', 'num_turns'],\n",
      "    num_rows: 15806\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c05d11-e0f7-44da-9367-eae9aaaf6803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"We will read about a scenario, and then have a question about it.\\n---\\nScenario:\\nBil and Mike have a shared Dropbox folder.\\nBil puts a file called 'schematic.pdf' inside /shared\\\\_folder/schematics\\nMike notices Bil put a file in there, and moves the file to /shared\\\\_folder/tmp\\nHe says nothing about this to Bil, and Dropbox also does not notify Bil.\\n\\nQuestion: After Bil and Mike finish talking via Discord, Bil wants to open 'schematic.pdf'. In which folder will he look for it?\",\n",
       "  'role': 'user'},\n",
       " {'content': \"Based on the scenario, Bil originally put the 'schematic.pdf' file in the /shared\\\\_folder/schematics folder. Since he was not informed about the file being moved by Mike and Dropbox didn't notify him either, he will most likely look for the file in the /shared\\\\_folder/schematics folder.\",\n",
       "  'role': 'assistant'},\n",
       " {'content': 'What wil Bil think when he looks in /shared\\\\_folder/schematics ?',\n",
       "  'role': 'user'},\n",
       " {'content': \"When Bil looks in the /shared\\\\_folder/schematics folder and does not find the 'schematic.pdf' file, he might be confused, surprised, or frustrated, as he will not be aware that Mike moved the file to /shared\\\\_folder/tmp. He may wonder if he made a mistake in uploading the file or if there was an issue with Dropbox. Bil might then reach out to Mike or check other folders in the shared Dropbox to locate the file.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['messages'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a909b5ae-4420-4de9-a8d4-84fbe4ab1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('tatsu-lab/alpaca',\n",
    "                      cache_dir=\"~/.cache/huggingface\",)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbc062a9-538d-4ce7-9957-f93b64bb9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cat_inst_input(example):\n",
    "    example['messages'] = [\n",
    "        {'role':'system', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "        {'role':'user', 'content': example['instruction']+example['input']},\n",
    "        {'role':'assistant', 'content': example['output']},\n",
    "    ]\n",
    "    return example\n",
    "    \n",
    "dataset_alpaca = dataset.map(map_cat_inst_input,\n",
    "                             remove_columns=[\"instruction\", \"input\", \"output\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fc3bb1b-6b33-4251-82f0-28646f287ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.8213672637939453, metrics={'train_runtime': 15.3755, 'train_samples_per_second': 1.301, 'train_steps_per_second': 0.65, 'total_flos': 6813150609408.0, 'train_loss': 1.8213672637939453})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = SFTConfig(\n",
    "    output_dir=\"output/qwen3_sft\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    max_length = 256,\n",
    "    max_steps = 10\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=\"Qwen/Qwen3-0.6B\",\n",
    "    args=config,\n",
    "    train_dataset=dataset_alpaca['train']\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6629b-c3a6-4dac-94cc-eed7bccf5282",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "1. 利用 transformers 库提高的方法实现带 trainer 的训练\n",
    "2. 利用 trl 库实现调包训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6370ff-dd55-401c-9728-ea4559d8389e",
   "metadata": {},
   "source": [
    "## 实践拓展\n",
    "\n",
    "1. 根据以上代码写出 python 训练代码，epochs=1 训练一个 SFT 模型，并保存\n",
    "2. 将训练的模型进行生成\n",
    "3. 采用多卡方式进行 SFT 训练\n",
    "\n",
    "## 思考问题\n",
    "\n",
    "1. 数据长短方差大，如何减少 padding?\n",
    "2. 什么是数据的 packing 策略? 写出带 packing 的 input_ids, attention_mask, labels\n",
    "3. 查找文档辨别，trl::SFTTrainer 对多轮对话数据，是 fitting 每轮回答，还是最后一轮回答？\n",
    "4. 写出多轮对话的 generate 函数? \n",
    "5. 写出批量多轮对话数据(batch) 的 generate 函数？\n",
    "6. 分析多轮对话过程, KVCache 的变化情况，padding 方式对 KVCache 的影响。\n",
    "8. 阅读 Trainer 源代码, 画出流程图\n",
    "9. 阅读 SFTTrainer 源代码, 画出流程图"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

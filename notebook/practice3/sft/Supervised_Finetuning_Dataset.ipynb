{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37da920-5b85-4a91-b298-7dafc42d214d",
   "metadata": {},
   "source": [
    "# Supervised Finetuning Dataset\n",
    "\n",
    "语言模型的输入是 token 序列, 对于有监督学习(Supervised FineTuning,SFT)任务可以将语言任务数据转化为通用的问答（QA）模式，SFT学习时仅拟合回答(A)\n",
    "\n",
    "所以有监督任务数据面临（1）将有监督任务数据转化为通用的 QA (2) 构造有监督学习的 label\n",
    "\n",
    "1. message format\n",
    "2. dataset\n",
    "3. data collactor function\n",
    "\n",
    "处理流程\n",
    "\n",
    "1. SFT 数据为 QA 对, 考虑多轮对话情形, 使用 chat message 组织对话，使用自定义对话模版\n",
    "2. 将单条数据 tokenizer 化\n",
    "3. 编写 collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5a95e-2ead-4422-9d05-1a1c90b2010e",
   "metadata": {},
   "source": [
    "## 数据 & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db9e684-bf2f-4e71-bed5-c1a9a4662d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-0.6B', \n",
    "                                          #local_dir='~/.cache/huggingface/', # 如果可以直连 huggingface, 去除此行.\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef259aa-ff6f-45bc-8bc3-045c2eb5b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFINIED_SYSTEM_PROMPT='你是小冬瓜智能体,请安全详细回答用户 USER 的问题'\n",
    "messages_1=[\n",
    "    {'role':'SYSTEM', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'USER', 'content':'$sin^2x+cos^2x=?'},\n",
    "    {'role':'ASSISTANT', 'content':'结果为 $\\\\boxed{1}$'},\n",
    "    {'role':'USER', 'content':'为什么?'},\n",
    "    {'role':'ASSISTANT', 'content':'在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$'},\n",
    "]\n",
    "\n",
    "messages_2=[    \n",
    "    {'role':'SYSTEM', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'USER', 'content':'什么是人工智能?'},\n",
    "    {'role':'ASSISTANT', 'content':'人工智能是让机器模拟人类思维的技术。'},\n",
    "]\n",
    "messages_3=[    \n",
    "    {'role':'SYSTEM', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'USER', 'content':'如何计算复利?'},\n",
    "    {'role':'ASSISTANT', 'content':'复利计算公式：本息和 = 本金 × (1 + 利率)^期数。'},\n",
    "]\n",
    "messages_4=[    \n",
    "    {'role':'SYSTEM', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'USER', 'content':'“哈基米”翻译成英文'},\n",
    "    {'role':'ASSISTANT', 'content':'“哈基米”翻译成英文通常是 \"Hakimi\"（人名音译）。'},\n",
    "]\n",
    "messages_list = [messages_1, messages_2, messages_3, messages_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970ec76-e7ee-44ff-a01e-74896a445f37",
   "metadata": {},
   "source": [
    "官方 tokenizer 会自带 chat template 模版, 以下代码仅作示例, 我们将自定义实现一个模版函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4871ccb3-ba0e-46e1-bd49-8ed0015467ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是小冬瓜智能体,请安全详细回答用户 USER 的问题<|im_end|>\n",
      "<|im_start|>user\n",
      "“哈基米”翻译成英文<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "“哈基米”翻译成英文通常是 \"Hakimi\"（人名音译）。<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.chat_template)  # 可查看模版描述\n",
    "\n",
    "messages_tmp=[    \n",
    "    {'role':'system', 'content':DEFINIED_SYSTEM_PROMPT},\n",
    "    {'role':'user', 'content':'“哈基米”翻译成英文'},\n",
    "    {'role':'assistant', 'content':'“哈基米”翻译成英文通常是 \"Hakimi\"（人名音译）。'},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages_tmp, \n",
    "                              tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a2f50-39e4-4935-93fe-79b7d4a748a5",
   "metadata": {},
   "source": [
    "## Chat Template\n",
    "\n",
    "对话模版有两种组织方式：\n",
    "\n",
    "1. 对文本进行格式化处理\n",
    "2. 对 token id 进行处理\n",
    "\n",
    "### 方式1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f00e34c-5427-43fd-ad98-f56ca353e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS>#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题#USER:$sin^2x+cos^2x=?#ASSISTANT:结果为 $\\boxed{1}$<EOS>#USER:为什么?#ASSISTANT:在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$<EOS>\n"
     ]
    }
   ],
   "source": [
    "def ChatTemplate(example):\n",
    "    prompt = '<SOS>'\n",
    "    for i, item in enumerate(example):\n",
    "        prompt += '#' + item['role'] + ':' + item['content'] \n",
    "        if i % 2 == 0 and i != 0:\n",
    "            prompt += '<EOS>' # 补全回答都要 EOS\n",
    "    return prompt\n",
    "    \n",
    "prompt = ChatTemplate(messages_1)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c54b72e5-fdaa-4de2-b768-f16ea95f521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18858, 3126, 61125, 46487, 25, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 2, 6448, 21701, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 19884, 2, 4939, 3846, 2821, 25, 59151, 17714, 57960, 79075, 90, 16, 31716, 27, 55940, 61125, 6448, 25, 100678, 30, 2, 4939, 3846, 2821, 25, 18493, 75317, 100213, 101913, 108112, 100380, 57218, 52129, 27442, 9370, 116539, 11, 75882, 43268, 18493, 30784, 8908, 121, 112, 101913, 111367, 105706, 3, 15940, 87, 11, 9407, 87, 54876, 100345, 105170, 99223, 22382, 21887, 30440, 33477, 3, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 28, 16, 3, 27, 55940, 29]]\n",
      "<SOS>#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题#USER:$sin^2x+cos^2x=?#ASSISTANT:结果为 $\\boxed{1}$<EOS>#USER:为什么?#ASSISTANT:在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$<EOS>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_id = tokenizer( [prompt], add_special_tokens=True ) # tokenizer 默认输入列表\n",
    "print(input_id['input_ids'])\n",
    "\n",
    "decode_prompt = tokenizer.decode(input_id['input_ids'][0], skip_special_tokens=False)\n",
    "print(decode_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a61b9-1ff1-4637-ab10-c0f54ff97232",
   "metadata": {},
   "source": [
    "1. 有监督学习任务，目标是拟合 Assistant 内容, 其损失函数与 pre-trained 时使用的 Cross-Entropy Loss 一致\n",
    "2. 分析以上处理方法，tokenize 字符串时, 就需要在 token id 序列中找到 ASSISTANT 的序列内容\n",
    "\n",
    "截取回答内容需要定位到:\n",
    "\n",
    "1. 头: #ASSISTANT 最后一个 token\n",
    "2. 尾: `<EOS>`\n",
    "\n",
    "代码省略在列表查找子列表位置问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aca5216-239a-4fe6-a84f-96b84f23d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4939, 3846, 2821]\n",
      "#\n",
      "ASS\n",
      "IST\n",
      "ANT\n",
      "[23835, 3126, 29]\n",
      "<E\n",
      "OS\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer('#ASSISTANT')['input_ids']\n",
    "print(ids)\n",
    "for i in ids:\n",
    "    print(tokenizer.decode(i))\n",
    "\n",
    "ids = tokenizer('<EOS>')['input_ids'] #并非按照我们期望encode成一个 token id\n",
    "print(ids)\n",
    "for i in ids:\n",
    "    print(tokenizer.decode(i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350cb3d7-d535-4806-bf74-9e432e945cb9",
   "metadata": {},
   "source": [
    "### Tokenizer 分析\n",
    "\n",
    "Tokenizer 会预设专用的 token，以 Qwen3 举例, 有 eos_token `<|im_end|>`, 但是句子开头在`'additional_special_tokens':` 上的 `<|im_start|>`\n",
    "\n",
    "较为特殊的是有 `'pad_token': '<|endoftext|>',`, 对应的词元写法应当为'<|pad|>'更加合适, 本 lc 不进行过度修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2ea344-a1f2-46bc-a903-9f39060e87c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbfbb15c-c8d9-41c4-a614-b870d364fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFINED_EOS_TOKEN = '<|im_end|>'\n",
    "DEFINED_SOS_TOKEN = '<|im_start|>'\n",
    "DEFINED_PAD_TOKEN = '<|endoftext|>'\n",
    "\n",
    "def ChatTemplateDefinedToken(example):\n",
    "    prompt = DEFINED_SOS_TOKEN\n",
    "    for i, item in enumerate(example):\n",
    "        prompt += '\\n#' + item['role'] + ':' + item['content'] \n",
    "        if i % 2 == 0 and i != 0:\n",
    "            prompt += DEFINED_EOS_TOKEN # 补全回答都要 EOS\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bce8459-bda3-4821-bdb1-f6a8206b8782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>\n",
      "#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题\n",
      "#USER:$sin^2x+cos^2x=?\n",
      "#ASSISTANT:结果为 $\\boxed{1}$<|im_end|>\n",
      "#USER:为什么?\n",
      "#ASSISTANT:在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$<|im_end|> \n",
      "\n",
      "\n",
      "[[151644, 198, 2, 46487, 25, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 198, 2, 6448, 21701, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 28, 5267, 2, 4939, 3846, 2821, 25, 59151, 17714, 57960, 79075, 90, 16, 31716, 151645, 198, 2, 6448, 25, 100678, 5267, 2, 4939, 3846, 2821, 25, 18493, 75317, 100213, 101913, 108112, 100380, 57218, 52129, 27442, 9370, 116539, 11, 75882, 43268, 18493, 30784, 8908, 121, 112, 101913, 111367, 105706, 3, 15940, 87, 11, 9407, 87, 54876, 100345, 105170, 99223, 22382, 21887, 30440, 33477, 3, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 28, 16, 3, 151645]] \n",
      "\n",
      "\n",
      "<|im_start|>\n",
      "#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题\n",
      "#USER:$sin^2x+cos^2x=?\n",
      "#ASSISTANT:结果为 $\\boxed{1}$<|im_end|>\n",
      "#USER:为什么?\n",
      "#ASSISTANT:在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$<|im_end|> \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatTemplateDefinedToken(messages_1)\n",
    "print(prompt, '\\n\\n')\n",
    "\n",
    "input_id = tokenizer( [prompt], add_special_tokens=True ) # tokenizer 默认输入列表\n",
    "print(input_id['input_ids'], '\\n\\n')\n",
    "\n",
    "decode_prompt = tokenizer.decode(input_id['input_ids'][0], skip_special_tokens=False)\n",
    "print(decode_prompt, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962c386-380e-4443-848d-8921d67113f2",
   "metadata": {},
   "source": [
    "可以想办法在 label 前增加一个 special token, 用于定位 label 的开始位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c585cdb4-7d52-4995-b3e2-61a80b476d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [151648], 'attention_mask': [1]}\n",
      "\n",
      "\n",
      "[format prompt]:\n",
      " <|im_start|>\n",
      "#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题\n",
      "#USER:$sin^2x+cos^2x=?\n",
      "#ASSISTANT:<|box_start|>结果为 $\\boxed{1}$<|im_end|>\n",
      "#USER:为什么?\n",
      "#ASSISTANT:<|box_start|>在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$<|im_end|>\n",
      "\n",
      "\n",
      "[format prompt input_id]:\n",
      " [[151644, 198, 2, 46487, 25, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 198, 2, 6448, 21701, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 28, 5267, 2, 4939, 3846, 2821, 25, 151648, 59151, 17714, 57960, 79075, 90, 16, 31716, 151645, 198, 2, 6448, 25, 100678, 5267, 2, 4939, 3846, 2821, 25, 151648, 18493, 75317, 100213, 101913, 108112, 100380, 57218, 52129, 27442, 9370, 116539, 11, 75882, 43268, 18493, 30784, 8908, 121, 112, 101913, 111367, 105706, 3, 15940, 87, 11, 9407, 87, 54876, 100345, 105170, 99223, 22382, 21887, 30440, 33477, 3, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 28, 16, 3, 151645]]\n",
      "\n",
      "\n",
      "[format prompt input_id decode]:\n",
      " <|im_start|>\n",
      "#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题\n",
      "#USER:$sin^2x+cos^2x=?\n",
      "#ASSISTANT:<|box_start|>结果为 $\\boxed{1}$<|im_end|>\n",
      "#USER:为什么?\n",
      "#ASSISTANT:<|box_start|>在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "DEFINED_LABEL_START_TOKEN = '<|box_start|>'\n",
    "\n",
    "\n",
    "def ChatTemplateForLabel(example):\n",
    "    prompt = DEFINED_SOS_TOKEN\n",
    "    for i, item in enumerate(example):\n",
    "        prompt += '\\n#' + item['role'] + ':' \n",
    "        if item['role'] == 'ASSISTANT':\n",
    "            prompt += DEFINED_LABEL_START_TOKEN\n",
    "        prompt += item['content'] \n",
    "        if i % 2 == 0 and i != 0:\n",
    "            prompt += DEFINED_EOS_TOKEN # 补全回答都要 EOS\n",
    "    return prompt\n",
    "\n",
    "print(tokenizer(DEFINED_LABEL_START_TOKEN))\n",
    "\n",
    "prompt = ChatTemplateForLabel(messages_1)\n",
    "print('\\n\\n[format prompt]:\\n', prompt, )\n",
    "\n",
    "input_id = tokenizer( [prompt], add_special_tokens=True ) # tokenizer 默认输入列表\n",
    "print('\\n\\n[format prompt input_id]:\\n',input_id['input_ids'])\n",
    "\n",
    "decode_prompt = tokenizer.decode(input_id['input_ids'][0], skip_special_tokens=False)\n",
    "print('\\n\\n[format prompt input_id decode]:\\n',decode_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f2661b-b721-4c52-b8d7-3295ad359307",
   "metadata": {},
   "source": [
    "上述的 tokenizer 化出现了诡异的现象，`<|box_start|>` 应当被处理成 ID.151648, 但在 `[format prompt input_id]:` 序列里并无`151648` ID\n",
    "\n",
    "结论是对文本格式化处理，再 tokenize, 我们所标记的 special token 可能会被编译成其他数据，结论是方式 1 不安全。\n",
    "\n",
    "先对各 message 做 tokenize， 再格式化拼接。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18d311-f464-4a37-a35c-ad3de83054aa",
   "metadata": {},
   "source": [
    "## 方式2\n",
    "\n",
    "对 token id 进行拼接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd7f17c-4725-4b9f-84fd-c7ffd8a26292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151644"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(DEFINED_SOS_TOKEN).input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6479f5c0-85ce-4b56-b5ce-3fb9703d0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151644, 198, 2, 46487, 25, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 198, 2, 6448, 21701, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 19884, 198, 2, 4939, 3846, 2821, 25, 59151, 17714, 57960, 79075, 90, 16, 31716, 151645, 198, 2, 6448, 25, 100678, 30, 198, 2, 4939, 3846, 2821, 25, 18493, 75317, 100213, 101913, 108112, 100380, 57218, 52129, 27442, 9370, 116539, 11, 75882, 43268, 18493, 30784, 8908, 121, 112, 101913, 111367, 105706, 3, 15940, 87, 11, 9407, 87, 54876, 100345, 105170, 99223, 22382, 21887, 30440, 33477, 3, 15940, 61, 17, 87, 10, 9407, 61, 17, 87, 28, 16, 3, 151645] \n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "\n",
      "<|im_start|>\n",
      "#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题\n",
      "#USER:$sin^2x+cos^2x=?\n",
      "#ASSISTANT:结果为 $\\boxed{1}$<|im_end|>\n",
      "#USER:为什么?\n",
      "#ASSISTANT:在单位圆上的任意一点与原点的连线,该线在 xy 轴上的投影分别为$sinx,cosx$,根据勾股定理可证$sin^2x+cos^2x=1$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "def ChatTemplateToken(example, tokenizer):\n",
    "    sos_token_id = tokenizer(DEFINED_SOS_TOKEN).input_ids[0] \n",
    "    eos_token_id = tokenizer(DEFINED_EOS_TOKEN).input_ids[0] \n",
    "    \n",
    "    input_ids = [ sos_token_id ]\n",
    "    is_labels = [ 0 ]\n",
    "    \n",
    "    for i, item in enumerate(example):\n",
    "        if item['role'] == 'ASSISTANT':\n",
    "            prompt = '\\n#' + item['role'] + ':'\n",
    "            content_prompt = item['content'] \n",
    "            prompt_token_ids = tokenizer(prompt).input_ids\n",
    "            content_prompt = tokenizer(content_prompt).input_ids\n",
    "\n",
    "            is_labels += [0]*len(prompt_token_ids) + [1]*len(content_prompt) + [1] # last [1] is eos \n",
    "            input_ids += prompt_token_ids + content_prompt + [eos_token_id]\n",
    "            \n",
    "        else:\n",
    "            prompt = '\\n#' + item['role'] + ':' + item['content'] \n",
    "            prompt_token_ids = tokenizer(prompt).input_ids\n",
    "            input_ids += prompt_token_ids\n",
    "            \n",
    "            is_labels += [0]*len(prompt_token_ids)\n",
    "            \n",
    "    return input_ids, is_labels\n",
    "\n",
    "input_ids, is_labels = ChatTemplateToken(messages_1, tokenizer)\n",
    "\n",
    "print(input_ids, '\\n\\n')\n",
    "print(is_labels, '\\n\\n')\n",
    "\n",
    "format_prompt = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "print(format_prompt,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249062ab-5964-4060-9118-d98fb3817811",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset 一般分 3 种\n",
    "\n",
    "1. Raw-Data Dataset\n",
    "2. Token-id Dataset\n",
    "3. Batch token-id Dataset, 在 2 的基础上 batch 化处理，即要做 padding 等操作，这里的 padding 需要对 attention mask、label 同等处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acb481f0-b07c-4336-b819-60ca9e5878b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'SYSTEM', 'content': '你是小冬瓜智能体,请安全详细回答用户 USER 的问题'},\n",
       " {'role': 'USER', 'content': '什么是人工智能?'},\n",
       " {'role': 'ASSISTANT', 'content': '人工智能是让机器模拟人类思维的技术。'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RawSFTDataset:\n",
    "    def __init__(self, messages):\n",
    "        self.data = messages\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = RawSFTDataset(messages_list)\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020a616e-51c5-46cc-a536-1577561de4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151644, 198, 2, 46487, 25, 105043, 30709, 99949, 100857, 100168, 31914, 11, 14880, 99464, 100700, 102104, 20002, 13872, 43589, 86119, 198, 2, 6448, 25, 106582, 104455, 30, 198, 2, 4939, 3846, 2821, 25, 104455, 20412, 99258, 102182, 105717, 103971, 102141, 105535, 1773, 151645]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<|im_start|>\n",
      "#SYSTEM:你是小冬瓜智能体,请安全详细回答用户 USER 的问题\n",
      "#USER:什么是人工智能?\n",
      "#ASSISTANT:人工智能是让机器模拟人类思维的技术。<|im_end|>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 采用拼接 token_id 方法构造数据集\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TokenSFTDataset(Dataset):\n",
    "    def __init__(self, messages_list, tokenizer):\n",
    "        self.data = [ ChatTemplateToken(messages, tokenizer) for messages in messages_list ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids':self.data[idx][0],\n",
    "            'is_label':self.data[idx][1]\n",
    "        }\n",
    "\n",
    "dataset = TokenSFTDataset(messages_list, tokenizer)\n",
    "print(dataset[1]['input_ids'])\n",
    "print(dataset[1]['is_label'])\n",
    "\n",
    "print(tokenizer.decode(dataset[1]['input_ids']))\n",
    "\n",
    "# 特别注意 <EOS> 一定是 label, 如果没有, 则会导致 SFT 模型生成时无法停止\n",
    "print(tokenizer.decode(dataset[1]['input_ids'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147eacbc-c38b-4f65-bbf4-b7283d74c68a",
   "metadata": {},
   "source": [
    "至于第3种方法, batch化数据集不是一个好的策略，\n",
    "\n",
    "1. 其所产生的 padding 数据增加存储占用\n",
    "2. 好处在于在高速训练过程, 可以直接取batch数据传入到GPU去计算\n",
    "\n",
    "更通用的方式是手写一个 collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a3bfb-68f2-408d-8928-84e420eb7ab1",
   "metadata": {},
   "source": [
    "## Data collate function\n",
    "\n",
    "在深度学习训练过程，数据集会 shuffle, 多条随机数据成 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b74eb4ae-ef84-4bc0-b18b-ed85b5891d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenSFTDataset(Dataset):\n",
    "    def __init__(self, messages_list, tokenizer):\n",
    "        data_list = [ ChatTemplateToken(messages, tokenizer) for messages in messages_list ]\n",
    "        self.data = []\n",
    "        for data in data_list:\n",
    "            self.data.append( \n",
    "                [torch.tensor(data[0], dtype=torch.long), \n",
    "                 torch.tensor(data[1], dtype=torch.long)]\n",
    "            )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        token 返回数据一般是 tensor\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': self.data[idx][0],\n",
    "            'is_label':self.data[idx][1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "719671ff-48b9-4fae-a0cf-e783e9aa8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddding_collate_fn(batch_data, pad_token_id=None, ignore_index=-100):\n",
    "\n",
    "    input_lens = []\n",
    "    label_lens = []\n",
    "    bs = len(batch_data)\n",
    "\n",
    "    # padding longest maxlen\n",
    "    for data in batch_data:\n",
    "        input_lens.append(data['input_ids'].shape[0])\n",
    "        max_input_len = torch.max(torch.tensor(input_lens, dtype=torch.long))\n",
    "    \n",
    "    # Right Padding\n",
    "    input_ids = torch.ones(bs, max_input_len, dtype=torch.long) * pad_token_id\n",
    "    attention_masks = torch.zeros(bs, max_input_len, dtype=torch.long)\n",
    "    labels = torch.ones(bs, max_input_len, dtype=torch.long) * ignore_index\n",
    "\n",
    "    for i in range(bs):\n",
    "        input_ids[i, :input_lens[i]] = batch_data[i]['input_ids']\n",
    "        attention_masks[i, :input_lens[i]] = 1\n",
    "        \n",
    "        idx = torch.where( batch_data[i]['is_label'] != 0)[0]\n",
    "        labels[i, idx-1] = batch_data[i]['input_ids'][idx]\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "class PaddingCollateFunction:\n",
    "    def __init__(self, pad_token_id: int, ignore_index):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def __call__(self, batch) -> dict:\n",
    "        batch = paddding_collate_fn(batch, self.pad_token_id, self.ignore_index )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3187865b-2fda-4871-8b1a-46d00d16ec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,    198,      2,  46487,     25, 105043,  30709,  99949, 100857,\n",
      "         100168,  31914,     11,  14880,  99464, 100700, 102104,  20002,  13872,\n",
      "          43589,  86119,    198,      2,   6448,  21701,  15940,     61,     17,\n",
      "             87,     10,   9407,     61,     17,     87,  19884,    198,      2,\n",
      "           4939,   3846,   2821,     25,  59151,  17714,  57960,  79075,     90,\n",
      "             16,  31716, 151645,    198,      2,   6448,     25, 100678,     30,\n",
      "            198,      2,   4939,   3846,   2821,     25,  18493,  75317, 100213,\n",
      "         101913, 108112, 100380,  57218,  52129,  27442,   9370, 116539,     11,\n",
      "          75882,  43268,  18493,  30784,   8908,    121,    112, 101913, 111367,\n",
      "         105706,      3,  15940,     87,     11,   9407,     87,  54876, 100345,\n",
      "         105170,  99223,  22382,  21887,  30440,  33477,      3,  15940,     61,\n",
      "             17,     87,     10,   9407,     61,     17,     87,     28,     16,\n",
      "              3, 151645],\n",
      "        [151644,    198,      2,  46487,     25, 105043,  30709,  99949, 100857,\n",
      "         100168,  31914,     11,  14880,  99464, 100700, 102104,  20002,  13872,\n",
      "          43589,  86119,    198,      2,   6448,     25, 106582, 104455,     30,\n",
      "            198,      2,   4939,   3846,   2821,     25, 104455,  20412,  99258,\n",
      "         102182, 105717, 103971, 102141, 105535,   1773, 151645,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,  59151,  17714,  57960,  79075,     90,     16,\n",
      "          31716, 151645,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,  18493,  75317, 100213, 101913,\n",
      "         108112, 100380,  57218,  52129,  27442,   9370, 116539,     11,  75882,\n",
      "          43268,  18493,  30784,   8908,    121,    112, 101913, 111367, 105706,\n",
      "              3,  15940,     87,     11,   9407,     87,  54876, 100345, 105170,\n",
      "          99223,  22382,  21887,  30440,  33477,      3,  15940,     61,     17,\n",
      "             87,     10,   9407,     61,     17,     87,     28,     16,      3,\n",
      "         151645,   -100],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100, 104455,  20412,  99258, 102182,\n",
      "         105717, 103971, 102141, 105535,   1773, 151645,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100]])\n"
     ]
    }
   ],
   "source": [
    "DEFINE_IGNORE_INDEX=-100\n",
    "\n",
    "        \n",
    "dataset = TokenSFTDataset(messages_list, tokenizer)\n",
    "collate_fn = PaddingCollateFunction(pad_token_id=0, ignore_index=DEFINE_IGNORE_INDEX)\n",
    "dataloader = DataLoader(dataset, \n",
    "                    batch_size=2, \n",
    "                    shuffle=False, # True\n",
    "                    collate_fn = collate_fn) \n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['attention_masks'])\n",
    "    print(batch['labels'])\n",
    "\n",
    "    break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b696b8-7434-403c-afaf-7810d55382fd",
   "metadata": {},
   "source": [
    "# 细节：\n",
    "\n",
    "Q1: 根据 prompt 写出 label？\n",
    "\n",
    "| Prompt | 回   | 答   | :    | 瓜   | 哥   | 真   | 帅      | `<EOS>` |\n",
    "| ------ | ---- | ---- | ---- | ---- | ---- | ---- | ------- | ------- |\n",
    "| Label  | ? | ?| ?   | ?   | ?   |?   | ?| ?    |\n",
    "\n",
    "A1: \n",
    "\n",
    "| Prompt | 回   | 答   | :    | 瓜   | 哥   | 真   | 帅      | `<EOS>` |\n",
    "| ------ | ---- | ---- | ---- | ---- | ---- | ---- | ------- | ------- |\n",
    "| Label  | -100 | -100 | 瓜   | 哥   | 真   | 帅   | `<EOS>` | -100    |\n",
    "\n",
    "---\n",
    "\n",
    "Q2:\n",
    "\n",
    "对于token id 序列长度为 8 的输入, 其输出 logits 为 8x|V|, 那么有效的 logits 是哪些？\n",
    "\n",
    "A2:\n",
    "\n",
    "logits[2:-1], pos_id `2,3,4,5,6`\n",
    "\n",
    "| Prompt | 回   | 答   | :    | 瓜   | 哥   | 真   | 帅      | `<EOS>` |\n",
    "| ------ | ---- | ---- | ---- | ---- | ---- | ---- | ------- | ------- |\n",
    "| Label  | -100 | -100 | 瓜   | 哥   | 真   | 帅   | `<EOS>` | -100    |\n",
    "| pos_id  | 0 | 1 | 2   | 3   | 4   | 5   | 6 | 7    |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Q3:\n",
    "\n",
    "在 pytorch 的 loss_fn 中, 如何忽略特定 label 的损失\n",
    "\n",
    "A3:\n",
    "\n",
    "`loss_fn = nn.CrossEntropyLoss(ignore_index = -100)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf70dc8-faf1-4670-b89a-49a44d36d631",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "1. SFT 的数据要较多细节，建议阅读完本章节后用 pytorch 手写一遍 dataset、template、collate_fn、dataloader；\n",
    "3. 在工程开发工程中，需要通过 pytorch 灵活处理数据集(如处理多轮对话)，如上述的 `labels` 实际上在模版函数上提前记录好 `is_label`\n",
    "2. HuggingFace 的 Transformers 框架提供了一系列数据处理类，可以直接调用。由于 Transfomers 框架对 pytorch 的数据类再做了封装，定制 Transformers 的数据类实现较为繁琐。\n",
    "3. Tokenizer 有较多坑，例如 special token 未编码成 1 个 token id；\n",
    "4. 需要每一步 check 中间数据, 避免从后往前排查。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

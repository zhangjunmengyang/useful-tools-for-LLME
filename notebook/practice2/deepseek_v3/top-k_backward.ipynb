{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802ce4a4-2538-44e1-a6a4-69b07f48fba9",
   "metadata": {},
   "source": [
    "Top-K 求导问题\n",
    "\n",
    "[MoE训练中的Top-K运算不会导致不可导（不连续）吗？](https://www.zhihu.com/question/11071292653/answer/1913934460161852591)\n",
    "\n",
    "结论：\n",
    "\n",
    "- top-k 数学上是不可导的\n",
    "- torch 实现的 top-k 可导，其导数反向传播类似 embedding 层\n",
    "\n",
    "在 sMoE 论文中， APPENDICES.A 将 top-k 转化为一个连续的概率分布\n",
    "\n",
    "> Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n",
    "\n",
    "在主流的 sMoE 实现中（如 Mixtral8x7B）， 都用 torch 自带的 top-k 算子。\n",
    "\n",
    "注意不要手动用 `torch.max` 去取 top 元素，可能导致无法反传梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0c52c8-e4fe-4c1b-b0df-fb961835a607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk idx: tensor([[2, 1],\n",
      "        [5, 3]])\n",
      "pytorch top-k p grad tensor([[ 0.0000,  0.0044,  0.0815,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0345,  0.0000, -0.0008,  0.0000,  0.0000]])\n",
      "pytorch input_gradient:  tensor([ 0.0054, -0.0018, -0.0061, -0.0017])\n",
      "pytorch w_gate:  tensor([-0.0020, -0.0089,  0.0362, -0.0038])\n",
      "--------------------------------------------------\n",
      "hand-write top-k p grad tensor([[ 0.0000,  0.0044,  0.0815,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0345,  0.0000, -0.0008,  0.0000,  0.0000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[-0.0012, -0.0051,  0.0201, -0.0012, -0.0034, -0.0030, -0.0043, -0.0019],\n",
      "        [ 0.0005,  0.0006,  0.0008, -0.0046,  0.0007,  0.0011,  0.0003,  0.0006]],\n",
      "       grad_fn=<CopySlices>)\n",
      "hand-write input_gradient:  tensor([ 0.0054, -0.0018, -0.0061, -0.0017], grad_fn=<SliceBackward0>)\n",
      "hand-write w_gate:  tensor([-0.0020, -0.0089,  0.0362, -0.0038], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# torch autograd\n",
    "dim = 16\n",
    "experts_num = 8\n",
    "top_k = 2\n",
    "bs = 2\n",
    "label = torch.randn(bs, top_k)\n",
    "a = torch.randn(bs, dim, requires_grad=True)\n",
    "w_gate = nn.Linear(dim, experts_num)\n",
    "y = w_gate(a)\n",
    "y.retain_grad()\n",
    "p = F.softmax(y, dim = -1)\n",
    "p.retain_grad()\n",
    "v, idx = torch.topk(p, k = top_k, dim = -1)\n",
    "print(f'topk idx: {idx}')\n",
    "loss = ((v - label) ** 2 ) / label.numel()\n",
    "loss = loss.sum()\n",
    "loss.backward()\n",
    "print('pytorch top-k p grad', p.grad)\n",
    "print('pytorch input_gradient: ' , a.grad[0,:4])\n",
    "print('pytorch w_gate: ' , w_gate.weight.grad.t()[0,:4])\n",
    "\n",
    "print('-' * 50)\n",
    "# hand-write\n",
    "dv = 2 * (v - label) / label.numel()\n",
    "dp = torch.zeros(bs, experts_num)\n",
    "# only backward select-top-k element's gradient\n",
    "for i in range(bs):\n",
    "    dp[i, idx[i]] = dv[i,:]\n",
    "print('hand-write top-k p grad', dp)\n",
    "\n",
    "dy = torch.zeros_like(y)\n",
    "for i in range(bs):\n",
    "    tmp_p = torch.zeros(experts_num)\n",
    "    # tmp_p[idx[i]] = p[i, idx[i]] \n",
    "    tmp_p = p[i,:]\n",
    "    d_s = torch.diag(tmp_p) - torch.outer(tmp_p, tmp_p)\n",
    "    dy[i, :] = dp[i, :] @ d_s  \n",
    "print(dy)\n",
    "\n",
    "d_a = dy @ w_gate.weight\n",
    "d_w_gate = (a.t() @ dy)\n",
    "print('hand-write input_gradient: ' , d_a[0,:4])\n",
    "print('hand-write w_gate: ' , d_w_gate[0,:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0162d6-ef6f-4cb2-ac33-e815fc3268df",
   "metadata": {},
   "source": [
    "## Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79484ad-0c5f-4d4c-9522-e1ed496779e1",
   "metadata": {},
   "source": [
    "> Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n",
    "\n",
    "对于单个 token， \n",
    "\n",
    "分位数 x = 专家 i 的 gate 分数 - 去除专家 i 后的 top-k 分数\n",
    "\n",
    "在标准正态分布的累积分布函数 $p = \\phi(x)$ 取概率（而非通过 top-k + softmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

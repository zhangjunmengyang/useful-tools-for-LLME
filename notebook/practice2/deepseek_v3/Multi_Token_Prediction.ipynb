{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c099801-cee1-4813-9c51-e51fa54de04e",
   "metadata": {},
   "source": [
    "## Multi_Token_Prediction\n",
    "\n",
    "传统的预训练任务为 `next-token-prediction`， 多词元预测任务(Multi-token-prediction, MTP) 是 DeepSeek-V3 的训练任务，其本质是 `next-next-token-prediction`，此概念来源于一个并行解码技术。V3 仅采用 MTP-loss 做训练，训练完毕后将额外的 lm head 去除\n",
    "\n",
    "\n",
    "对于预训练文本“拿铁就是牛奶加咖啡”\n",
    "\n",
    "- ntp: `p7 = p(y_牛|拿铁就是牛奶加)`\n",
    "- mtp: 任务1: `p7 = p(y_咖|拿铁就是牛奶加)` , 任务2 `p8 = p(y_啡|拿铁就是牛奶加)`, 对于 p8 任务来说预测 “啡” 字是比较大的概率的，能够达到高准确率的话，那么就能在解码中并行预测多个token了。\n",
    "- mtp basic实现：对于token ‘加’的输出 logits(hidden state) 送到两个任务头。MTP 本质是多任务学习，相较 mtp(nntp) 较 ntp 更难，学习的特征越丰富\n",
    "- mtp-v3：用 NTP 构建 NNTP 学习任务\n",
    "\n",
    "这里的 MTP 实际字面意义不严格，更准确说法是：next-i-token-prediction(nitp)， 下 i 个词元预测。\n",
    "\n",
    "- 下 1 个词元预测：`p7 = p(y_牛|拿铁就是牛奶加)`\n",
    "- 下 2 个词元预测：`p8 = p(y_啡|拿铁就是牛奶加)`\n",
    "\n",
    "为了实现这种训练, 本文依次讲解：\n",
    "\n",
    "1. dummy MTP dataset\n",
    "2. basic MTP\n",
    "3. MTP\n",
    "4. MTP backward 分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f88a0d-abaa-432d-88b7-1053c1bf290d",
   "metadata": {},
   "source": [
    "## MTP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e40c0a-ddd8-4363-98c8-82210fbf6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c913b93f-77bc-4886-a6ca-82401f2f53ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [1.4317, 0.3677, 1.2551]])\n",
      "tensor([[0.0000, 0.0000, 1.4317],\n",
      "        [0.3677, 1.2551, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x[:1] = 0\n",
    "print(x)\n",
    "y = torch.roll(x, shifts=-1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9859cf32-e68f-4f7e-b10d-a00f51ca3f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([83, 74, 53,  6,  3,  2, 29, 44, 93, 32])\n",
      "tensor([[  74,   53,    6,    3,    2,   29,   44,   93,   32, -100],\n",
      "        [  53,    6,    3,    2,   29,   44,   93,   32, -100, -100],\n",
      "        [   6,    3,    2,   29,   44,   93,   32, -100, -100, -100],\n",
      "        [   3,    2,   29,   44,   93,   32, -100, -100, -100, -100],\n",
      "        [   2,   29,   44,   93,   32, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "class MTPDataset(Dataset):\n",
    "    def __init__(self, x, n=5):\n",
    "        super().__init__()\n",
    "        if x is None:\n",
    "            return \n",
    "        self.num_tokens = n\n",
    "        self.x = x.clone()\n",
    "        bsz, seq_len = x.shape\n",
    "        self.y = torch.ones(n, bsz, seq_len, dtype=torch.long) * IGNORE_INDEX\n",
    "        for i in range(n):\n",
    "            self.y[i, :, :-i-1] = x[:, i+1:]\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        bsz, seq_len = x.shape\n",
    "        return bsz\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {'input_ids': self.x[idx,:],\n",
    "            'labels':  self.y[:, idx, :],}\n",
    "        return data\n",
    "\n",
    "bsz = 2\n",
    "seq_len = 10\n",
    "vocab_size = 100\n",
    "dim = 512\n",
    "N = 5\n",
    "\n",
    "x = torch.randint(vocab_size, (bsz, seq_len))\n",
    "dataset = MTPDataset(x, n=N)\n",
    "print(dataset[0]['input_ids'])\n",
    "print(dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a894a-faf8-4edb-be63-a58697d84e64",
   "metadata": {},
   "source": [
    "## MTP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3339a756-07c1-4114-94f5-772f6f581890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTPLanguageModel(\n",
      "  (embd): Embedding(100, 512)\n",
      "  (w): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (lm_heads): ModuleList(\n",
      "    (0-4): 5 x Linear(in_features=512, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MTPLanguageModel(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, num_mtp = 5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_mtp = num_mtp\n",
    "        self.embd = nn.Embedding(vocab_size, dim)\n",
    "        self.w = nn.Linear(dim, dim)\n",
    "        self.lm_heads = nn.ModuleList(\n",
    "            [ nn.Linear(dim, vocab_size) for _ in range(self.num_mtp) ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        X = self.embd(x)\n",
    "        hidden_states = self.w(X)\n",
    "\n",
    "        logits_list = [\n",
    "            lm_head(hidden_states).unsqueeze(dim = 0) for lm_head in self.lm_heads\n",
    "        ]\n",
    "\n",
    "        return torch.cat(logits_list, dim = 0)\n",
    "\n",
    "model = MTPLanguageModel(dim = dim, vocab_size=vocab_size, num_mtp = N)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872b18bf-fdaf-47bc-a466-ef7df7ff9abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "logits_list = model(x)\n",
    "# print(len(logits_list))\n",
    "print(logits_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913fec8-edc1-417b-89f8-b8b5c157e4b5",
   "metadata": {},
   "source": [
    "## MTP Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5bd606-978e-4853-865d-cc983d5d1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0573, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def mtp_loss(X, labels, weight):\n",
    "    n, bsz, seq_len, vocab_size = X.shape\n",
    "    n, bsz, seq_len = labels.shape\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = IGNORE_INDEX)\n",
    "    loss_mtp = torch.zeros(n)\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        tmp_loss = loss_fn( X[i].view(bsz*seq_len, vocab_size), labels[i].view(bsz*seq_len))\n",
    "        loss_mtp[i] = tmp_loss\n",
    "\n",
    "    loss = (loss_mtp * weight).mean()\n",
    "    return loss\n",
    "\n",
    "weight = torch.tensor( [1.0, 0.5, 0.4, 0.2, 0.1])\n",
    "loss = mtp_loss(logits_list, dataset.y, weight)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19762bf-246f-4b0d-a66e-928bb4cffa48",
   "metadata": {},
   "source": [
    "## Next-Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8086b9c-a464-4de5-9ca5-0953b3c398e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31)\n",
      "tensor([31, 15, 71, 52, 38])\n"
     ]
    }
   ],
   "source": [
    "# head 0, next token prediction\n",
    "next_token = torch.argmax(logits_list[0, 0, -1, :], dim = -1) # heads:0, bsz:0, seq_id:-1, dim \n",
    "print(next_token)\n",
    "\n",
    "# head-0~N, multi token token prediction\n",
    "next_next_token = torch.argmax(logits_list[:, 0, -1, :], dim = -1) # heads:0, bsz:0, seq_id:-1, dim \n",
    "print(next_next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4710c8-98ee-4b0c-8d19-5423689bb49e",
   "metadata": {},
   "source": [
    "## DeepSeek MTP\n",
    "\n",
    "1. DeepSeek MTP 做法中，MTP 头 share 主体模型的 lm head\n",
    "2. MTP 组件包含：embd(shared), norm, linear, transformer-block, lm_head(shared)\n",
    "3. MTP 之间的特征是串行递归关系（可以理解为传 latent 特征，而非具体 token)\n",
    "4. 每头的输入为 `[t1,t2,t3,t4]`, `[t2,t3,t4,t5]`, 这与我们上述的输入形式有区别\n",
    "5. v3 的 MTP 每个头训练仍然是 next-token-prediction 形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8677db3e-1da9-45ee-9c00-73d929a899c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeekMTP(\n",
      "  (embd): Embedding(100, 512)\n",
      "  (w): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (lm_head): Linear(in_features=512, out_features=100, bias=True)\n",
      "  (mtp_heads): ModuleList(\n",
      "    (0-4): 5 x MTPModule(\n",
      "      (RMSNorm_pre): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (RMSNorm_cur): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Proj): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (Transformer_block): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MTPModule(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.RMSNorm_pre = nn.Linear(dim, dim)\n",
    "        self.RMSNorm_cur = nn.Linear(dim, dim)\n",
    "        self.Proj = nn.Linear(dim*2, dim)\n",
    "        self.Transformer_block = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, X_embd, H_pre):\n",
    "        X_embd, H_pre = self.RMSNorm_cur(X_embd), self.RMSNorm_pre(H_pre)\n",
    "        X = torch.cat((X_embd, H_pre), dim = -1)\n",
    "        X = self.Proj(X)\n",
    "        X = self.Transformer_block(X)\n",
    "        return X\n",
    "\n",
    "class DeepSeekMTP(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, num_mtp=5):\n",
    "        '''\n",
    "        lm_head 为主体输出头\n",
    "        num_mtp 额外增加 MTP 输出头模块数量\n",
    "        总输出头模块：1+num_mtp\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_mtp = num_mtp\n",
    "        self.embd = nn.Embedding(vocab_size, dim)\n",
    "        self.w = nn.Linear(dim, dim)\n",
    "        self.lm_head = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "        self.mtp_heads = nn.ModuleList(\n",
    "            [ MTPModule(dim) for _ in range(self.num_mtp) ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        bsz, seq_len = x.shape\n",
    "        X = self.embd(x)\n",
    "        hidden_states = self.w(X[:, -self.num_mtp:, :])\n",
    "        lm_logits = self.lm_head(hidden_states) # bsz, len, vocab_size\n",
    "        mtp_logits = torch.randn(self.num_mtp, \n",
    "                                 bsz, \n",
    "                                 seq_len - self.num_mtp, \n",
    "                                 vocab_size)\n",
    "\n",
    "        # 以下循环为递归调用\n",
    "        for i in range(self.num_mtp):\n",
    "            X_cur = X[:, i+1: i+1+seq_len-self.num_mtp, :]\n",
    "            # 讨论: 分析 detach 作用\n",
    "            hidden_states_i = self.mtp_heads[i](X_cur, hidden_states.detach())\n",
    "            mtp_logits_i = self.lm_head(hidden_states_i)\n",
    "            mtp_logits[i] = mtp_logits_i\n",
    "            hidden_states = hidden_states_i\n",
    "\n",
    "        return lm_logits, mtp_logits\n",
    "\n",
    "        \n",
    "model = DeepSeekMTP(dim = dim, vocab_size = vocab_size, num_mtp = N)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d65907c-3a47-42e7-8fd1-a72152580d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70, 86, 43, 19, 24, 58, 38,  6, 77, 84],\n",
      "        [28, 37, 30, 61, 54, 40, 32,  4, 29, 17]])\n",
      "tensor([[  86,   43,   19,   24,   58,   38,    6,   77,   84, -100],\n",
      "        [  37,   30,   61,   54,   40,   32,    4,   29,   17, -100]])\n"
     ]
    }
   ],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, x, n=5):\n",
    "        super().__init__()\n",
    "        if x is None:\n",
    "            return \n",
    "        self.num_tokens = n\n",
    "        self.x = x.clone()\n",
    "        x[:, 0] = IGNORE_INDEX\n",
    "        self.y = torch.roll(x, shifts = -1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        bsz, seq_len = x.shape\n",
    "        return bsz\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {'input_ids': self.x[idx, :],\n",
    "                'labels':  self.y[idx, :],}\n",
    "        return data\n",
    "\n",
    "x = torch.randint(vocab_size, (bsz, seq_len))\n",
    "dataset = LMDataset(x.clone())\n",
    "print(dataset[:]['input_ids'])\n",
    "print(dataset[:]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd8fb07-424a-4bd1-a530-8a6caf6c85f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70, 86, 43, 19, 24, 58, 38,  6, 77, 84],\n",
      "        [28, 37, 30, 61, 54, 40, 32,  4, 29, 17]])\n",
      "torch.Size([2, 5, 100])\n",
      "torch.Size([5, 2, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "input_ids = dataset[:]['input_ids']\n",
    "lm_logits, mtp_logits = model(input_ids)\n",
    "print(lm_logits.shape)\n",
    "print(mtp_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94bb0653-3300-4003-b13a-6a91d0391e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1376, grad_fn=<AddBackward0>)\n",
      "tensor(4.6760, grad_fn=<NllLossBackward0>)\n",
      "tensor([4.5976, 4.6188, 4.6195, 4.6066, 4.6396], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "def deepseek_v3_mtp_loss(lm_logits,\n",
    "                         mtp_logits, \n",
    "                         y,\n",
    "                        lam=0.1):\n",
    "\n",
    "    N, bsz, seq_len, vocab_size = mtp_logits.shape\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "\n",
    "    \n",
    "    loss_lm_head = loss_fn(lm_logits.view(bsz*seq_len,vocab_size), \n",
    "                           y[:,:-N].reshape(bsz*seq_len))\n",
    "\n",
    "    loss_mtp_head = torch.zeros(N)\n",
    "    for i in range(N):\n",
    "        loss_mtp_head[i] = loss_fn(mtp_logits[i,:,:,:].view(bsz*seq_len,vocab_size),\n",
    "                                   y[:,i:-N+i].reshape(bsz*seq_len))\n",
    "                                \n",
    "    loss = loss_lm_head + lam*loss_mtp_head.mean()\n",
    "    return loss, loss_lm_head, loss_mtp_head\n",
    "\n",
    "loss, loss_lm,loss_mtp = deepseek_v3_mtp_loss(lm_logits, \n",
    "                     mtp_logits, \n",
    "                     y= dataset[:]['labels'],\n",
    "                     lam = 0.1)\n",
    "print(loss)\n",
    "print(loss_lm)\n",
    "print(loss_mtp)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe84ba-94b9-4f42-9715-ad0c275c1f55",
   "metadata": {},
   "source": [
    "## MTP 特征分析\n",
    "\n",
    "- Proj前cat [next_predcition_feature, embedding], 此向量包含 t1 hidden 和当前 embd, 其做法是 cat, 在 RNN 网络中通常为加法: h(t1)+x(t2)\n",
    "- TransformerBlock 输出后为 t2 预测 next token(t3) 的特征\n",
    "- 虽然 MTP 类损失是多任务损失，其本质仍是 next-token-prediction， 而非 next-next-token-prediction\n",
    "- 准确来说 v3 是： reccurent-next-token-prediction\n",
    "- mtp moudle 的输入去除一些首 token, 输入信息不完整\n",
    "- inference 角度，MTP 的 NNTP 成本（单层解码块）原低于网络主体 forward（多层解码块）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3281af4-072a-4dee-a483-c452a6da69b1",
   "metadata": {},
   "source": [
    "## 梯度分析\n",
    "\n",
    "1. 每个 mtp 接收上一块数据的 Last hidden，那么其 backwad 计算路径应当为 $MTP_N,MTP_{N-1},\\ldots, MTP_1$\n",
    "2. 如果将 上一块的输出 detach, 那么每个块可以根据局部 mtp loss backward，此时 mtp module 内部参数更新，以及整体lmhead、embd更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de98bcd-15d1-4281-8a61-629c3c7c5f73",
   "metadata": {},
   "source": [
    "## V3-MTP Next-Next-Token-Prediction\n",
    "\n",
    "2. 试讨论以下的 inference 程序是否正确？\n",
    "3. MTP-basic 与 V3-MTP NNTP 差异在哪？\n",
    "\n",
    "Training 阶段\n",
    "\n",
    " - 主体模型输入 [t1,t2,t3,t4] 预测 [t2,t3,t4,t5]\n",
    " - MTP1 输入  [t2,t3,t4,t5] 预测 [t3,t4,t5,t6]\n",
    "\n",
    "对于 training 阶段, t5 是提前知道的，在 inference 阶段时，需要 lm-head 取 argmax 解出 t5, 才能计算 t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b61ea090-d76b-473a-b761-580198a3a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([97, 55, 77,  2, 73, 31])\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# 主体模型输入 [t1,t2,t3,t4] 预测 [t2,t3,t4,t5]\n",
    "\n",
    "ntp = torch.argmax(lm_logits[:, -1, :], dim = -1).unsqueeze(1)\n",
    "nntp = torch.argmax(mtp_logits[:, :, -1, :], dim = -1).t()\n",
    "pred = torch.cat((ntp,nntp), dim = 1)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ec9724f-aef0-4892-b3ca-49604b9f7ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([70, 86, 43, 19, 24, 58, 38,  6, 77, 84])\n",
      "tensor([70, 86, 43, 19, 24, 58, 38,  6, 77, 84, 97])\n",
      "tensor([70, 86, 43, 19, 24, 58, 38,  6, 77, 84, 97, 49])\n",
      "tensor([70, 86, 43, 19, 24, 58, 38,  6, 77, 84, 97, 49, 91])\n",
      "tensor([70, 86, 43, 19, 24, 58, 38,  6, 77, 84, 97, 49, 91, 14])\n",
      "tensor([70, 86, 43, 19, 24, 58, 38,  6, 77, 84, 97, 49, 91, 14, 28])\n",
      "tensor([70, 86, 43, 19, 24, 58, 38,  6, 77, 84, 97, 49, 91, 14, 28, 93])\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "x = dataset[:]['input_ids'].clone()\n",
    "bsz, seq_len = x.shape\n",
    "X = model.embd(x)\n",
    "hidden_states = model.w(X) # input全部输入\n",
    "lm_logits = model.lm_head(hidden_states) # bsz, len, vocab_size\n",
    "next_token = torch.argmax(lm_logits[:, -1, :], dim = -1).unsqueeze(dim = 1)\n",
    "print(x[0,:])\n",
    "x = torch.cat((x, next_token), dim = 1)\n",
    "print(x[0,:])\n",
    "\n",
    "# # 以下循环为递归调用\n",
    "for i in range(model.num_mtp):\n",
    "    # input\n",
    "    X = model.embd(x[:, i+1:])\n",
    "\n",
    "    # output\n",
    "    hidden_states_i = model.mtp_heads[i](X, hidden_states)\n",
    "    mtp_logits_i = model.lm_head(hidden_states_i)\n",
    "    hidden_states = hidden_states_i\n",
    "\n",
    "    # next-token-prediction\n",
    "    next_token = torch.argmax(mtp_logits_i[:, -1, :], dim = -1).unsqueeze(dim = 1)\n",
    "    x = torch.cat((x, next_token), dim = 1)\n",
    "    print(x[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4e951-4189-4099-9364-c74e1f5cb54a",
   "metadata": {},
   "source": [
    "讨论：\n",
    "\n",
    "1. 从 LM+MTP 整体来看输入 [t1,t2,t3] 预测 [t4,t5,t6,...], 但是分别从 lm_head, mtp-i 看其做的仍然是 NTP\n",
    "3. Inference 时，MTP 内部有 Transformer-Block 是否需要 kv-cache？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653e78d-cafd-4296-b2f4-5e2c11b7b35f",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "1. basic mtp 真正去做 next-next-token-prediciton（NNTP）， 其行为为隔空预测\n",
    "2. deepseek-v3 mtp 训练做 next-token-prediction（NTP）。 释放的v3砍掉mtp头\n",
    "4. V3 并没有讨论 NNTP 的性能指标，其涉及到“并行解码”技术\n",
    "5. 讨论：为什么 MTP 能提升模型预测能力？V3 使用 mtp 训练，加强了模型主体的 lm_head 预测, 加强的原理可看成在时许上引入了 latent 时序 feature。 那么 Attn 做序列加权组合， FFN 做特征表示， MoE做集成学习，LM_head做NTP，而带 MTP-LM_head做时序特征预测表示，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6937fb-fbe1-409a-a388-cabbecdb6184",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437v1)\n",
    "\n",
    "[Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e1d707-4227-42f3-a444-49653b68de90",
   "metadata": {},
   "source": [
    "# Multi Latent Attention(MLA)\n",
    "\n",
    "MHA/GQA ç½‘ç»œä¸­åœ¨ Inference æ—¶ä¼šäº§ç”Ÿå¤§é‡çš„ KV-Cacheï¼Œå…¶å ç”¨è¶Šå¤šï¼Œå°†å‡å°‘ Decoding æ—¶ batching æ•°é‡ã€‚\n",
    "\n",
    "MLA æ˜¯ä¸ºäº†å‡å°‘ KV-Cache è€Œç”Ÿçš„æŠ€æœ¯ã€‚ ä¸ºäº†å¿«é€Ÿç†è§£ MLA æ˜¯ä¸€ç§ç±»ä¼¼ MQA(Multi_Query_Attention) çš„æŠ€æœ¯ï¼ˆå³å•å¤´KVï¼‰ï¼ŒMLAåšåˆ°äº†\n",
    "\n",
    "1. MLA è¾¾åˆ° MQA å•å¤´ KV çš„ cache é‡çº§ï¼Œ å¯¹äº Kã€V éœ€è¦å­˜ä¸¤ä»½æ•°æ®ï¼Œ MLA åªè¦å­˜ä¸€ä»½ Cache æ•°æ®\n",
    "2. å°† MQA çš„ KV æŠ•å½±çœ‹æˆæ˜¯ä¸€ç§ç‰¹å¾å‹ç¼©ï¼ˆ40å¤´->1å¤´ï¼Œ å‡å°‘97.5% æ•°æ®é‡ï¼‰ï¼Œå…¶ç®— MHA æ˜¯ KV å•å¤´shareæˆå¤šå¤´\n",
    "3. MLA ä»å•å¤´åˆ°å¤šå¤´çš„æ˜ å°„æ˜¯é€šè¿‡ä¸€ä¸ªå‡ç»´æŠ•å½±çŸ©é˜µå¾—åˆ°çš„ï¼Œ å¯¹äº MQA ç±»å‹ç¼©æŠ•å½±ï¼Œè¢« MLA è§†ä¸ºæ˜¯å°† latentç‰¹å¾ æŠ•å½±åˆ° éšç©ºé—´(learnableï¼‰\n",
    "\n",
    "MQA å…¬å¼åŒ–ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "k_\\text{mqa} = WK_\\text{down}(X), X \\in \\mathbb{R} ^{N \\times d}, W_\\text{down} \\in \\mathbb{R} ^{d  \\times d'},  \n",
    "$$\n",
    "\n",
    "$$\n",
    "K \\gets \\text{extend}(k_\\text{mqa})\n",
    "$$\n",
    "\n",
    "MLA å…¬å¼åŒ–ä¸º\n",
    "\n",
    "$$\n",
    "c = W_\\text{down}(X), X \\in \\mathbb{R} ^{N  \\times  d}, W_\\text{down} \\in \\mathbb{R} ^{d  \\times d'},\n",
    "$$\n",
    "$$\n",
    "K = WK_\\text{up}(c)ï¼Œ WK_\\text{up} \\in \\mathbb{R} ^{d'  \\times d},  \n",
    "$$\n",
    "\n",
    "MLA é¢å¤–å¢åŠ ä¸€äº›å·¥ç¨‹ trickï¼Œä¿è¯ç²¾åº¦ã€RoPEé€‚é…ç­‰é—®é¢˜ã€‚ \n",
    "\n",
    "\n",
    "ä¸ºä»€ä¹ˆ MLA æ•ˆæœæ¯” MQA å¥½ï¼Ÿæœ¬è´¨ä¸Šï¼š\n",
    "\n",
    "1. pretrained ä»»åŠ¡ä¸Šå­¦ä¹ å‹ç¼©/ä½ç§©è¡¨ç¤ºï¼Œæœ¬èº«é«˜ç»´ç‰¹å¾æœ‰å†—ä½™å’Œç¨€ç–çš„ç‰¹æ€§ï¼Œè¿™æ˜¯èƒ½å‹ç¼©çš„åŸºç¡€\n",
    "2. LoRA æŠ€æœ¯è¯æ˜ï¼Œç‰¹å¾çš„ä½ç§©è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ï¼Œé™¤äº†å‹ç¼©å¤–ï¼Œè¿˜å­˜åœ¨å‡ç»´æŠ•å½±ï¼Œè¿™æ˜¯ MQA ä¸å…·å¤‡çš„\n",
    "3. MLA ä¸ MHA æ˜¯å¦ç­‰æ•ˆï¼Ÿ å¯ä»¥è®¤ä¸º MLA çš„æŠ•å½±çŸ©é˜µåšäº†ä¸€æ¬¡çŸ©é˜µåˆ†è§£ Wq = WAWB å» è¿‘ä¼¼æ»¡å¤´ Wq = [dim,dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753857b-5bec-4b5a-a466-748692b910db",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab411a50-74bb-4712-92cb-9e8d0e34c498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1297c8dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba34e285-1b4a-468d-b718-c940df817129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ModelArgs object at 0x12eff52d0>\n"
     ]
    }
   ],
   "source": [
    "# @dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 64\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int =  2\n",
    "\n",
    "    # down ä¸¤è€…è¿œå°äº dim\n",
    "    dc_kv: int = 4\n",
    "    dc_q: int = 4\n",
    "\n",
    "bs = 3\n",
    "seq_len = 5\n",
    "\n",
    "config = ModelArgs()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2da701-0413-493d-9b44-47f5ca88b305",
   "metadata": {},
   "source": [
    "## Multi Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd60e9c3-258b-4231-a687-54532e6f7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads # 18/6 = 3\n",
    "\n",
    "        self.wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.wk = nn.Linear(self.dim, self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.dim, self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        Q, K, V = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # å¤šå¤´Q\n",
    "        Q = Q.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        Q = Q.transpose(1, 2)  # (bs, n_heads, seqlen, head_dim)\n",
    "        \n",
    "        # å•å¤´KV\n",
    "        K = K[:, None, :, :]\n",
    "        V = V[:, None, :, :]\n",
    "        print(Q.shape, K.shape)\n",
    "    \n",
    "        S = Q @ K.transpose(2, 3) / math.sqrt(self.head_dim)\n",
    "        P = F.softmax(S.float(), dim=-1)\n",
    "        Z = P @ V \n",
    "        Z = Z.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        O = self.wo(Z)\n",
    "        return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa607562-f290-427b-92aa-e0b0fdf91af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiQueryAttention(\n",
      "  (wq): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (wk): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (wv): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (wo): Linear(in_features=64, out_features=64, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MQA = MultiQueryAttention(config)\n",
    "print(MQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7e88bd-7074-4e01-a814-476238ad3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(bs, seq_len, config.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae17e52-a0a2-4d47-9176-8e9b00e62687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 5, 8]) torch.Size([3, 1, 5, 8])\n",
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "O = MQA(X)\n",
    "print(X.shape)\n",
    "print(O.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f7a65-fe89-4cd1-8f63-d03335ce4023",
   "metadata": {},
   "source": [
    "## Multi-Heads Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867810fa-ca2f-4822-95d8-bfe67dd0b1e5",
   "metadata": {},
   "source": [
    "### model\n",
    "\n",
    "1. ä»¥ä¸‹ç”¨downå’Œupæƒé‡çŸ©é˜µï¼Œä»£æ›¿ç›´æ¥çš„wq,wk,wv\n",
    "2. MLAçŸ©é˜µå‘ç”Ÿäºè®­ç»ƒä¹‹æ—¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01cdab46-f575-416c-b00e-997b1af7ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadsLatentAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads # 18/6 = 3\n",
    "        self.dc_kv = args.dc_kv # down_c kv\n",
    "        self.dc_q = args.dc_q # down_c kv\n",
    "\n",
    "        # Q\n",
    "        self.wq_down = nn.Linear(self.dim, args.dc_q, bias=False,)\n",
    "        self.wq_up = nn.Linear(args.dc_q, self.dim , bias=False,)\n",
    "\n",
    "        # å•ä¸ª C æ˜ å°„åˆ° Kã€V\n",
    "        self.wkv_down = nn.Linear(self.dim, args.dc_kv, bias=False,)\n",
    "        self.wk_up = nn.Linear(args.dc_kv, self.dim, bias=False,)\n",
    "        self.wv_up = nn.Linear(args.dc_kv, self.dim, bias=False,)\n",
    "        \n",
    "        self.wo = nn.Linear(self.dim, self.dim, bias=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb8e36d-d8fb-4768-8810-a1f36e94eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "MultiHeadsLatentAttention(\n",
      "  (wq_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wq_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wkv_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wk_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wv_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wo): Linear(in_features=64, out_features=64, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(config.dc_kv)\n",
    "print(config.dc_q)\n",
    "mla = MultiHeadsLatentAttention(config)\n",
    "print(mla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3ec01-8073-4a03-b8e5-b4263465974e",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53604079-8bb9-46bd-b518-545a97b9b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(bs, seq_len, config.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a000b1-e950-4ce8-80eb-1a445521f830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 4]) torch.Size([3, 5, 4])\n",
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "# è¯¥æ®µä»£ç æ˜¯å…³é”®ï¼Œå…ˆé™ç»´ï¼Œå†å‡ç»´\n",
    "C_Q = mla.wq_down(X)\n",
    "Q = mla.wq_up(C_Q)\n",
    "\n",
    "C = mla.wkv_down(X)\n",
    "K = mla.wk_up(C)\n",
    "V = mla.wv_up(C)\n",
    "\n",
    "print(C_Q.shape, C.shape) # latent dim\n",
    "print(Q.shape)\n",
    "print(K.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f29a56c-10f6-406b-bc00-b4d992dbc018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "# ä¸ä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›æ— å·®åˆ«\n",
    "Q = Q.reshape(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "K = K.reshape(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "V = V.reshape(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "\n",
    "Q = Q.transpose(1,2)\n",
    "K = K.transpose(1,2) # bs, n_heads, [seq_len, head_dim]\n",
    "V = V.transpose(1,2)\n",
    "\n",
    "S = Q @ K.transpose(2, 3) / math.sqrt(mla.head_dim)\n",
    "P = F.softmax(S.float(), dim=-1)\n",
    "Z = P @ V \n",
    "Z = Z.transpose(1, 2).contiguous().view(bs, seq_len, -1)\n",
    "O = mla.wo(Z)\n",
    "print(O.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c075a3-6f4d-4c25-8cd9-392be916ff4d",
   "metadata": {},
   "source": [
    "### çŸ©é˜µå¸æ”¶\n",
    "\n",
    "ä»¥ä¸Šå‚æ•°å‘ç”Ÿåœ¨è®­ç»ƒä¹‹æ—¶ï¼Œè®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥åšå¸æ”¶æ“ä½œï¼Œå…·ä½“æŒ‡wqå‚æ•°å’ŒWoå‚æ•°ï¼Œ\n",
    "\n",
    "æˆ‘ä»¬å†™å‡ºå¦‚ä¸‹ç­‰å¼ï¼š\n",
    "\n",
    "Q = wq_up * ( wq_down * h ) = (wq_up * wq_down) * h\n",
    "\n",
    "Q = wq * h \n",
    "\n",
    "ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "1. è®­ç»ƒæ—¶çœæ˜¾å­˜\n",
    "2. è®­ç»ƒå®Œï¼Œæ¨ç†Qæ»¡çŸ©é˜µä¿ç²¾åº¦ï¼Œ\n",
    "3. ç”±äºKV Cacheçš„å­˜åœ¨ï¼Œdecodingé˜¶æ®µæ—¶one-by-one tokenè¿›è¡Œqè®¡ç®—\n",
    "4. KV Cacheæå…·å‡å°‘å­˜å‚¨ä½“ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2485230f-5994-4023-afab-6cba4e8b0977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0488,  0.7823, -0.7392,  0.9954, -0.0828], grad_fn=<SliceBackward0>)\n",
      "tensor([-1.0488,  0.7823, -0.7392,  0.9954, -0.0828])\n"
     ]
    }
   ],
   "source": [
    "WQ = (mla.wq_up.weight.data @ mla.wq_down.weight.data).t() \n",
    "\n",
    "C_Q = mla.wq_down(X)\n",
    "Q = mla.wq_up(C_Q)\n",
    "print(Q[0,0,:5])\n",
    "\n",
    "Q = X @ WQ\n",
    "print(Q[0,0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf785cf2-4dc8-42a0-a2bb-381f60107bd1",
   "metadata": {},
   "source": [
    "### çŸ©é˜µå¸æ”¶åçš„forward(éè®­ç»ƒé˜¶æ®µï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a76ddd-55b8-4589-95f1-c7a66d17e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = X @ WQ\n",
    "C = mla.wkv_down(X)\n",
    "K = mla.wk_up(C)\n",
    "V = mla.wv_up(C)\n",
    "\n",
    "# do attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ec2d5-f4f0-4bb2-9461-4946b0e3ce51",
   "metadata": {},
   "source": [
    "### KV cacheå­˜çš„æ˜¯ä»€ä¹ˆï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1c071-fd97-4e3c-8985-4385afbc70c4",
   "metadata": {},
   "source": [
    "ä¼ ç»ŸMHAçš„KV Cacheå¤§å°: `[2, bs, seq_len, n_kv_head * head_dim]`, å…¶ä¸­2ä»£è¡¨Kå’ŒV\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬å­˜å‚¨MLA upåçš„çŸ©é˜µK,V cacheï¼Œé‚£ä¹ˆä¸MHAå­˜å‚¨æ— å·®åˆ«\n",
    "\n",
    "é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å­˜å‚¨kv downçš„çŸ©é˜µï¼šå³ c_kv = w_kv_down @ h, æ­¤æ—¶ï¼š\n",
    "\n",
    "`[1, bs, seq_len, dc_kv]`\n",
    "\n",
    "å¦‚æœåŸæ¥  2 * n_kv_head * head_dim = dim = 2 * 4096, å¦‚æœdc_kv  = 512\n",
    "\n",
    "é‚£ä¹ˆMLA KV-cacheå°±ä¸ºMHAçš„ 1 / 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe65fa1-04d5-483f-9be0-e0e2d3c28165",
   "metadata": {},
   "source": [
    "### MLAå‹ç¼©çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3126fc9-e532-4ee8-b73f-cf75cbbf5f85",
   "metadata": {},
   "source": [
    "è®¡ç®—æ—¶é—´æ¢ç©ºé—´\n",
    "\n",
    "å­˜å‚¨æ—¶åˆ»ï¼Œw_k_up\n",
    "\n",
    "decodingæ—¶åˆ» å³å°†ï¼š\n",
    "\n",
    "`K = w_k_up @ c_kv`\n",
    "\n",
    "`V = w_v_up @ c_kv`\n",
    "\n",
    "å‹ç¼©KV-Cacheçš„é‡çš„æ„ä¹‰ï¼Ÿä»¥vLLMæ¥è¯´ï¼Œå‡å°‘KV-Cacheé‡ã€‚æ¨ç†æœåŠ¡èƒ½è·‘æ›´å¤§çš„batch-sizeï¼Œä»è€Œæé«˜inferenceï¼Œdecodingçš„æ•ˆç‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadf938-8278-4484-a22f-972666622ece",
   "metadata": {},
   "source": [
    "### MLAä¸‹ä½ç½®ç¼–ç é—®é¢˜\n",
    "\n",
    "1. å¸¸è§„ç®—æ³•ä¸ºï¼šRoPE(W_k_up( w_k_down(h) )), è¿™é‡Œçš„RoPEæ²¡æ³•ä½ç§©åˆ†è§£\n",
    "2. å¯ä»¥å†™ä¸ºattentionï¼š `Rk @ Wkup @ wkdown(h)^T` `@` `[Rq @ Wqup @ wqdown(h)]^T` \n",
    "3. ä¸Šå¼æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬æ‰€å­˜å‚¨kv cacheæ˜¯wkdown(h)ä¸º `seq x dc_kv`, é‚£ä¹ˆæˆ‘ä»¬æ¯æ¬¡è®¡ç®—å‡ºäº†è¦upï¼Œè€Œä¸”è¿˜è¦å¯¹**æ‰€æœ‰token**å¢åŠ RoPEæ“ä½œ\n",
    "4. æœ‰ä»€ä¹ˆæ–¹å¼å¯ä»¥å‡å°‘RoPEæ“ä½œå—ï¼Ÿè§£å†³æ–¹æ¡ˆä¸ºåœ¨hä¸Šï¼Œå¢åŠ é¢å¤–çš„å‚æ•°çŸ©é˜µæ¯”å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e368027c-134e-43df-9a37-307c7b2b8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadsLatentAttention_withRoPE(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        # self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads # 18/6 = 3\n",
    "        self.dc_kv = args.dc_kv\n",
    "        self.dc_q = args.dc_q\n",
    "\n",
    "        # Q\n",
    "        self.wq_down = nn.Linear(self.dim, args.dc_q, bias=False,)\n",
    "        self.wq_up = nn.Linear(args.dc_q, self.dim , bias=False,)\n",
    "\n",
    "        # å•ä¸ª C æ˜ å°„åˆ° Kã€V\n",
    "        self.wkv_down = nn.Linear(self.dim, args.dc_kv, bias=False,)\n",
    "        self.wk_up = nn.Linear(args.dc_kv, self.dim, bias=False,)\n",
    "        self.wv_up = nn.Linear(args.dc_kv, self.dim, bias=False,)\n",
    "        \n",
    "        self.wo = nn.Linear(self.dim, self.dim, bias=False,)\n",
    "\n",
    "        # RoPE Weight\n",
    "        # K æ¯å¤´ä¸€æ ·ï¼Œ Qæ¯å¤´ä¸ä¸€æ ·\n",
    "        self.wq_up_rope = nn.Linear(self.dc_q, self.dim , bias=False,)\n",
    "        self.wk_head_rope = nn.Linear(self.dim, self.head_dim , bias=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a237ae87-cdfa-4a88-8276-0d618cd67517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadsLatentAttention_withRoPE(\n",
      "  (wq_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wq_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wkv_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wk_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wv_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wo): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (wq_up_rope): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wk_head_rope): Linear(in_features=64, out_features=8, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mla_rope = MultiHeadsLatentAttention_withRoPE(config)\n",
    "print(mla_rope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5796accc-0d9f-4212-8f99-673018fd812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# å¢åŠ rope\n",
    "C_Q = mla_rope.wq_down(X)\n",
    "Q = mla_rope.wq_up(C_Q)\n",
    "print(Q.shape)\n",
    "\n",
    "C_KV = mla_rope.wkv_down(X)\n",
    "K = mla_rope.wk_up(C_KV)\n",
    "V = mla_rope.wv_up(C_KV)\n",
    "\n",
    "\n",
    "# ä½ç½®ç¼–ç ç›¸å…³\n",
    "R_Q = mla_rope.wq_up_rope(C_Q) #å¤šå¤´\n",
    "R_K = mla_rope.wk_head_rope(X) #å•å¤´\n",
    "print(R_Q.shape)\n",
    "print(R_K.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d890c66-4891-4264-ba0a-60c9fd56269d",
   "metadata": {},
   "source": [
    "äº§ç”Ÿæ–°çš„ç–‘é—®ï¼Œr_q å’Œ r_k ç»´åº¦ä¸åŒï¼Œé‚£ä¹ˆåšropeçš„dimå¦‚ä½•å¤„ç†ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b96c3e98-42d3-492f-b784-2b09d2976757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 5, 8])\n",
      "torch.Size([3, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# ç®€æ˜“å®ç° RoPE\n",
    "\n",
    "R_Q = R_Q.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim).transpose(1,2)\n",
    "R_K = R_K.unsqueeze(dim = 1).repeat( repeats = [1, mla_rope.n_heads, 1, 1])\n",
    "print(R_Q.shape)\n",
    "print(R_K.shape)\n",
    "\n",
    "## åµŒå…¥rope\n",
    "rope_matrix_q = [torch.randn(mla_rope.head_dim, mla_rope.head_dim)] * seq_len\n",
    "rope_matrix_k = [torch.randn(mla_rope.head_dim, mla_rope.head_dim)] * seq_len\n",
    "def apply_rope_q(X, seq_len, rope_matrix):\n",
    "    for i in range(seq_len):\n",
    "        X[:, i, :] = X[:, i, :] @ rope_matrix[i]\n",
    "    return X\n",
    "    \n",
    "RoPE_Q = apply_rope_q(R_Q, seq_len, rope_matrix_q)\n",
    "RoPE_K = apply_rope_q(R_K, seq_len, rope_matrix_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1174ea-6c9c-4b8b-bde0-1e863c87012a",
   "metadata": {},
   "source": [
    "å¯¹äºqæ¯å¤´ï¼Œcatä¸ä¸€æ ·çš„ä½ç½®ä¿¡æ¯\n",
    "\n",
    "å¯¹äºkæ¯å¤´ï¼Œcatä¸€æ ·çš„ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47e9bced-8fd6-43ef-80a9-88ca966998ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 5, 8])\n",
      "torch.Size([3, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# ä¸ä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›æ— å·®åˆ«\n",
    "# æœª\n",
    "Q = Q.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim).transpose(1,2)\n",
    "K = K.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim).transpose(1,2)\n",
    "V = V.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim).transpose(1,2)\n",
    "\n",
    "print(Q.shape) \n",
    "print(K.shape)\n",
    "\n",
    "# cat æ“ä½œ\n",
    "Q = torch.cat((Q, RoPE_Q), dim = -1) # Q ä¹Ÿç§°ä¸ºæ˜¯ Q_nope\n",
    "K = torch.cat((K, RoPE_K), dim = -1)\n",
    "\n",
    "print(Q.shape) \n",
    "print(K.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76d7d0b6-c115-48c6-950a-f4c7a2b79d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "### å¸¸è§„attention\n",
    "# scores = query @ key.transpose(2,3) # keys: bs, n_heads, [head_dim, seq_len]\n",
    "# keys: bs, n_heads, [2head_dim, seq_len]\n",
    "\n",
    "S = Q @ K.transpose(2, 3) / math.sqrt(2 * mla_rope.head_dim) # catådimç»´åº¦å˜åŒ–ï¼Œåº•æ•°ä¹Ÿæœ‰å˜åŒ–\n",
    "P = F.softmax(S.float(), dim=-1)\n",
    "Z = P @ V \n",
    "Z = Z.transpose(1, 2).contiguous().view(bs, seq_len, -1)\n",
    "O = mla.wo(Z)\n",
    "print(O.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b825d-6b48-442c-b0ad-345d7b8e58c4",
   "metadata": {},
   "source": [
    "## å¢åŠ é—®é¢˜\n",
    "\n",
    "1. ä¸ºä»€ä¹ˆä½ç½®ç¼–ç è¦åˆ†ç¦»\n",
    "2. qå’Œkçš„ä½ç½®ç¼–ç ç»´åº¦ä¸ä¸€æ ·ï¼Œé‚£ä¹ˆropeçš„ç»´åº¦æ˜¯å¦ä¸€æ ·\n",
    "3. v_up å¦‚ä½•è¢« wo å¸æ”¶\n",
    "5. å†™å‡ºå¸¦kv_cacheç‰ˆæœ¬çš„MLAï¼ŒåŠdecodingä»£ç \n",
    "6. MLAè®­ç»ƒçš„æ˜¾å­˜è®¡ç®—\n",
    "7. MLAå¹¶è¡Œå‚æ•°åˆ†é…ã€å¼ é‡å¹¶è¡Œç»†èŠ‚å’Œé€šä¿¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174e9d5-9781-49bd-ada2-e162cc4f7305",
   "metadata": {},
   "source": [
    "### ç­”æ¡ˆ1\n",
    "\n",
    "1. ä½ç½®ç¼–ç æ•æ„Ÿ\n",
    "2. ä¸åˆ†ç¦»çš„è¯ï¼Œinferenceé˜¶æ®µè¦å¯¹kvåšé‡å¤ropeå˜æ¢ï¼Œ\n",
    "3. å¦‚æœåˆ†ç¦»é‚£ä¹ˆç›´æ¥æ‹¼æ¥ç®—å¥½çš„rope k(æˆ‘ç†è§£å…¶å®ä½ç½®ç¼–ç rope kä¹Ÿåº”è¯¥æ˜¯cacheèµ·æ¥çš„ï¼‰\n",
    "\n",
    "åŸè®ºæ–‡\n",
    "\n",
    "> To be specific, RoPE is position-sensitive for both keys and queries\n",
    "\n",
    "> As a result, we must recompute the keys for all the prefix tokens during inference, which will significantly hinder the inference efficiency\n",
    "\n",
    "> During inference, the decoupled key should also be cached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b9f4a-a2d4-46b4-898c-eb9be2154494",
   "metadata": {},
   "source": [
    "### ç­”æ¡ˆ2:\n",
    "\n",
    ">  Therefore, DeepSeek-V2 requires a total KV cache containing $(ğ‘‘_ğ‘ + ğ‘‘^ğ‘…_â„)ğ‘™ $ elements.\n",
    "1. å…ˆè¯´KV cacheé‡æ˜¯å¤šå°‘, læ˜¯é•¿åº¦ï¼Œd_cæ˜¯latentç»´åº¦ï¼Œ$d^R_h$æ˜¯å•å¤´çš„å‘é‡ç»´åº¦\n",
    "\n",
    "2. ä¸ºä»€ä¹ˆQKä½ç½®ç¼–ç æœ‰åŒºåˆ«ï¼ŒQæ˜¯å¤šå¤´$d^R_h * h$, hæ˜¯å¤´æ•°ï¼Œé‚£ä¹ˆå¤šå¤´ä¿ç²¾åº¦\n",
    "\n",
    "3. rope Kæ˜¯å•å¤´ï¼Œä»cacheè§’åº¦æ¥çœ‹ï¼Œå­˜å•å¤´k ropeæ˜¯ç»æµçš„ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ ï¼Œå¦‚æœæ˜¯k ropeå¤šå¤´ç»´åº¦æœ¬è´¨ä¸Šä¸å¸¸è§„KVæ— å·®åˆ«\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dad02a-3509-42d4-8d82-13013262be1b",
   "metadata": {},
   "source": [
    "### ç­”æ¡ˆ3\n",
    "\n",
    "åŸè®ºæ–‡å…¬å¼\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "   [\\mathbf{v}_{t, 1}^{C};\\mathbf{v}_{t, 2}^{C};...;\\mathbf{v}_{t, n_{h}}^{C}] = \\mathbf{v}_{t}^{C} &= W^{UV} \\mathbf{c}_{t}^{KV}, \\\\\n",
    "    \\mathbf{o}_{t, i} &= \\sum_{j=1}^{t} \\operatorname{Softmax}_j(\\frac{\\mathbf{q}_{t, i}^T \\mathbf{k}_{j, i}}{\\sqrt{d_{h} + d_{h}^{R}}}) \\mathbf{v}_{j, i}^{C}, \\\\\n",
    "    \\mathbf{u}_{t} &= W^{O} [\\mathbf{o}_{t, 1};\\mathbf{o}_{t, 2};...;\\mathbf{o}_{t, n_{h}}],\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "å…³äºå¸æ”¶, åŸè®ºæ–‡æåˆ°uvå¸æ”¶åˆ°wo\n",
    "\n",
    "> Fortunately, due to the associative law of matrix multiplication, we can absorb $W^{UK}$ into $W^{UQ}$, and $W^{UV}$ into $W^{O}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77b58811-4eb9-4e68-b05e-3f83c34e7231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-701.2956, -410.6637, -586.3719,  204.6843,   56.4123],\n",
      "        [-258.3982, -733.2527, -224.1205,  125.4380,   -6.8913],\n",
      "        [-258.3982, -733.2527, -224.1205,  125.4380,   -6.8913],\n",
      "        [ 468.6087,  -20.3747, 1123.2045, -544.6324,  241.1628],\n",
      "        [-264.7623, -486.6645, -184.1303,  127.2174,  143.6075]])\n",
      "tensor([[-701.2963, -410.6649, -586.3724,  204.6843,   56.4113],\n",
      "        [-258.3976, -733.2532, -224.1204,  125.4391,   -6.8913],\n",
      "        [-258.3976, -733.2532, -224.1204,  125.4391,   -6.8913],\n",
      "        [ 468.6092,  -20.3742, 1123.2051, -544.6312,  241.1630],\n",
      "        [-264.7619, -486.6647, -184.1305,  127.2183,  143.6076]])\n"
     ]
    }
   ],
   "source": [
    "# ç¼–ç¨‹ä¸¾ä¾‹ WUV åˆ° WOçš„çŸ©é˜µå¸æ”¶\n",
    "\n",
    "c = 64\n",
    "d = 4096\n",
    "seq_len = 16 # length\n",
    "\n",
    "c_kv = torch.randn(seq_len, c)\n",
    "W_O = torch.randn(d, d)\n",
    "W_UV = torch.randn(c, d)\n",
    "\n",
    "Q = torch.randn(seq_len, d) # ignore rope q\n",
    "K = torch.randn(seq_len, d) # ignore rope k\n",
    "\n",
    "# åŸè®ºæ–‡å…¬å¼ä»£ç ï¼Œå¿½ç•¥åº•æ•° \n",
    "V = c_kv @ W_UV\n",
    "S = Q @ K.t() \n",
    "P = F.softmax(S, dim = -1)\n",
    "O = P @ V\n",
    "U = O @ W_O\n",
    "print(U[:5,:5])\n",
    "\n",
    "# å¸æ”¶ç‰ˆæœ¬\n",
    "w_uv_absord = W_UV @ W_O  \n",
    "# é‚£ä¹ˆå®é™…éƒ¨ç½²æ—¶ï¼Œå°±ä¿ç•™è¿™ä¸ªå‚æ•°çŸ©é˜µ w_uv_absordã€‚\n",
    "# åŸæœ¬å­˜ dxd + cxd, é‚£ä¹ˆç°åœ¨åªå­˜ cxd (å‡å°‘çš„å‚æ•°é‡ä½“æ„Ÿéå¸¸æ˜æ˜¾)\n",
    "\n",
    "V = c_kv @ w_uv_absord\n",
    "S = Q @ K.t()  \n",
    "P = F.softmax(S, dim = -1)\n",
    "O = P @ V\n",
    "# U = O @ W_o # ä¸éœ€è¦å†å˜æ¢äº†\n",
    "print(O[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2222536-eee7-4684-bdc9-35604b41b552",
   "metadata": {},
   "source": [
    "### ç­”æ¡ˆ4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b12c24-069b-42b7-b613-fc1e67c3ee69",
   "metadata": {},
   "source": [
    "1. åˆ†ææœ‰å“ªäº›çŸ©é˜µå¯ä»¥å¸æ”¶\n",
    "\n",
    "> Fortunately, due to the associative law of matrix multiplication, we can absorb $W^{UK}$ into $W^{UQ}$, and $W^{UV}$ into $W^{O}$.\n",
    "\n",
    "2. å¦å¤–qçš„çŸ©é˜µå¯ä»¥å¸æ”¶ï¼ŒqçŸ©é˜µé™¤äº†å¸¸è§„çš„ wq x = Q ä¹‹å¤–ï¼Œ è¿˜å¾—åšä¸€æ¬¡ wq_rope x = q_rope, å¢åŠ ä¸€æ¬¡çŸ©é˜µä¹˜\n",
    "\n",
    "3. kv cacheè¦å­˜ 1. `rope k`ï¼Œ 2.latent `C`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d95eba-0a30-4abe-8614-c8f75fbcfa19",
   "metadata": {},
   "source": [
    "### ç­”æ¡ˆ5å¤æ‚ï¼Œå…ˆskip\n",
    "\n",
    "### ç­”æ¡ˆ6åˆ†æ\n",
    "\n",
    "#### åˆ‡åˆ†æ–¹æ¡ˆA\n",
    "\n",
    "1. å…ˆè¯´down å‚æ•°çŸ©é˜µï¼Œ Wk_down ( d x c ), å¦‚æœæŒ‰ç…§cåˆ‡åˆ†ï¼Œæ¯ä¸ªGPU å­˜å‚¨ wk_down_shard (d x c'), è¾“å‡º h(l,d) -> C(l, c')\n",
    "\n",
    "2. up çŸ©é˜µï¼Œwk_up (c x d), æŒ‰ç…§cåˆ‡åˆ†, æ¯ä¸ªgpuå­˜å‚¨ wk_up_shard(c' x d) ï¼Œæ¯ä¸ªGPUè¾“å‡º   C(l, c') x  (c', d) -> K(l, d)\n",
    "\n",
    "è¿™é‡Œå°±æœ‰é—®é¢˜Kåœ¨æ¯ä¸ªGPUä¸Šéƒ½æ˜¯æ»¡å¤´çš„ï¼Œ é‡æ–°åˆ‡åˆ†ï¼š\n",
    "\n",
    "#### åˆ‡åˆ†æ–¹æ¡ˆB \n",
    "\n",
    "1. å…ˆè¯´down å‚æ•°çŸ©é˜µï¼Œ Wk_down ( d x c ), å¦‚æœæŒ‰ç…§cåˆ‡åˆ†ï¼Œæ¯ä¸ªGPU å­˜å‚¨ wk_down_shard (d x c'), è¾“å‡º h(l,d) -> C(l, c')\n",
    "\n",
    "2. å°†æ¯ä¸ª C(l, c') åˆ†å‘ç»™å…¶ä»–GPUï¼Œ é‚£ä¹ˆæ¯å—GPUéƒ½æœ‰å®Œæ•´çš„ C(l, c)ï¼Œ # è€ƒè™‘åˆ°câ€˜æ˜¯éå¸¸å°é‡çš„, æ¯”å¦‚cæ˜¯64ï¼Œ câ€˜æ˜¯8\n",
    "\n",
    "3. up çŸ©é˜µï¼Œwk_up (c x d), æŒ‰ç…§dåˆ‡åˆ†,  æ¯ä¸ªgpuå­˜å‚¨ wk_up_shard(c x d'), æ¯ä¸ªGPUè¾“å‡º C(l, c) x  (c, d') -> K(l, d')\n",
    "\n",
    "æ‰€ä»¥è¿™æ ·ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹å­˜åœ¨å„GPUä¸Šï¼Œé‚£ä¹ˆå°±å¯ä»¥head-parallelç®— å¤šå¤´æ³¨æ„åŠ›äº†ã€‚\n",
    "\n",
    "- å¾…åˆ†æï¼Œ ä¸Šè¿°æœªåˆ†ææ˜¯å¦backwardé€šä¿¡å‹å¥½\n",
    "\n",
    "\n",
    "ä¾æ¬¡ç±»æ¨å¯ä»¥è®¾è®¡å‡ºMLAçš„tensor parallelæ¶æ„\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

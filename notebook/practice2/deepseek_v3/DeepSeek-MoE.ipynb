{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b361efba-75e9-4eb5-9911-391af7ccc93d",
   "metadata": {},
   "source": [
    "# DeepSeek-V3 MoE\n",
    "\n",
    "DeepSeek-V3 671B 是一个大型的 MoE 模型，其 MoE 模型特点为：\n",
    "\n",
    "1. 包含路由专家(router experts)和共享专家(shared experts), 前者是 sparse 的\n",
    "2. 增加 bias 来帮助模型学习负载均衡\n",
    "3. 序列级别负载均衡 loss, 计算方式跟随 Switch Transformer 模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b6910-f9be-4a4d-af36-9f4d051b75a3",
   "metadata": {},
   "source": [
    "MoE 公式\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{h}_{t}^{\\prime} &= \\mathbf{u}_{t} + \\sum_{i=1}^{N_{s}} {\\text{FFN}^{(s)}_{i}\\left( \\mathbf{u}_{t} \\right)} + \\sum_{i=1}^{N_r} {g_{i,t} \\text{FFN}^{(r)}_{i}\\left( \\mathbf{u}_{t} \\right)}, \\\\\n",
    "    g_{i,t} &= \\frac{g^{\\prime}_{i,t}}{\\sum_{j=1}^{N_r} g^{\\prime}_{j,t}}, \\\\\n",
    "    g^{\\prime}_{i,t} &= \\begin{cases} \n",
    "    s_{i,t}, & s_{i,t} \\in \\text{Topk} (\\{ s_{j, t} | 1 \\leq j \\leq N_r \\}, K_{r}), \\\\\n",
    "    0, & \\text{otherwise}, \n",
    "    \\end{cases} \\\\\n",
    "    s_{i,t} &= \\text{Sigmoid} \\left( {\\mathbf{u}_{t}}^{T} \\mathbf{e}_{i} \\right),\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "可学习的负载均衡偏置\n",
    "\n",
    "\\begin{align}\n",
    "    g^{\\prime}_{i,t} &= \\begin{cases} \n",
    "    s_{i,t}, & s_{i,t} + b_i \\in \\operatorname{Topk} (\\{ s_{j, t} + b_j | 1 \\leq j \\leq N_r \\}, K_{r}), \\\\\n",
    "    0, & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "序列级别负载均衡\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_{\\mathrm{Bal}} &= \\alpha \\sum_{i=1}^{N_r}{f_i P_i}, \\\\\n",
    "    % f_i = \\frac{N_r}{K_r T} \\sum_{t=1}^{T} \\mathcal{1}( \\text{Expert $i$ } & \\text{belongs to the Top-$K_r$ set for Token $t$} ), \\\\\n",
    "    f_i = \\frac{N_r}{K_r T} \\sum_{t=1}^{T} \\mathcal{1} & \\left( s_{i,t} \\in \\operatorname{Topk} ( \\{ s_{j, t} | 1 \\leq j \\leq N_r \\}, K_{r} ) \\right), \\\\\n",
    "    s^{\\prime}_{i,t} &= \\frac{s_{i,t}}{\\sum_{j=1}^{N_r} s_{j,t}}, \\\\\n",
    "    P_i &= \\frac{1}{T} \\sum_{t=1}^{T}{s^{\\prime}_{i,t}},\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa14f3-c465-4878-b0ef-89c3bc60f692",
   "metadata": {},
   "source": [
    "## DeepSeek-V3 MoE basic \n",
    "\n",
    "实现一个简易版的模型, 了解其主要工作模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4137703-3bdf-4438-a881-5c888792c973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b78d9f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5112c4c9-2092-4b71-a7c6-a46301c128c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDeepSeekV3MoE(nn.Module):\n",
    "    def __init__(self, dim, \n",
    "                 expert_nums = 8, \n",
    "                 top_k = 2, \n",
    "                 shared_expert_nums = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Route Experts: \n",
    "        self.expert_nums = expert_nums\n",
    "        self.k = top_k\n",
    "        self.experts = nn.ModuleList(\n",
    "            [ nn.Linear(dim, dim) for _ in range(self.expert_nums)]\n",
    "        )\n",
    "        self.w_gate = nn.Linear(dim, self.expert_nums)\n",
    "        \n",
    "        # Shared Experts: \n",
    "        self.shared_expert_nums = shared_expert_nums\n",
    "        self.shared_experts = nn.ModuleList(\n",
    "            [ nn.Linear(dim, dim) for _ in range(self.shared_expert_nums)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 整体前向计算\n",
    "        y_route, weight, idx = self.forward_route(x) \n",
    "        y_shared = self.forward_shared(x) \n",
    "        y = x + y_route + y_shared\n",
    "        load_loss = self.load_balance_sequence_wise(weight, idx)\n",
    "        return y, load_loss\n",
    "    \n",
    "    def forward_route(self, x):\n",
    "        # 默认稀疏选择专家 0\n",
    "        return self.experts[0](x), None, None\n",
    "\n",
    "    def forward_shared(self, x):\n",
    "        y = torch.zeros_like(x)\n",
    "        for i in range(self.shared_expert_nums):\n",
    "            y += self.shared_experts[i](x) # not gate\n",
    "        return y\n",
    "        \n",
    "    def load_balance_sequence_wise(self, weight, idx):\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066c0d07-51e1-4f3e-8440-5807c9230b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeekV3MoE(\n",
      "  (experts): ModuleList(\n",
      "    (0-19): 20 x Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (w_gate): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (shared_experts): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "bs = 2\n",
    "seq_len = 8\n",
    "dim = 64\n",
    "expert_nums = 20\n",
    "expert_shared_nums = 4\n",
    "topk = 2\n",
    "x = torch.randn(bs, seq_len, dim)\n",
    "label = torch.randn(bs, seq_len, dim)\n",
    "\n",
    "# model\n",
    "model = BasicDeepSeekV3MoE(dim, expert_nums=expert_nums, top_k = topk, shared_expert_nums = expert_shared_nums)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915d6ac4-9745-4b6f-9b1c-ba76d6bdfa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 64])\n",
      "torch.Size([2, 8, 64])\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "y, load_loss = model(x)\n",
    "print(y.shape)\n",
    "print(load_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2b26a-e810-4380-bc00-3654c2507da0",
   "metadata": {},
   "source": [
    "## Expert \n",
    "\n",
    "在公式里标记为 FFN 作为一个 expert, 实现一个 SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a70c6491-d14d-405e-a2b7-79bb71351dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module): # expert\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim\n",
    "        self.dim_out = self.dim_in * 8 // 3\n",
    "        self.w1 = nn.Linear(self.dim_in, self.dim_out , bias = False)\n",
    "        self.w_act = nn.Linear(self.dim_in, self.dim_out, bias = False) \n",
    "        self.w2 = nn.Linear(self.dim_out, self.dim_in, bias = False)  \n",
    "        self.SiLU = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.w1(x)\n",
    "        h_act = self.w_act(x)\n",
    "        h_act_up = self.SiLU(h_act) * h\n",
    "        output = self.w2(h_act_up)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa9892-d8a2-4737-a590-5eb170770463",
   "metadata": {},
   "source": [
    "## MoE\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{h}_{t}^{\\prime} &= \\mathbf{u}_{t} + \\sum_{i=1}^{N_{s}} {\\operatorname{FFN}^{(s)}_{i}\\left( \\mathbf{u}_{t} \\right)} + \\sum_{i=1}^{N_r} {g_{i,t} \\operatorname{FFN}^{(r)}_{i}\\left( \\mathbf{u}_{t} \\right)}, \\\\\n",
    "    g_{i,t} &= \\frac{g^{\\prime}_{i,t}}{\\sum_{j=1}^{N_r} g^{\\prime}_{j,t}}, \\\\\n",
    "    g^{\\prime}_{i,t} &= \\begin{cases} \n",
    "    s_{i,t}, & s_{i,t} \\in \\operatorname{Topk} (\\{ s_{j, t} | 1 \\leq j \\leq N_r \\}, K_{r}), \\\\\n",
    "    0, & \\text{otherwise}, \n",
    "    \\end{cases} \\\\\n",
    "    s_{i,t} &= \\operatorname{Sigmoid} \\left( {\\mathbf{u}_{t}}^{T} \\mathbf{e}_{i} \\right),\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a457006f-7511-468e-b629-6a9e25f1c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeekV3MoE(\n",
      "  (experts): ModuleList(\n",
      "    (0-19): 20 x Expert(\n",
      "      (w1): Linear(in_features=64, out_features=170, bias=False)\n",
      "      (w_act): Linear(in_features=64, out_features=170, bias=False)\n",
      "      (w2): Linear(in_features=170, out_features=64, bias=False)\n",
      "      (SiLU): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (w_gate): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (shared_experts): ModuleList(\n",
      "    (0-3): 4 x Expert(\n",
      "      (w1): Linear(in_features=64, out_features=170, bias=False)\n",
      "      (w_act): Linear(in_features=64, out_features=170, bias=False)\n",
      "      (w2): Linear(in_features=170, out_features=64, bias=False)\n",
      "      (SiLU): SiLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeepSeekV3MoE(nn.Module):\n",
    "    def __init__(self, dim, expert_nums = 8, top_k = 2, shared_expert_nums = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Route Experts: \n",
    "        self.expert_nums = expert_nums\n",
    "        self.k = top_k\n",
    "        self.experts = nn.ModuleList([ Expert(dim) for _ in range(self.expert_nums)])\n",
    "        self.w_gate = nn.Linear(dim, self.expert_nums)\n",
    "\n",
    "        # Auxiliary-Loss-Free Load Balancing\n",
    "        self.bias = torch.nn.Parameter(torch.zeros( expert_nums )) \n",
    "        self.alpha = 0.001\n",
    "        \n",
    "        # Shared Experts: \n",
    "        self.shared_expert_nums = shared_expert_nums\n",
    "        self.shared_experts = nn.ModuleList([ Expert(dim) for _ in range(self.shared_expert_nums)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_route, weight, idx = self.forward_route(x) \n",
    "        y_shared = self.forward_shared(x) \n",
    "        y = x + y_route + y_shared\n",
    "        # load_loss = self.load_balance_sequence_wise(weight, idx)\n",
    "        # load_loss = 1.0\n",
    "        return y, weight, idx\n",
    "    \n",
    "    def forward_route(self, x):\n",
    "\n",
    "        # gate 处理\n",
    "        g = self.w_gate(x)\n",
    "        g = F.sigmoid(g) # sigmoid 代替 softmax\n",
    "        weight, idx = torch.topk(g, k = self.k, dim = -1) \n",
    "        weight_norm = weight / (weight.sum(dim=-1, keepdim= True) + 1e-20)\n",
    "\n",
    "        # dispatch\n",
    "        expert_results = [None] * self.expert_nums\n",
    "        for i in range(self.expert_nums):\n",
    "            cur_pos = torch.where(idx == i) \n",
    "            x_select = x[cur_pos[0], cur_pos[1], :] \n",
    "            if x_select.shape[0] > 0: \n",
    "                expert_results[i] = self.experts[i](x_select)\n",
    "        \n",
    "        # combine\n",
    "        y_result = torch.zeros_like(x) \n",
    "        for i in range(self.expert_nums):\n",
    "            cur_pos = torch.where(idx == i) \n",
    "            if expert_results[i] != None:\n",
    "                y_result[cur_pos[0], cur_pos[1], :] += \n",
    "                expert_results[i] * weight_norm[cur_pos[0], cur_pos[1], cur_pos[2]].unsqueeze(-1)\n",
    "        \n",
    "        return y_result, g, idx \n",
    "\n",
    "    def forward_shared(self, x):\n",
    "        y = torch.zeros_like(x)\n",
    "        for i in range(self.shared_expert_nums):\n",
    "            y += self.shared_experts[i](x) # not gate\n",
    "        return y\n",
    "\n",
    "# model\n",
    "model = DeepSeekV3MoE(dim, expert_nums=expert_nums, top_k = topk, shared_expert_nums = expert_shared_nums)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d77a90a0-7ea8-4a30-9d74-afe7541064c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 64])\n",
      "torch.Size([2, 8, 64])\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "y, weight, idx = model(x)\n",
    "print(y.shape)\n",
    "print(load_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ba43f-277c-40db-ab02-c35355f2d03e",
   "metadata": {},
   "source": [
    "## Load Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "860f7f4f-dd1c-4101-a487-4b111b48a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0005], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def load_balance_sequence_wise(model, s, idx):\n",
    "    '''\n",
    "    sequence-wise 是为了在 inference 阶段, prefill 时能够均匀分散在各 GPU 中。\n",
    "    更直观的理解是当 bsz 为 1 时, prefill 可以直接受益。\n",
    "    '''\n",
    "    Nr = model.expert_nums # n routes expert\n",
    "    bs, seq_len, dim = s.shape # seq : pad token 要去除，或者增加 mask, 避免计入 loss 里\n",
    "\n",
    "    l_lab = torch.zeros(1)\n",
    "\n",
    "    for k in range(bs):\n",
    "\n",
    "        # Compute fi\n",
    "        fi = torch.zeros(Nr)\n",
    "        # pi = torch.zeros(self.expert_nums)\n",
    "\n",
    "        # seq-count \n",
    "        seq_expert_count = torch.zeros(Nr)\n",
    "        idx_seq = idx[k,:,:]\n",
    "\n",
    "        for i in range(model.expert_nums):\n",
    "            seq_expert_count[i] = torch.where(idx_seq == i)[1].numel()\n",
    "        \n",
    "        fi = Nr / (model.k * seq_len) * seq_expert_count\n",
    "\n",
    "        # Compute pi\n",
    "        s_seq = s[k, :, :]\n",
    "        si_ = s / s.sum(dim = -1, keepdim = True)\n",
    "        pi = si_.sum(dim = 0) / seq_len\n",
    "\n",
    "        l_bal_seq = (fi * pi).sum() / seq_len # seq_len_no_pad or use mask\n",
    "        l_lab += l_bal_seq\n",
    "        \n",
    "    l_lab = model.alpha * l_lab\n",
    "    return l_lab\n",
    "\n",
    "loss = load_balance_sequence_wise(model, weight, idx)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd384a-eddf-4f87-b3cb-3e8bfebb8f22",
   "metadata": {},
   "source": [
    "## 修正门控权重\n",
    "\n",
    "基于 $s_{j, t} + b_j$ 求 top-k 专家，但是权重值为未修正的 $s_{i,t}$\n",
    "\n",
    "\\begin{align}\n",
    "    g^{\\prime}_{i,t} &= \\begin{cases} \n",
    "    s_{i,t}, & s_{i,t} + b_i \\in \\operatorname{Topk} (\\{ s_{j, t} + b_j | 1 \\leq j \\leq N_r \\}, K_{r}), \\\\\n",
    "    0, & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5b9c8ce2-a7b6-4e50-828e-802ac2df3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1617,  0.1617],\n",
      "        [-0.0920,  0.0920],\n",
      "        [-0.1056,  0.1056],\n",
      "        [ 0.0563, -0.0563],\n",
      "        [ 0.1383, -0.1383],\n",
      "        [ 0.0130, -0.0130],\n",
      "        [-0.0878,  0.0878],\n",
      "        [-0.1019,  0.1019],\n",
      "        [ 0.0464, -0.0464],\n",
      "        [ 0.0575, -0.0575],\n",
      "        [ 0.0681, -0.0681],\n",
      "        [ 0.0763, -0.0763],\n",
      "        [-0.1013,  0.1013],\n",
      "        [-0.1906,  0.1906],\n",
      "        [ 0.0014, -0.0014],\n",
      "        [ 0.0119, -0.0119]], grad_fn=<SubBackward0>)\n",
      "torch.Size([2, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# fix gate 处理\n",
    "\n",
    "# bias = model.bias\n",
    "bias = torch.randn(model.expert_nums)\n",
    "\n",
    "x_view = x.reshape(bs*seq_len, dim)\n",
    "\n",
    "s = F.sigmoid(model.w_gate(x_view)) # sigmoid 代替 softmax\n",
    "sb = s + bias[None, :] # bias 是可学习参数\n",
    "\n",
    "# weight, idx = torch.topk(s, k = model.k, dim = -1) \n",
    "sb_weight, idx = torch.topk(sb, k = model.k, dim = -1) \n",
    "\n",
    "weight = torch.zeros(bs*seq_len, model.k)\n",
    "for i in range(bs*seq_len):\n",
    "    weight[i, :] = s[i, idx[i, :]]\n",
    "\n",
    "weight_norm = weight / (weight.sum(dim=-1, keepdim= True) + 1e-20) \n",
    "sb_weight_norm = sb_weight / (sb_weight.sum(dim=-1, keepdim= True) + 1e-20) # ignore\n",
    "\n",
    "print(sb_weight_norm - weight_norm)\n",
    "\n",
    "weight_norm = weight_norm.reshape(bs, seq_len, model.k)\n",
    "print(weight_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b3737-9459-4f61-8f4b-c958544cb6aa",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "1. DeepSeekV3 MoE 由路由专家、共享专家、负载均衡偏置参数、序列级负载均衡组成\n",
    "2. 序列级负载均衡目的是 prefill 各序列均衡，当 bsz = 1 时，其均衡效用显著\n",
    "\n",
    "讨论：\n",
    "\n",
    "1. batch、seq、layers 之间的整体 load-balance 计算形式\n",
    "2. sigmoid 代替 softmax 原因\n",
    "3. 讨论 sMoE、SwithTransformer、V3-MoE之间的 稀疏归一化 差异\n",
    "4. 讨论 Mixtral 8x7B load balance 是 MoE 内计算， 还是基于所有 MoE 层专家进行计算？ 后者目的是什么？\n",
    "5. V3 稀疏 MoE 专家数量为 256 个，如果进行分布式专家并行部署，是否一定需要 256 个GPU以上？\n",
    "6. 讨论 shared expert 与 router expert 二者集成学习有什么差异\n",
    "7. 从 Mixtral 8x7B, specilist 类问题是否会集中分配给特定专家吗？如果不会是什么原因造成的？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6e3704-b9f0-44a4-9225-086bd22bd481",
   "metadata": {},
   "source": [
    "# DeepSeek-V3 \n",
    "\n",
    "V3 各组件：\n",
    "\n",
    "1. `lecture/lc5_deepseek_v3/DeepSeek-MoE.ipynb`\n",
    "2. `lecture/lc5_deepseek_v3/Load_Balance.ipynb`\n",
    "3. `lecture/lc5_deepseek_v3/Multi_Latent_Attention.ipynb`\n",
    "4. `lecture/lc5_deepseek_v3/Multi_Token_Prediction.ipynb`\n",
    "5. `lecture/lc5_deepseek_v3/YaRN.ipynb`\n",
    "\n",
    "前置理解 Notebook：\n",
    "\n",
    "1. `lecture/lc5_deepseek_v3/Mixture-of-Experts.ipynb`\n",
    "2. `lecture/lc5_deepseek_v3/top-k_backward.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc4f58d-92b5-43cb-948d-5251375dcb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114664db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f92e91-f1f0-4034-9410-d22878caeac8",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43e06a2-a43c-4d46-8c02-1a2eaadef0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeekV3Config(learning_rate=0.001, vocab_size=200, dim=512, n_heads=8, head_dim=64, num_layers=12, pad_token_id=0, attention_bias=False, expert_nums=20, shared_expert_nums=4, top_k=4, dc_kv=32, dc_q=32, position_encoding_base=10000.0, base_scale=10.0, yarn_alpha=1, yarn_beta=32, max_len=512, num_mtp=5)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DeepSeekV3Config:\n",
    "    learning_rate: float = 0.001\n",
    "    vocab_size: int = 200\n",
    "    dim: int = 512\n",
    "    n_heads: int = 8\n",
    "    head_dim: int = dim//n_heads\n",
    "    num_layers: int = 12\n",
    "    pad_token_id: int = 0\n",
    "    attention_bias: bool = False # without bias\n",
    "\n",
    "    # MoE\n",
    "    expert_nums: int=20\n",
    "    shared_expert_nums: int=4\n",
    "    top_k: int = 4\n",
    "\n",
    "    # MLA\n",
    "    dc_kv: int = 32\n",
    "    dc_q: int = 32\n",
    "\n",
    "    # YaRN\n",
    "    position_encoding_base: float = 10000.0\n",
    "    base_scale: float = 10.0\n",
    "    yarn_alpha: int = 1\n",
    "    yarn_beta: int = 32\n",
    "    max_len: int = 512\n",
    "\n",
    "    # MTP\n",
    "    num_mtp: int = 5\n",
    "\n",
    "    # loss\n",
    "\n",
    "config = DeepSeekV3Config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26d861-3b63-4a04-a033-4149ae38164f",
   "metadata": {},
   "source": [
    "## RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036e0c74-ccc3-4374-a01d-c3079474077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, epsilon = 0.0001):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.dim = dim \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(self.dim))\n",
    "                                        \n",
    "    def forward(self, x):\n",
    "        RMS = torch.mean(x ** 2.0 , dim = -1, keepdim = True) \n",
    "        x_hat = x / torch.sqrt( RMS + self.epsilon )\n",
    "        x_out = x_hat * self.gamma \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471446d6-8e1a-4533-bad4-a6a01ea24b37",
   "metadata": {},
   "source": [
    "## YaRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b025327-4e78-4103-b6b7-30fe16d586f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0.6607044696629862\n",
      "YaRN Re-Scale: 1.2302585092994045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 16, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    _, _, seq_len, _ = x.size()\n",
    "    cos = cos[:seq_len,:]\n",
    "    sin = sin[:seq_len,:]\n",
    "\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    x1, x2 = torch.chunk(x, 2, dim=-1)\n",
    "    o1 = x1 * cos - x2 * sin\n",
    "    o2 = x2 * cos + x1 * sin\n",
    "    return torch.cat((o1, o2), dim=-1)\n",
    "    \n",
    "class YaRN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = config.head_dim\n",
    "        self.base = config.position_encoding_base\n",
    "        self.initial_context_length = config.max_len\n",
    "        self.scaling_factor = config.base_scale\n",
    "        self.ntk_alpha = config.yarn_alpha\n",
    "        self.ntk_beta = config.yarn_beta\n",
    "\n",
    "        cos, sin = self._compute_cos_sin(self.initial_context_length)\n",
    "        self.register_buffer('cos', cos)\n",
    "        self.register_buffer('sin', sin)\n",
    "\n",
    "    def _compute_concentration_and_inv_freq(self) -> torch.Tensor:\n",
    "        \"\"\"See YaRN paper: https://arxiv.org/abs/2309.00071\"\"\"\n",
    "        freq = self.base ** (\n",
    "            torch.arange(0, self.head_dim, 2, dtype=torch.float)\n",
    "            / self.head_dim\n",
    "        )        \n",
    "        # self.scaling_factor = max( 1, self.cur_context_length  / self.initial_context_length)\n",
    "        if self.scaling_factor > 1.0:\n",
    "            concentration = (\n",
    "                0.1 * math.log(self.scaling_factor) + 1.0\n",
    "            )  \n",
    "            print('t:',  1/concentration**2)\n",
    "\n",
    "            d_half = self.head_dim / 2\n",
    "            low = (\n",
    "                d_half\n",
    "                * math.log(self.initial_context_length / (self.ntk_beta * 2 * math.pi))\n",
    "                / math.log(self.base)\n",
    "            )\n",
    "            high = (\n",
    "                d_half\n",
    "                * math.log(self.initial_context_length / (self.ntk_alpha * 2 * math.pi))\n",
    "                / math.log(self.base)\n",
    "            )\n",
    "            assert 0 < low < high < d_half - 1\n",
    "\n",
    "            interpolation = 1.0 / (self.scaling_factor * freq)\n",
    "            extrapolation = 1.0 / freq\n",
    "\n",
    "            ramp = (\n",
    "                torch.arange(d_half, dtype=torch.float32, ) - low\n",
    "            ) / (high - low)\n",
    "            mask = 1 - ramp.clamp(0, 1)\n",
    "            inv_freq = interpolation * (1 - mask) + extrapolation * mask\n",
    "        else:\n",
    "            concentration = 1.0\n",
    "            inv_freq = 1.0 / freq\n",
    "\n",
    "        return concentration, inv_freq\n",
    "\n",
    "    def _compute_cos_sin(self, num_tokens: int):\n",
    "        concentration, inv_freq = self._compute_concentration_and_inv_freq()\n",
    "        t = torch.arange(num_tokens, dtype=torch.float32)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        cos = freqs.cos() * concentration\n",
    "        sin = freqs.sin() * concentration\n",
    "        return cos, sin\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        query = _apply_rotary_emb(query, self.cos, self.sin)\n",
    "        key = _apply_rotary_emb(key, self.cos, self.sin)\n",
    "        return query, key\n",
    "\n",
    "bsz = 2\n",
    "seq_len = 16\n",
    "\n",
    "Q = torch.randn(bsz, config.n_heads, seq_len, config.head_dim)\n",
    "K = torch.randn(bsz, config.n_heads, seq_len, config.head_dim)\n",
    "\n",
    "yarn = YaRN(config)\n",
    "q_rope, k_rope = yarn.forward(Q, K)\n",
    "_apply_rotary_emb(Q, yarn.cos, yarn.sin).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec89ee-c5aa-4a2d-9b08-4c7565da3024",
   "metadata": {},
   "source": [
    "## MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a2536d5-53fc-423c-b179-05bc5b0d763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLA(\n",
      "  (wq_down): Linear(in_features=512, out_features=32, bias=False)\n",
      "  (wq_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "  (wkv_down): Linear(in_features=512, out_features=32, bias=False)\n",
      "  (wk_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "  (wv_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "  (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (wq_up_rope): Linear(in_features=32, out_features=512, bias=False)\n",
      "  (wk_head_rope): Linear(in_features=512, out_features=64, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.head_dim = config.head_dim\n",
    "        self.dc_kv = config.dc_kv\n",
    "        self.dc_q = config.dc_q\n",
    "\n",
    "        # Q\n",
    "        self.wq_down = nn.Linear(self.dim, self.dc_q, bias=False,)\n",
    "        self.wq_up = nn.Linear(self.dc_q, self.dim , bias=False,)\n",
    "\n",
    "        # 单个 C 映射到 K、V\n",
    "        self.wkv_down = nn.Linear(self.dim, self.dc_kv, bias=False,)\n",
    "        self.wk_up = nn.Linear(self.dc_kv, self.dim, bias=False,)\n",
    "        self.wv_up = nn.Linear(self.dc_kv, self.dim, bias=False,)\n",
    "        \n",
    "        self.wo = nn.Linear(self.dim, self.dim, bias=False,)\n",
    "\n",
    "        # RoPE Weight\n",
    "        # K 每头一样， Q每头不一样\n",
    "        self.wq_up_rope = nn.Linear(self.dc_q, self.dim, bias=False,)\n",
    "        self.wk_head_rope = nn.Linear(self.dim, self.head_dim , bias=False,)\n",
    "        \n",
    "    def forward(self, X, sin, cos, mask = None):\n",
    "        bsz, seq_len, _ = X.shape\n",
    "        C_Q = self.wq_down(X)\n",
    "        Q = self.wq_up(C_Q)\n",
    "        C_KV = self.wkv_down(X)\n",
    "        K = self.wk_up(C_KV)\n",
    "        V = self.wv_up(C_KV)\n",
    "\n",
    "        \n",
    "        R_Q = self.wq_up_rope(C_Q) #多头\n",
    "        R_K = self.wk_head_rope(X) #单头\n",
    "        \n",
    "        R_Q = R_Q.view(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        R_K = R_K.unsqueeze(dim = 1).repeat( repeats = [1, self.n_heads, 1, 1])\n",
    "        RoPE_Q_head = _apply_rotary_emb(R_Q, sin, cos)\n",
    "        RoPE_K_head = _apply_rotary_emb(R_K, sin, cos)\n",
    "        \n",
    "        # 与传统多头注意力无差别\n",
    "        Q = Q.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        K = K.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        V = V.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        Q, K, V = Q.transpose(1,2), K.transpose(1,2), V.transpose(1,2)\n",
    "        \n",
    "\n",
    "        Q = torch.cat((Q, RoPE_Q_head), dim = -1)\n",
    "        K = torch.cat((K, RoPE_K_head), dim = -1)\n",
    "\n",
    "        S = Q @ K.transpose(2, 3) / math.sqrt(2 * self.head_dim) # cat后dim维度变化，底数也有变化\n",
    "        P = F.softmax(S.float(), dim=-1)\n",
    "        Z = P @ V \n",
    "        Z = Z.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
    "        O = self.wo(Z)\n",
    "        return O\n",
    "\n",
    "mla = MLA(config)\n",
    "print(mla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18f2c285-52bc-45ee-8fe1-1980317576b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(bsz, seq_len, config.dim)\n",
    "X_ = mla(X, mask = None, sin = yarn.sin, cos = yarn.cos)\n",
    "print(X_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222183a5-f18c-4996-b0d1-edc873626b9d",
   "metadata": {},
   "source": [
    "## MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cab1df6a-ef65-4715-b66a-466a9265fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module): # expert\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim\n",
    "        self.dim_out = self.dim_in * 8 // 3\n",
    "        self.w1 = nn.Linear(self.dim_in, self.dim_out , bias = False)\n",
    "        self.w_act = nn.Linear(self.dim_in, self.dim_out, bias = False) \n",
    "        self.w2 = nn.Linear(self.dim_out, self.dim_in, bias = False)  \n",
    "        self.SiLU = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.w1(x)\n",
    "        h_act = self.w_act(x)\n",
    "        h_act_up = self.SiLU(h_act) * h\n",
    "        output = self.w2(h_act_up)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57bc0ea1-e6c8-4d5b-8fb3-8a62910cfd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeekV3MoE(\n",
      "  (experts): ModuleList(\n",
      "    (0-19): 20 x Expert(\n",
      "      (w1): Linear(in_features=512, out_features=1365, bias=False)\n",
      "      (w_act): Linear(in_features=512, out_features=1365, bias=False)\n",
      "      (w2): Linear(in_features=1365, out_features=512, bias=False)\n",
      "      (SiLU): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (w_gate): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (shared_experts): ModuleList(\n",
      "    (0-3): 4 x Expert(\n",
      "      (w1): Linear(in_features=512, out_features=1365, bias=False)\n",
      "      (w_act): Linear(in_features=512, out_features=1365, bias=False)\n",
      "      (w2): Linear(in_features=1365, out_features=512, bias=False)\n",
      "      (SiLU): SiLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeepSeekV3MoE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Route Experts: \n",
    "        self.expert_nums = config.expert_nums\n",
    "        self.k = config.top_k\n",
    "        self.dim = config.dim\n",
    "        self.experts = nn.ModuleList([ Expert(self.dim) for _ in range(self.expert_nums)])\n",
    "        self.w_gate = nn.Linear(self.dim, self.expert_nums)\n",
    "\n",
    "        # Auxiliary-Loss-Free Load Balancing\n",
    "        self.bias = torch.nn.Parameter(torch.zeros( self.expert_nums )) \n",
    "        self.alpha = 0.001\n",
    "        \n",
    "        # Shared Experts: \n",
    "        self.shared_expert_nums = config.shared_expert_nums\n",
    "        self.shared_experts = nn.ModuleList([ Expert(self.dim) for _ in range(self.shared_expert_nums)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_route, weight, idx = self.forward_route(x) \n",
    "        y_shared = self.forward_shared(x) \n",
    "        y = x + y_route + y_shared\n",
    "        # load_loss = self.load_balance_sequence_wise(weight, idx)\n",
    "        # load_loss = 1.0\n",
    "        return y, weight, idx\n",
    "    \n",
    "    def forward_route(self, x):\n",
    "        # gate 处理\n",
    "        g = self.w_gate(x)\n",
    "        g = F.sigmoid(g) # sigmoid 代替 softmax\n",
    "        weight, idx = torch.topk(g, k = self.k, dim = -1) \n",
    "        weight_norm = weight / (weight.sum(dim=-1, keepdim= True) + 1e-20)\n",
    "\n",
    "        # dispatch\n",
    "        expert_results = [None] * self.expert_nums\n",
    "        for i in range(self.expert_nums):\n",
    "            cur_pos = torch.where(idx == i) \n",
    "            x_select = x[cur_pos[0], cur_pos[1], :] \n",
    "            if x_select.shape[0] > 0: \n",
    "                expert_results[i] = self.experts[i](x_select)\n",
    "        \n",
    "        # combine\n",
    "        y_result = torch.zeros_like(x) \n",
    "        for i in range(self.expert_nums):\n",
    "            cur_pos = torch.where(idx == i) \n",
    "            if expert_results[i] != None:\n",
    "                y_result[cur_pos[0], cur_pos[1], :] += expert_results[i] * weight_norm[cur_pos[0], cur_pos[1], cur_pos[2]].unsqueeze(-1)\n",
    "        \n",
    "        return y_result, g, idx \n",
    "\n",
    "    def forward_shared(self, x):\n",
    "        y = torch.zeros_like(x)\n",
    "        for i in range(self.shared_expert_nums):\n",
    "            y += self.shared_experts[i](x) # not gate\n",
    "        return y\n",
    "\n",
    "# model\n",
    "model = DeepSeekV3MoE(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc8269a6-f98a-456f-891a-f0fd6a4c2325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "Y, _, _ = model(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847d231-acf7-49c6-965c-536a14bb9849",
   "metadata": {},
   "source": [
    "## DeepSeek-V3 Decode Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5e82357-a2d3-4000-9721-164b3807b783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeekV3Block(\n",
      "  (Norm1): RMSNorm()\n",
      "  (Norm2): RMSNorm()\n",
      "  (MLA): MLA(\n",
      "    (wq_down): Linear(in_features=512, out_features=32, bias=False)\n",
      "    (wq_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "    (wkv_down): Linear(in_features=512, out_features=32, bias=False)\n",
      "    (wk_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "    (wv_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "    (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (wq_up_rope): Linear(in_features=32, out_features=512, bias=False)\n",
      "    (wk_head_rope): Linear(in_features=512, out_features=64, bias=False)\n",
      "  )\n",
      "  (MoE): DeepSeekV3MoE(\n",
      "    (experts): ModuleList(\n",
      "      (0-19): 20 x Expert(\n",
      "        (w1): Linear(in_features=512, out_features=1365, bias=False)\n",
      "        (w_act): Linear(in_features=512, out_features=1365, bias=False)\n",
      "        (w2): Linear(in_features=1365, out_features=512, bias=False)\n",
      "        (SiLU): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (w_gate): Linear(in_features=512, out_features=20, bias=True)\n",
      "    (shared_experts): ModuleList(\n",
      "      (0-3): 4 x Expert(\n",
      "        (w1): Linear(in_features=512, out_features=1365, bias=False)\n",
      "        (w_act): Linear(in_features=512, out_features=1365, bias=False)\n",
      "        (w2): Linear(in_features=1365, out_features=512, bias=False)\n",
      "        (SiLU): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeepSeekV3Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Norm1 = RMSNorm(config.dim)\n",
    "        self.Norm2 = RMSNorm(config.dim)\n",
    "        self.MLA = MLA(config)\n",
    "        self.MoE = DeepSeekV3MoE(config)\n",
    "\n",
    "    def forward(self, X, mask=None, sin=None, cos=None):\n",
    "        X = X + self.MLA(self.Norm1(X), sin, cos)\n",
    "        X_moe, weight, idx = self.MoE(self.Norm2(X))\n",
    "        X = X + X_moe\n",
    "        return X, weight, idx\n",
    "\n",
    "block = DeepSeekV3Block(config)\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c2fa929-22ea-4eb2-b942-91d63d2365dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 512])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y,_,_ = block(X, sin=yarn.sin, cos=yarn.cos)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2726db7-04c2-406b-9835-caec23bdacca",
   "metadata": {},
   "source": [
    "## DeepSeek-V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9083cb29-a656-4512-ad4c-4b7a1df8ac04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0.6607044696629862\n",
      "YaRN Re-Scale: 1.2302585092994045\n",
      "DeepSeekV3Model(\n",
      "  (embd): Embedding(200, 512)\n",
      "  (yarn): YaRN()\n",
      "  (decoder): ModuleList(\n",
      "    (0-11): 12 x DeepSeekV3Block(\n",
      "      (Norm1): RMSNorm()\n",
      "      (Norm2): RMSNorm()\n",
      "      (MLA): MLA(\n",
      "        (wq_down): Linear(in_features=512, out_features=32, bias=False)\n",
      "        (wq_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "        (wkv_down): Linear(in_features=512, out_features=32, bias=False)\n",
      "        (wk_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "        (wv_up): Linear(in_features=32, out_features=512, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wq_up_rope): Linear(in_features=32, out_features=512, bias=False)\n",
      "        (wk_head_rope): Linear(in_features=512, out_features=64, bias=False)\n",
      "      )\n",
      "      (MoE): DeepSeekV3MoE(\n",
      "        (experts): ModuleList(\n",
      "          (0-19): 20 x Expert(\n",
      "            (w1): Linear(in_features=512, out_features=1365, bias=False)\n",
      "            (w_act): Linear(in_features=512, out_features=1365, bias=False)\n",
      "            (w2): Linear(in_features=1365, out_features=512, bias=False)\n",
      "            (SiLU): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (w_gate): Linear(in_features=512, out_features=20, bias=True)\n",
      "        (shared_experts): ModuleList(\n",
      "          (0-3): 4 x Expert(\n",
      "            (w1): Linear(in_features=512, out_features=1365, bias=False)\n",
      "            (w_act): Linear(in_features=512, out_features=1365, bias=False)\n",
      "            (w2): Linear(in_features=1365, out_features=512, bias=False)\n",
      "            (SiLU): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeepSeekV3Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        lm_head 为主体输出头\n",
    "        num_mtp 额外增加 MTP 输出头模块数量\n",
    "        总输出头模块：1+num_mtp\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        \n",
    "        self.embd = nn.Embedding(self.vocab_size, self.dim)\n",
    "\n",
    "        self.yarn = YaRN(config)\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [ DeepSeekV3Block(config) for _ in range(config.num_layers) ]\n",
    "        )\n",
    "        # self.mtp_body =  MTPModule(config)\n",
    "        \n",
    "        self.lm_head = nn.Linear(self.dim, self.vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        X = self.embd(x)  \n",
    "        moe_weight_list = []\n",
    "        moe_idx_list = []\n",
    "        \n",
    "        for decoder_block in self.decoder:\n",
    "            X, weight, idx = decoder_block(X, mask, yarn.sin, yarn.cos)\n",
    "            moe_weight_list.append(weight)\n",
    "            moe_idx_list.append(idx)\n",
    "            \n",
    "        logits = self.lm_head(X)\n",
    "        return logits, moe_weight_list, moe_idx_list\n",
    "        \n",
    "model = DeepSeekV3Model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9701750b-0c26-4787-a8b1-9ad85573bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16, 200])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.randint(config.vocab_size, (bsz, seq_len))\n",
    "logits, _, _ = model(input_ids, mask = None)\n",
    "print(input_ids.shape)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17634a0-86c5-4747-99ad-e939b89e8074",
   "metadata": {},
   "source": [
    "## MTP\n",
    "\n",
    "1. 如果考虑 位置编码、mask 的话， MTP 实现稍微麻烦些。\n",
    "2. 当前以一个 Linear 来替换，便于实现。后续优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2404920d-fd63-4870-85ee-da14e453805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0.6607044696629862\n",
      "YaRN Re-Scale: 1.2302585092994045\n"
     ]
    }
   ],
   "source": [
    "class MTPModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.RMSNorm_pre = RMSNorm(self.dim)\n",
    "        self.RMSNorm_cur = RMSNorm(self.dim)\n",
    "        self.Proj = nn.Linear(self.dim*2, self.dim)\n",
    "        \n",
    "        # TODO: use decode block\n",
    "        # self.Transformer_block = DeepSeekV3Block(config)\n",
    "        self.Transformer_block = nn.Linear(self.dim, self.dim)\n",
    "        \n",
    "    def forward(self, X_embd, H_pre,):\n",
    "        X_embd, H_pre = self.RMSNorm_cur(X_embd), self.RMSNorm_pre(H_pre)\n",
    "        X = torch.cat((X_embd, H_pre), dim = -1)\n",
    "        X = self.Proj(X)\n",
    "        X = self.Transformer_block(X) # 需要传参 mask, rope\n",
    "        return X\n",
    "\n",
    "class DeepSeekV3ModelMTP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        lm_head 为主体输出头\n",
    "        num_mtp 额外增加 MTP 输出头模块数量\n",
    "        总输出头模块：1+num_mtp\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.num_mtp = config.num_mtp\n",
    "        self.embd = nn.Embedding(self.vocab_size, self.dim)\n",
    "        self.yarn = YaRN(config)\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [ DeepSeekV3Block(config) for _ in range(config.num_layers) ]\n",
    "        )\n",
    "        self.lm_head = nn.Linear(self.dim, self.vocab_size)\n",
    "\n",
    "        # 新增 MTP 头 \n",
    "        self.mtp_heads = nn.ModuleList(\n",
    "            [ MTPModule(config) for _ in range(self.num_mtp) ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # main: input\n",
    "        bsz, seq_len = x.shape\n",
    "        X = self.embd(x)\n",
    "        X = X[:, :-self.num_mtp, :]\n",
    "\n",
    "        # main: decoder\n",
    "        moe_weight_list = []\n",
    "        moe_idx_list = []\n",
    "        for decoder_block in self.decoder:\n",
    "            X, weight, idx = decoder_block(X,\n",
    "                                           mask, \n",
    "                                           yarn.sin, \n",
    "                                           yarn.cos)\n",
    "            moe_weight_list.append(weight)\n",
    "            moe_idx_list.append(idx)\n",
    "        # main: output\n",
    "        lm_logits = self.lm_head(X) # bsz, len, vocab_size\n",
    "        hidden_states = X.clone()\n",
    "\n",
    "        # MTP: \n",
    "        X = self.embd(x)\n",
    "        mtp_logits = torch.zeros(self.num_mtp, \n",
    "                                 bsz, \n",
    "                                 seq_len - self.num_mtp, \n",
    "                                 self.vocab_size)\n",
    "        # MTP Recurrent\n",
    "        for i in range(self.num_mtp):\n",
    "            # MTP: input\n",
    "            X_cur = X[:, i+1: i+1+seq_len-self.num_mtp, :]\n",
    "\n",
    "            # MTP: body\n",
    "            hidden_states_i = self.mtp_heads[i](X_cur, hidden_states.detach())\n",
    "\n",
    "            # MTP: head\n",
    "            mtp_logits_i = self.lm_head(hidden_states_i)\n",
    "\n",
    "            # MTP: update\n",
    "            mtp_logits[i] = mtp_logits_i\n",
    "            hidden_states = hidden_states_i\n",
    "\n",
    "        return lm_logits, moe_weight_list, moe_idx_list, mtp_logits\n",
    "        \n",
    "model = DeepSeekV3ModelMTP(config)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0557e2bd-f9b0-46ea-b090-e1e4ee2d0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 11, 200])\n",
      "torch.Size([5, 2, 11, 200])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "lm_logits, moe_weight_list, moe_idx_list, mtp_logits = model(input_ids)\n",
    "print(input_ids.shape)\n",
    "print(lm_logits.shape)\n",
    "print(mtp_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11d7f3-899a-4521-9082-33f51f66d6ca",
   "metadata": {},
   "source": [
    "## Load Balance loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66ae15b8-aa69-46a7-852f-58f82374bf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0004], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def load_balance_sequence_wise(config, s, idx):\n",
    "    Nr = config.expert_nums # n routes expert\n",
    "    bs, seq_len, dim = s.shape # seq : pad token 要去除，或者增加 mask, 避免计入 loss 里\n",
    "    l_lab = torch.zeros(1)\n",
    "    for k in range(bs):\n",
    "\n",
    "        # Compute fi\n",
    "        fi = torch.zeros(Nr)\n",
    "        seq_expert_count = torch.zeros(Nr)\n",
    "        idx_seq = idx[k,:,:]\n",
    "        for i in range(config.expert_nums):\n",
    "            seq_expert_count[i] = torch.where(idx_seq == i)[1].numel()\n",
    "        fi = Nr / (config.top_k * seq_len) * seq_expert_count\n",
    "\n",
    "        # Compute pi\n",
    "        s_seq = s[k, :, :]\n",
    "        si_ = s / s.sum(dim = -1, keepdim = True)\n",
    "        pi = si_.sum(dim = 0) / seq_len\n",
    "        l_bal_seq = (fi * pi).sum() / seq_len # seq_len_no_pad or use mask\n",
    "        l_lab += l_bal_seq\n",
    "        \n",
    "    l_lab = 0.001 * l_lab\n",
    "    return l_lab\n",
    "\n",
    "load_balance_loss = load_balance_sequence_wise(config, moe_weight_list[0], moe_idx_list[0])\n",
    "print(load_balance_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "704155cc-6b94-4147-9a33-c238efc95c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.5676, grad_fn=<AddBackward0>)\n",
      "tensor(23.0371, grad_fn=<NllLossBackward0>)\n",
      "tensor([5.3156, 5.3155, 5.2962, 5.3688, 5.2321], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "def DeepSeekV3LMLoss(lm_logits,\n",
    "                     mtp_logits, \n",
    "                     y,\n",
    "                     lam=0.1):\n",
    "\n",
    "    N, bsz, seq_len, vocab_size = mtp_logits.shape\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "    loss_lm_head = loss_fn(lm_logits.view(bsz*seq_len,vocab_size), \n",
    "                           y[:,:-N].reshape(bsz*seq_len))\n",
    "\n",
    "    if mtp_logits is None:\n",
    "        loss_mtp_head = None\n",
    "    else:\n",
    "        loss_mtp_head = torch.zeros(N)\n",
    "        for i in range(N):\n",
    "            loss_mtp_head[i] = loss_fn(mtp_logits[i,:,:,:].view(bsz*seq_len,vocab_size),\n",
    "                                       y[:,i:-N+i].reshape(bsz*seq_len))\n",
    "                                \n",
    "    loss = loss_lm_head + lam*loss_mtp_head.mean()\n",
    "    return loss, loss_lm_head, loss_mtp_head\n",
    "\n",
    "labels = torch.zeros_like(input_ids)\n",
    "labels[:, :-1] = input_ids[:, 1:]\n",
    "labels[:, -1] = IGNORE_INDEX\n",
    "\n",
    "loss, loss_lm, loss_mtp = DeepSeekV3LMLoss(lm_logits, \n",
    "                     mtp_logits, \n",
    "                     y=labels,\n",
    "                     lam = 0.1)\n",
    "print(loss)\n",
    "print(loss_lm)\n",
    "print(loss_mtp)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb02eb-f52d-46e4-9862-0baf570ed37f",
   "metadata": {},
   "source": [
    "## Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "110cdce6-f351-4d3f-939f-995e2427d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepSeekV3Loss(\n",
    "    config,\n",
    "    lm_logits,\n",
    "     mtp_logits, \n",
    "     y,\n",
    "     lam=0.1,\n",
    "    weight_list=None,\n",
    "    idx_list=None,\n",
    "    lam_len=0.1,\n",
    "):\n",
    "    loss, loss_lm, loss_mtp = DeepSeekV3LMLoss(lm_logits, mtp_logits, y=labels, lam = 0.1)\n",
    "    loss_load = torch.zeros(config.num_layers)\n",
    "    i = 0\n",
    "    for s, idx in zip(weight_list, idx_list):\n",
    "        loss_load[i] = load_balance_sequence_wise(config, s, idx)\n",
    "        i = i+1\n",
    "    loss_total = loss + lam_len * loss_load.mean()\n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9d3cf-9f33-4f7a-8064-297376ed1dd8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d8fe4-40f7-4dba-bbb7-bb8d7da9a939",
   "metadata": {},
   "source": [
    "## MLA Absorb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f5791-98a9-412a-9d7a-b2c605dbafdb",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

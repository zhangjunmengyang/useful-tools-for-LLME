{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7d1492-dc4e-49af-ab95-f02778b07c91",
   "metadata": {},
   "source": [
    "# GPT-2\n",
    "\n",
    "## GPT 系列模型介绍\n",
    "\n",
    "在 Transformer 模型中，使用了 Encoder-Decoder 架构来进行语言建模：本质上，是一种 有 Seq2Seq 有监督学习。 通过监督数据所训练的模型其难以在其他场景泛化。\n",
    "\n",
    "有监督学习的缺陷在于：数据成本昂贵，语言数据标签通常需要人工编写或标注。这是所有深度学习模型都会面临的问题。\n",
    "\n",
    "无监督学习正是从根本性上解决大规模模型对训练数据的需求。比如世界知识里，纯文本（如书本）是远多于（问答数据）的。从纯文本中进行预训练（Pre-trained），语言模型能学习到通用文本自身的表征（而不是从监督数据获取），语言模型所习得的文本处理能力能在**通用**下游任务泛化。\n",
    "\n",
    "- GPT-1 提出了 Decoder-only 的 “预训练” 学习目标。并在其他阶段使用有监督学习（supervised learning, SFT).\n",
    "- GPT-2 验证了通用文本预训练的模型，可以在不做SFT基础下，通过 Zero-Shot learning 解决 NLP 应用任务\n",
    "- GPT-3 训练 175B 预训练模型，提出 “上下文学习（In-Context Learning, ICL）” 范式，在多个任务上取得良好表现。 GPT-3 是预训练模型发展最关键的转折，也是打败 Encoder-only 和 Encoder-Decoder Transformer 架构的关键工作。\n",
    "\n",
    "GPT 系列模型的关键在于两种关键技术的配合：\n",
    "\n",
    "1. Decoder-Transformer： 其模型架构中的 self-masked attention 天然可以**并行**做“自回归学习”\n",
    "2. Casual Language Modeling： 因果语言建模是构造序列数据学习的方法，通过历史信息预测下一个时刻信息。\n",
    "  \n",
    "CLM 本质是 **构造一个简单的（无须人工label）通用的学习任务，将外在知识转化为内在参数**, 同时 Decoder-Transformer 参数能够 Scaling-up。需要注意到，多数模型都可以做 CLM 如：RNN、LSTM 和 Transformer。 特别是 Transformer 其本质是 **带了编码器表征** 的自回归模型。\n",
    "\n",
    "在学习 GPT 系列模型的重要意义在于\n",
    "\n",
    "1. GPT 验证了大规模预训练模型的可行性。（Ilya 尝试字符RNN预训练）\n",
    "2. GPT 具有极强的泛化性，是其能够扩展众多下游任务的一个关键特性。\n",
    "3. GPT 模型能够 Scaling-Up（参数规模提升，性能提升）\n",
    "\n",
    "\n",
    "本 Notebook 实现以下关键技术：\n",
    "\n",
    "1. Casual Language Modeling(CLM)\n",
    "3. GPT2-Config\n",
    "4. GPTModel\n",
    "5. Dummy Block Dataset\n",
    "6. Train\n",
    "7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8cabf47-d0d0-4ced-aa58-7eee622059b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10dc48db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd7f44-0d5f-4ada-a3eb-55ebc4fa7f60",
   "metadata": {},
   "source": [
    "## Casusal Language Modeling\n",
    "\n",
    "- GPT 与 Transformer 中去除 encode-decode 的 Decoder 结构相同。\n",
    "- Decoder 本身就是在做 因果语言建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512130a2-f56a-4af3-8cb7-753170cb1cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42, 67, 76, 14, 26, 35, 20, 24, 50, 13]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "batch_size = 1\n",
    "seq_len = 10 \n",
    "\n",
    "x = torch.randint(vocab_size, (batch_size, seq_len))\n",
    "print(x) # 给定序列数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136e48a2-7392-4a74-9c59-88fb809bf78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 100])\n",
      "torch.Size([1, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个参数模型学习：\n",
    "\n",
    "class CLMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(vocab_size, dim)\n",
    "        self.w = nn.Linear(dim, dim)\n",
    "        self.LM_head = nn.Linear(dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            - x dim[batch,seq]\n",
    "        '''\n",
    "        h = self.embd(x)\n",
    "        h = self.w(h)\n",
    "        logits = self.LM_head(h)\n",
    "        prob = self.softmax(logits)\n",
    "        return logits, prob\n",
    "\n",
    "model = CLMModel(vocab_size = vocab_size,\n",
    "         dim = 64)\n",
    "\n",
    "y_logits, y_prob = model(x)\n",
    "\n",
    "print(x.shape) # # batch_size, seq_len\n",
    "print(y_logits.shape) # batch_size, seq_len, vocab_size\n",
    "print(y_prob.shape) # batch_size, seq_len, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80135be-9260-4f53-8e27-23cc06aee628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42, 67, 76, 14, 26, 35, 20, 24, 50, 13]])\n",
      "[[67, 76, 14, 26, 35, 20, 24, 50, 13]] [42]\n",
      "tensor([[67, 76, 14, 26, 35, 20, 24, 50, 13, 42]])\n"
     ]
    }
   ],
   "source": [
    "# 基于 输入序列 创建 next-token 标签。\n",
    "\n",
    "print(x)\n",
    "print(x[:,1:].tolist(), x[:,0].tolist()) # 默认循环左移\n",
    "\n",
    "label = torch.zeros_like(x)\n",
    "label[:, :-1] = x[:, 1:]\n",
    "label[:, -1] = x[:, 0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbd6184-6111-4665-b29a-3ec6aee405f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input seq \t\t [42]\n",
      "next token label\t 67\n",
      "input seq \t\t [42, 67]\n",
      "next token label\t 76\n",
      "input seq \t\t [42, 67, 76]\n",
      "next token label\t 14\n",
      "input seq \t\t [42, 67, 76, 14]\n",
      "next token label\t 26\n",
      "input seq \t\t [42, 67, 76, 14, 26]\n",
      "next token label\t 35\n",
      "input seq \t\t [42, 67, 76, 14, 26, 35]\n",
      "next token label\t 20\n",
      "input seq \t\t [42, 67, 76, 14, 26, 35, 20]\n",
      "next token label\t 24\n",
      "input seq \t\t [42, 67, 76, 14, 26, 35, 20, 24]\n",
      "next token label\t 50\n",
      "input seq \t\t [42, 67, 76, 14, 26, 35, 20, 24, 50]\n",
      "next token label\t 13\n"
     ]
    }
   ],
   "source": [
    "# 因果建模\n",
    "for i in range(batch_size):\n",
    "    for j in range(seq_len - 1):\n",
    "        print('input seq \\t\\t', x[i, :j+1].tolist())\n",
    "        print('next token label\\t', x[0, j+1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db831af7-4976-4293-ab79-1b2e55585edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5059, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "\n",
    "y_logits, y_prob = model(x)\n",
    "b, s, v = y_logits.shape\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 有 bs * seq_len 个 next-token-prediction 预测任务\n",
    "loss = loss_fn(y_logits.reshape( b*s, v), \n",
    "               label.reshape(b*s)) \n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec6426-78d4-4aa5-aa0e-37eab29056b4",
   "metadata": {},
   "source": [
    "以上示例准确来说是：自回归下一个词元学习。那么这里明明有 label， 为什么仍然说 GPT-2 是无监督学习呢？\n",
    "\n",
    "- 无监督是不需要标签， 自监督是通过本身的数据即是输入，也是标签。那么 “自监督” 是更准确的，自监督是无监督的一种特例\n",
    "- 自回归是通过历史数据预测下一个时刻数据。自回归是自监督的一种具体学习模式。\n",
    "\n",
    "讨论：\n",
    "\n",
    "上一章节的机器翻译的Transformer 是否是无监督学习？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c22ec9-ad3a-4aad-98af-d54b98faaecb",
   "metadata": {},
   "source": [
    "## GPT2-Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51b9b92-9c53-4c1e-a322-986a5ae4cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    learning_rate: float = 0.001\n",
    "    # src_vocab_size: int = 100\n",
    "    # trg_vocab_size: int = 200\n",
    "    vocab_size: int = 64 # 各个语种都在一个词表里\n",
    "    max_len: int = 512\n",
    "    dim: int = 512\n",
    "    heads: int = 8\n",
    "    num_layers: int = 6\n",
    "    position_encoding_base: float = 10000.0\n",
    "    pad_token_id: int = 0\n",
    "    # src_pad_token_id: int = 0\n",
    "    # trg_pad_token_id: int = 0\n",
    "    attention_bias: bool = False\n",
    "    initializer_range: float = 0.02\n",
    "    embd_pdrop: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d880f66e-b000-4a99-bbf1-7260d0ac7958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config(learning_rate=0.001, vocab_size=100, max_len=512, dim=512, heads=8, num_layers=6, position_encoding_base=10000.0, pad_token_id=0, attention_bias=False, initializer_range=0.02)\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size = vocab_size\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8bc1bb-22c0-4163-8b47-48b58fa36374",
   "metadata": {},
   "source": [
    "## GPTModel\n",
    "\n",
    "1. input layer\n",
    "2. Attention\n",
    "3. decoder block\n",
    "4. output layer\n",
    "5. GPT-2 model\n",
    "\n",
    "GPT-2 去除 model 中 `nn.Linear` 层的 `bias` 项, 并将 LayerNorm 前置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1541fe-5cce-4d06-a610-6fe424de7476",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1cbaf07-7b4e-4150-b8c6-839ccc906a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2InputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    词向量 + 位置编码\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=100, dim=512, max_len=1024, base=10000.0, embd_pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        theta_ids = torch.arange(0, dim, 2)  # 0, 2, 4, ..., 512\n",
    "        theta = 1 / (base ** (theta_ids / dim))\n",
    "        pe = torch.zeros(dim)  # 512, sin( theta_0 ),cos( theta_0), ...\n",
    "        pe[theta_ids] = theta\n",
    "        pe[theta_ids+1] = theta\n",
    "\n",
    "        position_ids = torch.arange(0, max_len)  # 0, 1, 2, ..., 1024\n",
    "        self.PE = torch.outer(position_ids, pe)  # 1024 x 512\n",
    "\n",
    "        self.PE[:, theta_ids] = torch.sin(self.PE[:, theta_ids])\n",
    "        self.PE[:, theta_ids+1] = torch.sin(self.PE[:, theta_ids+1])\n",
    "\n",
    "        # self.embd_pdrop = embd_pdrop\n",
    "        # self.drop = nn.Dropout(embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        嵌入向量 + 绝对位置编码(标准实现)\n",
    "        \"\"\"\n",
    "        bs, seq_len = input_ids.shape\n",
    "        X = self.embedding(input_ids)\n",
    "        PE = self.PE[:seq_len, :]\n",
    "        X_ = X + PE\n",
    "        # X_ = self.drop(X_)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86a800-9198-4c97-bfeb-ce93e9489e76",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "```\n",
    "    |\n",
    "    |------->|\n",
    "layernorm_1  |\n",
    "    |        |\n",
    "attention    |\n",
    "    |<-------|\n",
    "    |------->|\n",
    "layernorm_2  |\n",
    "    |        |\n",
    "feedforward  |\n",
    "    |<-------|\n",
    "    |\n",
    "    V\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73441571-fdf9-47c9-a977-e436934bf032",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6c350-e5e5-4f31-9dd0-64a2b9e8ae39",
   "metadata": {},
   "source": [
    "### Decoder Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4270b9-d5b8-4f03-bb87-3ef3b85280d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## 设计 masked-self attention 的 mask\n",
    "data = [\n",
    "    [1, 2, 4, 5, ],\n",
    "    [5, 2, 1, 8, 7, 10, 32],\n",
    "]\n",
    "mask = torch.zeros(2, 7) \n",
    "for i in range(2):\n",
    "    mask[i, :len(data[i])] = 1\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d478e3-05e7-42b1-bbd3-a69561413ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tril_mask = torch.tril(torch.ones(7, 7)) \n",
    "print(tril_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b378d8-26f7-4eb1-9ad0-654651261bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[     -0., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0., -100000., -100000., -100000.],\n",
      "         [-100000., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [-100000., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [-100000., -100000., -100000., -100000., -100000., -100000., -100000.]],\n",
      "\n",
      "        [[     -0., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0.,      -0., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0.,      -0.,      -0., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0.,      -0.,      -0.,      -0.]]])\n"
     ]
    }
   ],
   "source": [
    "def get_add_mask(mask, tril_mask, neg_inf = -100000.0):\n",
    "    bs, seq_len = mask.shape\n",
    "    batch_mask = torch.zeros(bs, seq_len, seq_len)\n",
    "    for i in range(bs):\n",
    "        batch_mask[i, ] = torch.outer( mask[i,:], mask[i,:])\n",
    "        batch_mask[i, ] *= tril_mask\n",
    "    add_mask = (1 - batch_mask ) * neg_inf\n",
    "    return add_mask\n",
    "\n",
    "add_mask = get_add_mask(mask, tril_mask)\n",
    "print(add_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "477bc228-3bd8-4cac-a514-d0b5b9e009e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[     -0., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0., -100000., -100000., -100000.],\n",
      "         [-100000., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [-100000., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [-100000., -100000., -100000., -100000., -100000., -100000., -100000.]],\n",
      "\n",
      "        [[     -0., -100000., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0., -100000., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0., -100000., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0., -100000., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0.,      -0., -100000., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0.,      -0.,      -0., -100000.],\n",
      "         [     -0.,      -0.,      -0.,      -0.,      -0.,      -0.,      -0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 去除循环实现\n",
    "def get_add_mask(mask, tril_mask, neg_inf = -100000.0):\n",
    "    batch_mask = mask.unsqueeze(2) * mask.unsqueeze(1) * tril_mask\n",
    "    add_mask = (1 - batch_mask ) * neg_inf\n",
    "    return add_mask\n",
    "\n",
    "add_mask = get_add_mask(mask, tril_mask)\n",
    "print(add_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e1221b-32db-4afd-a5ee-7b59f37d8e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9884, 0.0116, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2721, 0.4983, 0.2296, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4810, 0.3868, 0.0911, 0.0411, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0611, 0.3105, 0.1586, 0.0232, 0.1378, 0.1346, 0.1742],\n",
      "        [0.0464, 0.4581, 0.1123, 0.0241, 0.0945, 0.0676, 0.1970],\n",
      "        [0.4624, 0.3804, 0.0484, 0.0103, 0.0126, 0.0307, 0.0552]])\n"
     ]
    }
   ],
   "source": [
    "# multi-head attention score\n",
    "S = torch.randn(2, 8, 7, 7) # head 8\n",
    "S += add_mask[:,None,:,:]\n",
    "P = F.softmax(S, dim = -1)\n",
    "print(P[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4a150-198e-410c-b9c5-a108ab0ea1c1",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b98991f5-8dcc-4d6e-85e5-f4b2075432ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, heads=8):\n",
    "        super().__init__()\n",
    "        # self.WQ = nn.Linear(dim_in, dim_out)\n",
    "        # self.WK = nn.Linear(dim_in, dim_out)\n",
    "        # self.WV = nn.Linear(dim_in, dim_out)\n",
    "        self.dim_out = dim_out\n",
    "        self.WQKV = nn.Linear(dim_in, dim_out*3) # 存在 bias, 与 wq/wk/wv 独立变换不等价\n",
    "        self.WO = nn.Linear(dim_in, dim_out)\n",
    "        self.heads = heads\n",
    "        self.head_dim = dim_out // self.heads\n",
    "\n",
    "    def forward(self, X_Q, X_K, X_V, mask=None):\n",
    "        bs, seq_len, dim = X_Q.shape\n",
    "        # Q = self.WQ(X_Q)\n",
    "        # K = self.WK(X_K)\n",
    "        # V = self.WV(X_V)\n",
    "        QKV = self.WQKV(X_Q)\n",
    "        Q, K, V = QKV.split(dim = 2, split_size = self.dim_out)\n",
    "\n",
    "        # 拆分维度\n",
    "        Q_h = Q.reshape(bs, seq_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "        K_h = K.reshape(bs, seq_len, self.heads, self.head_dim).transpose(1, 2) \n",
    "        V_h = V.reshape(bs, seq_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled-dot product attention\n",
    "        S = Q_h @ K_h.transpose(2, 3) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            S = S + mask[:, None, :, :]\n",
    "        P = torch.softmax(S, dim=-1)  # 行 softmax\n",
    "        Z = P @ V_h\n",
    "        Z = Z.transpose(1, 2).reshape(bs, seq_len, dim)\n",
    "        output = self.WO(Z)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80e67b-401f-44cf-93d1-b3be1d53fbda",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "912673e7-bdf6-4b14-9dc4-6401e382f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, ):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, X, ):\n",
    "        mu = X.mean(dim=-1, keepdim=True)\n",
    "        var = X.var(dim=-1, keepdim=True)\n",
    "        X_hat = (X - mu) / torch.sqrt(var + self.epsilon)\n",
    "        Y = X_hat * self.gamma + self.beta\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9741aab-0ef9-4c56-9694-a22938072b24",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9d4b6-064f-4f35-ad3d-63a9a9c12956",
   "metadata": {},
   "source": [
    "### GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1d5186a-1b38-44e2-91fc-0873a17c850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GELU(x):\n",
    "    # read lecture/lc3_gpt/GELU.ipynb \n",
    "    cdf = 0.5 * (1.0 + torch.tanh( math.sqrt(2.0 / torch.pi) \n",
    "                                  * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    return x * cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bcb3d42-c0ae-41aa-a404-b288e9d83ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, dim, ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.W_up = nn.Linear(self.dim, 4 * self.dim)\n",
    "        # self.ReLU = nn.ReLU()\n",
    "        self.W_down = nn.Linear(4 * self.dim, self.dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_ = GELU(self.W_up(X))\n",
    "        Y = self.W_down(X_)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99324b52-e508-42da-a4d9-a1e7d5bda4cd",
   "metadata": {},
   "source": [
    "## Decoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01278340-4316-4075-98d5-cf90354c3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim=512, heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadScaleDotProductAttention(dim, dim, heads)\n",
    "        self.ln1 = LayerNorm(dim)\n",
    "        self.ffn = FeedForwardNetwork(dim)\n",
    "        self.ln2 = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        '''\n",
    "        Pre-Normaliztion\n",
    "        '''\n",
    "        X_ln = self.ln1(X)\n",
    "        X_attn = self.attn(X_ln, X_ln, X_ln, mask = mask)\n",
    "        X = X + X_attn\n",
    "\n",
    "        X_ln = self.ln2(X)\n",
    "        X_ffn = self.ffn(X_ln)\n",
    "        X = X + X_ffn\n",
    "        return X\n",
    "        \n",
    "    def forward_postnorm(self, X, mask=None):\n",
    "        '''\n",
    "        Post-Normaliztion\n",
    "        '''\n",
    "        X_attn = self.attn(X, X, X, mask = mask)\n",
    "        X_ln = self.ln1(X_attn)\n",
    "        X = X + X_ln\n",
    "\n",
    "        X_ffn = self.ffn(X)\n",
    "        X_ln = self.ln2(X_ffn)\n",
    "        X = X + X_ln\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693dc309-7e07-4549-85f7-40eabed88b84",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d2c380-4ae5-4165-8833-1573fcb9f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, config:GPT2Config=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embd = GPT2InputLayer(vocab_size=self.config.vocab_size, \n",
    "                                   dim=self.config.dim, \n",
    "                                   max_len=self.config.max_len)\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [GPT2DecoderBlock(self.config.dim, \n",
    "                              self.config.heads) for _ in range(self.config.num_layers)]\n",
    "        )\n",
    "        self.ln = LayerNorm(self.config.dim)\n",
    "        self.lm_head = nn.Linear(self.config.dim, \n",
    "                                 self.config.vocab_size,\n",
    "                                 bias = False) # 不学习预训练数据分布偏置\n",
    "\n",
    "        self.cache_mask = torch.tril(torch.ones(self.config.max_len, \n",
    "                                                self.config.max_len)) \n",
    "\n",
    "        self._init_weights(self) # 增加初始化\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape\n",
    "        add_mask = get_add_mask(x, self.cache_mask[:seq_len, :seq_len])\n",
    "        \n",
    "        X = self.embd(x)\n",
    "        for block in self.decoder:\n",
    "            X = block(X, mask = add_mask)\n",
    "        X = self.ln(X)\n",
    "        logits = self.lm_head(X)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        ref: src/transformers/models/gpt2/modeling_gpt2.py\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "        for name, p in module.named_parameters():\n",
    "            if name == \"WO.weight\" :\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                p.data.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.num_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d4afcf1-1b2d-413a-a3d3-ad7dad88c011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2Model(config)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a1e1b-f2db-4132-a4aa-10f307b3e0ef",
   "metadata": {},
   "source": [
    "## Pretrained Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d2cb4d9-9296-47df-bc2f-f27cde4de6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randrange(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5fa735a-d623-44bb-9f07-5da0f106889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20\n",
      "tensor([[13, 65, 69, 42, 73, 68, 61, 31, 50, 61, 34,  5, 59, 60, 66, 41, 50, 99,\n",
      "         39, 85, 30,  4, 18, 36, 86, 67,  7, 56, 38,  5, 24, 60, 89, 75, 32, 41,\n",
      "         68, 10, 92, 60, 73, 38, 95, 72, 48,  2, 42, 32, 13, 65,  4, 77, 74, 27,\n",
      "         70, 43, 95, 62, 35,  1, 82, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 3, 10, 59, 36, 39,  6,  8, 32, 48, 47, 13, 66, 85, 25, 42, 21, 40, 60,\n",
      "         38, 34, 79,  1, 49, 95, 45, 58, 71, 16, 17, 31, 85, 33, 27, 85, 65, 16,\n",
      "         84,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]])\n",
      "tensor([[65, 69, 42, 73, 68, 61, 31, 50, 61, 34,  5, 59, 60, 66, 41, 50, 99, 39,\n",
      "         85, 30,  4, 18, 36, 86, 67,  7, 56, 38,  5, 24, 60, 89, 75, 32, 41, 68,\n",
      "         10, 92, 60, 73, 38, 95, 72, 48,  2, 42, 32, 13, 65,  4, 77, 74, 27, 70,\n",
      "         43, 95, 62, 35,  1, 82, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [10, 59, 36, 39,  6,  8, 32, 48, 47, 13, 66, 85, 25, 42, 21, 40, 60, 38,\n",
      "         34, 79,  1, 49, 95, 45, 58, 71, 16, 17, 31, 85, 33, 27, 85, 65, 16, 84,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]])\n"
     ]
    }
   ],
   "source": [
    "## raw data\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "vocab_size = config.vocab_size\n",
    "\n",
    "input_ids = torch.randint(low=1, high=config.vocab_size, size=(batch_size, seq_len))\n",
    "print(-random.randrange(seq_len))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    input_ids[i,-random.randrange(seq_len):] = config.pad_token_id\n",
    "print(input_ids)\n",
    "\n",
    "label = torch.ones_like(input_ids) * config.pad_token_id\n",
    "label[:,:seq_len-1] = input_ids[:,1:]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a98d66-1e17-4467-a6e8-008eb91bc55a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90fa2953-d763-4992-bcf3-e1b71fcea8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size = vocab_size\n",
    ")\n",
    "\n",
    "print(config.vocab_size)\n",
    "model.embd\n",
    "\n",
    "model = GPT2Model(config)\n",
    "optimizer = optim.Adam(model.parameters(), lr = config.learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = config.pad_token_id)\n",
    "\n",
    "logits = model.embd(input_ids)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "logits = model(input_ids)\n",
    "loss = loss_fn(logits.reshape(batch_size * seq_len, vocab_size), \n",
    "               label.reshape(batch_size * seq_len))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fe6d7-3a61-4067-b22e-ad24cd282811",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "1. 最基本的推理是“greedy search”， 那么 inference 的阶段，为什么要用“search”？\n",
    "2. LLM 的本质是 概率生成模型，哪里可以体现生成过程的随机性？随机性是否合理？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a621c6-3593-412a-8af6-97ce6b09fe99",
   "metadata": {},
   "source": [
    "## greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5899c374-b765-4d17-bafc-0404cf84775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13, 65, 69, 42, 73, 68, 61, 31, 50, 61, 34,  5, 59, 60, 66, 41, 50, 99,\n",
      "         39, 85, 30,  4, 18, 36, 86, 67,  7, 56, 38,  5, 24, 60, 89, 75, 32, 41,\n",
      "         68, 10, 92, 60, 73, 38, 95, 72, 48,  2, 42, 32, 13, 65,  4, 77, 74, 27,\n",
      "         70, 43, 95, 62, 35,  1, 82, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0, 85, 60, 38, 95, 60],\n",
      "        [ 3, 10, 59, 36, 39,  6,  8, 32, 48, 47, 13, 66, 85, 25, 42, 21, 40, 60,\n",
      "         38, 34, 79,  1, 49, 95, 45, 58, 71, 16, 17, 31, 85, 33, 27, 85, 65, 16,\n",
      "         84,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0, 85, 85, 85, 85, 85]])\n"
     ]
    }
   ],
   "source": [
    "def generation(\n",
    "    model = None,\n",
    "    input_ids: torch.tensor = None,\n",
    "    max_new_token: int = 100,\n",
    "):\n",
    "    for i in range(max_new_token):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "        logits = logits[:, -1, :] # zhe\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        next_token_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        input_ids = torch.cat( [input_ids, next_token_idx], dim = -1 )\n",
    "    return input_ids\n",
    "\n",
    "# logits = model(input_ids)\n",
    "result = generation(model, input_ids, max_new_token = 5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e52d22-f5f8-429e-b57f-2cb365a75d8f",
   "metadata": {},
   "source": [
    "## left padding\n",
    "\n",
    "批量数据推理，用 left padding， 其生成时， 位置序列是连续的，符合预训练的建模。\n",
    "\n",
    "批量数据：\n",
    "\n",
    "- 训练时：左右 padding 都可以\n",
    "- 推理时：左padding\n",
    "\n",
    "非批量数据，不用考虑padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c420cbe9-15d9-4199-aae6-9c31ef7f114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1]])\n"
     ]
    }
   ],
   "source": [
    "## raw data\n",
    "batch_size = 2\n",
    "seq_len = 24\n",
    "vocab_size = config.vocab_size\n",
    "input_ids = torch.randint(low=1, high=config.vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    input_ids[i,: seq_len - random.randrange(seq_len)] = config.pad_token_id\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d1cca7e-b147-4030-9275-db00c3facb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79, 60, 38, 95, 60, 38],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 49, 95, 45, 58, 71]])\n"
     ]
    }
   ],
   "source": [
    "result = generation(model, input_ids, max_new_token = 5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5947ee5-c0bb-4ba2-9ea4-77f0377c79f7",
   "metadata": {},
   "source": [
    "## temperature\n",
    "\n",
    "为什么要采样？\n",
    "\n",
    "1. 语言模型本身是概率模型， 在预测过程，实际上是：**预测下一个词元概率**， 我们只是采用不同的采样方法根据**概率**选择一个**合理**的离散词元\n",
    "2. 为什么要有温度，仅改变分布是否更加 smooth 或 sharpe\n",
    "3. 如何理解输出分布，当温度$t\\neq1$，输出分布发生了改变，只是改变的程度不同。\n",
    "\n",
    "举例：\n",
    "\n",
    "给定 10 个球，3个蓝球，2个黑球，5个红球。那么这三个球的概率为:\n",
    "\n",
    "1. 蓝:0.3\n",
    "2. 黑:0.2\n",
    "3. 红:0.5\n",
    "\n",
    "类比预测 next-token prediction，则从一个箱子取一个球，对应的颜色，即是 next-token。 那么词表大小可以类比球色。在实现上可以用：`torch.multinomial`\n",
    "\n",
    "采样生成有什么优势？ \n",
    "\n",
    "1. greedy search 为什么是 “search”， 可以认为模型能够输出任何文本（如无限猴子理论），search 只是一种规则\n",
    "2. search 规则可以是确定性的（greedy search）也可以是随机性的(do sample)\n",
    "3. 我们如果以\"联合概率和\"最大为最优搜索， 我们难以搜索到全局最优生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3c2cac0-9197-4bda-84b8-c269b4bf479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop(0): tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79, 79, 60, 13, 65,  4],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 33, 65,  5, 59, 79]])\n",
      "loop(1): tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79, 47, 95, 38, 95, 22],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 42, 10, 21, 60, 10]])\n",
      "loop(2): tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79, 19, 26, 27, 95, 56],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 37, 85, 30, 35, 84]])\n"
     ]
    }
   ],
   "source": [
    "def generation(\n",
    "    model = None,\n",
    "    input_ids: torch.tensor = None,\n",
    "    max_new_token: int = 100,\n",
    "    temperature: float = 1.0, \n",
    "):\n",
    "    for i in range(max_new_token):\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "        logits = logits[:, -1, :] / temperature \n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        # next_token_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat( [input_ids, next_token_idx], dim = -1 )\n",
    "    return input_ids\n",
    "\n",
    "# logits = model(input_ids)\n",
    "for i in range(3):\n",
    "    result = generation(model, input_ids, max_new_token = 5)\n",
    "    print(f'loop({i}):', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3114f62-bb7e-4249-a8d2-edf8ba8bea2b",
   "metadata": {},
   "source": [
    "## top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bbd1c85-0876-4610-bf2e-9b0fcb3918b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop(0): tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79, 95, 73, 38, 34, 32],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 85,  5, 59, 73, 68]])\n",
      "loop(1): tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79,  1, 49, 60, 38, 13],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 82,  1, 31, 95, 45]])\n",
      "loop(2): tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79, 60, 38,  5, 59, 74],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 85, 85, 60, 66, 31]])\n"
     ]
    }
   ],
   "source": [
    "def generation(\n",
    "    model = None,\n",
    "    input_ids: torch.tensor = None,\n",
    "    max_new_token: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 100, \n",
    "):\n",
    "    top_k = min(top_k, model.config.vocab_size)\n",
    "    \n",
    "    for i in range(max_new_token):\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "        logits = logits[:, -1, :] / temperature \n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        value, idx = torch.topk(probs, k = top_k, dim = -1)\n",
    "\n",
    "        new_logits = torch.ones_like(probs) * -100000.0 \n",
    "        new_logits[:,idx] = logits[:,idx] \n",
    "        \n",
    "        probs = F.softmax(new_logits, dim = -1)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat( [input_ids, next_token_idx], dim = -1 )\n",
    "    return input_ids\n",
    "\n",
    "# logits = model(input_ids)\n",
    "for i in range(3):\n",
    "    result = generation(model, input_ids, max_new_token = 5, top_k = 10)\n",
    "    print(f'loop({i}):', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7563e4-6c9f-4f49-83b0-1baa2d67a9b3",
   "metadata": {},
   "source": [
    "## top-p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075ba28-63b1-4f3e-9664-894c964edaa6",
   "metadata": {},
   "source": [
    "top-k 存在截断问题，且生成过程中每个时刻的分布差异大，故K难以确定。\n",
    "\n",
    "如果用概率和就能避免上述问题。\n",
    "\n",
    "关键实现在于：\n",
    "\n",
    "1. 对 vocab prob 降序排序\n",
    "2. 用 `cumsum` 累计概率和, 并且找到第一个超过阈值 p 的位置 m\n",
    "3. 取 m~l 位置概率设置为 0， 并**映射**回原词表位置\n",
    "4. 对概率归一化\n",
    "5. 用 multinominal 采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "725973de-db37-48d5-b8a2-6c995ab0a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0841, 0.0000, 0.1266, 0.0845, 0.3021, 0.2462, 0.0783, 0.0000],\n",
      "        [0.1198, 0.1051, 0.3100, 0.1333, 0.1548, 0.0000, 0.0904, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "logits = torch.randn(2, 8)\n",
    "probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "topp_probs = sorted_probs.clone()\n",
    "\n",
    "cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "for i in range(2):\n",
    "    for j in range(vocab_size):\n",
    "        if cumsum_probs[i,j] > 0.95:\n",
    "            sorted_probs[i, j:] = 0\n",
    "            topp_probs[i, sorted_indices[i,:].tolist()] = sorted_probs[i, :]\n",
    "            break\n",
    "print(topp_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ca3c8a4-a7e9-46a5-8238-dd47334a0e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,\n",
      "         87,  7, 21, 96, 39, 79,  9, 97, 31, 36,  7],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 88, 56, 56,  4,  4, 82, 98, 19, 34, 55,\n",
      "         87, 46, 72, 46, 84,  1, 60, 25, 21, 43, 18]])\n"
     ]
    }
   ],
   "source": [
    "def generation(\n",
    "    model = None,\n",
    "    input_ids: torch.tensor = None,\n",
    "    max_new_token: int = 100,\n",
    "    top_p: float = 0.95,\n",
    "    temperature: float = 1.1,\n",
    "):\n",
    "    top_p = min(top_p, 1.0)\n",
    "    bs, seq_len = input_ids.shape\n",
    "    \n",
    "    for i in range(max_new_token):\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "        logits = logits[:, -1, :] / temperature \n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        topp_probs = sorted_probs.clone()\n",
    "        for i in range(bs):\n",
    "            # for j in range(model.config.vocab_size):\n",
    "            idx = torch.where(cumsum_probs[i,:] > top_p)\n",
    "            sorted_probs[i, idx[0][0]:] = 0 # 首个累计和大于 p 的位置\n",
    "            topp_probs[i, sorted_indices[i,:].tolist()] = sorted_probs[i, :]\n",
    "            \n",
    "        probs = topp_probs / topp_probs.sum(dim = -1).unsqueeze(dim = 1)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat( [input_ids, next_token_idx], dim = -1 )\n",
    "    return input_ids\n",
    "\n",
    "result = generation(model, input_ids, max_new_token = 5, top_p = 0.95)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa90715-3fde-4f4e-9f88-bb4c93f72d33",
   "metadata": {},
   "source": [
    "当 Top-K 和 Top-P 同时存在时，实现顺序是怎么样的？\n",
    "\n",
    "1. 先 top-k 粗筛， 再 top-p 精筛。 top-p 需要排序和cumsum，top-k 能减少 vocab 候选规模。\n",
    "2. softmax 发生在 top-k 之前。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f6475-bd4e-40ad-abd1-a6d110758826",
   "metadata": {},
   "source": [
    "## 其他\n",
    "\n",
    "1. batch 数量对推理效率的影响\n",
    "2. inference 分什么阶段\n",
    "3. inference 有哪些存在重复计算\n",
    "4. 为什么说语言模型是生成模型\n",
    "5. 语言模型为什么可以多种输出\n",
    "6. llm 为什么会产生重复生成现象，有什么方法可以避免吗？\n",
    "7. 什么是 KV-cache，如何实现，kv-cache 加速的本质是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf351d3c-53ad-494c-bc0c-f3c86bf6a00a",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[GPT2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "[GPT2 code](https://github.com/openai/gpt-2)\n",
    "\n",
    "[GPT2 by Transformers](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

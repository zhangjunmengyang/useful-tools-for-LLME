{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411fc916-acd1-4486-b693-805df02c71ec",
   "metadata": {},
   "source": [
    "# In-Context Learning\n",
    "\n",
    "## 定义\n",
    "\n",
    "1. GPT-2 预训练模型具有 zero-shot learning 能力\n",
    "2. GPT-3 提出 In-context learning 概念， zero-shot 为 in-context learning 特例。其包含 zero/one/few-shot leanring 三种常见形式\n",
    "3. in-context leanring 直观翻译是 “在上下文中学习”， “上下文“指输入信息。ICL 是 inference 技巧。\n",
    " \n",
    "什么是 zero-shot learning？\n",
    "\n",
    "1. 模型训练时未见过监督任务数据\n",
    "2. 模型输入未有示例\n",
    "3. learning 分两种，训练参数发生改变则是永久性的，而本 notebook 重点讨论**即时性学习**\n",
    "\n",
    "给定概率模型:\n",
    "\n",
    "$$\n",
    "p_\\theta( \\text{output}|\\text{input})\n",
    "$$\n",
    "\n",
    "在输入部分增加任务描述， 如翻译任务 `I have a dream.中译英:`, 其中`中译英:` 为任务描述，或者是一种提示或提示词。将输入描述和任务描述进行结构化组织的操作称之为格式化。\n",
    "\n",
    "$$\n",
    "p_\\theta( \\text{output}|\\text{input}, \\text{task})\n",
    "$$\n",
    "\n",
    "上述即为一种 zero-shot learning, 可以认为改变了“输入”分布，从而改变“输出”分布\n",
    "\n",
    "格式化分为两种：\n",
    "\n",
    "1. QA格式化：所有的问题都能表示为，“question：xxx answer：xxx”， 模型能理解问答模型\n",
    "2. task-specified 格式化：“Q：小冬瓜 翻译-> A：”, “Q：context{我在东北玩泥巴}，rewrite->A:”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106bd40-efe4-442b-878a-00b78ab9761a",
   "metadata": {},
   "source": [
    "## 为什么预训练模型能 zero-shot leanring Work\n",
    "\n",
    "1. 并非所有的模型都能 work， 如 BERT 或 一些参数量小的模型\n",
    "2. language model 输入输出具有强灵活性，任意任务都能组织成文本序列（模版化）. 无法现象一个纯图像分类模型，在不改变模型结构和参数的情况下，zero-shot learning 去做图像检测任务。\n",
    "3. 预训练数据中包含**知识**， 这种知识并非是具体的 task, 而是与 task 有关的内容。根据 GPT-2 论文 table.1 示例\n",
    "\n",
    "\n",
    "”I’m not the cleverest man in the world, but like they say in\n",
    "French: **Je ne suis pas un imbecile [I’m not a fool].**\n",
    "\n",
    "In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate\n",
    "in the riding of Joliette, wrote in French: **”Mentez mentez,\n",
    "il en restera toujours quelque chose,”** which translates as,\n",
    "**”Lie lie and something will always remain.”**\n",
    "\n",
    "“I hate the word **‘perfume'**\" Burr says. ‘It’s somewhat better\n",
    "in French: ‘parfum.’\n",
    "\n",
    "If listened carefully at 29:55, a conversation can be heard\n",
    "between two guys in French: “**-Comment on fait pour aller\n",
    "de l’autre cote? -Quel autre cot ´ e?´** ”, which means “**-How\n",
    "do you get to the other side? - What side?**”.\n",
    "\n",
    "If this sounds like a bit of a stretch, consider this question in French: **As-tualler au cinema?**´ , or **Did you go to\n",
    "the movies?**, which literally translates as Have-you to go to\n",
    "movies/theater?\n",
    "\n",
    "“**Brevet Sans Garantie Du Gouvernement**”, translated to\n",
    "English: “**Patented without government warranty**”.\n",
    "\n",
    "预训练数据中存在有类似“法语与英语翻译”的知识，相较与记忆知识，无监督预训练从海量语料中学会了 “翻译技巧”，**通用**学习了“文本推断逻辑”，这是预训练与监督学习的本质区别。\n",
    "\n",
    "另外，zero-shot learning 也表明模型具有强**泛化性**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f35b6e-4446-4d56-82a9-a1f87e542484",
   "metadata": {},
   "source": [
    "## zero-shot 太硬核，那么 one-shot 和 few-shot 是否还有含金量？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3108a0-80d3-4d9e-b5fa-5ddc655de798",
   "metadata": {},
   "source": [
    "什么是 one-shot 和 few-shot？ 在输入上增加示例：\n",
    "\n",
    "$$\n",
    "p_\\theta( \\text{output}|\\text{input}, \\text{task}, \\text{example}_1, \\text{example}_2, \\ldots )\n",
    "$$\n",
    "\n",
    "该处理方法可以提供示例来约束问答格式，或者给到模型直接参考。所有的 in-context leanring 可以抽象成：\n",
    "\n",
    "$$\n",
    "p_\\theta( \\text{output}|\\text{input}, \\text{prompt}), \n",
    "$$\n",
    "\n",
    "即 $\\text{prompt}$ 是自由的，改变输入能提高回答的正确性，这类方法都具有含金量。\n",
    "\n",
    "在涌现能力论文中，正是用 few-shot learning 方式测评，发现预训练模型部分结果好于一些经过“specified training” 的SOTA监督模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176060e-2a23-4c4b-9f99-106c82fc46b1",
   "metadata": {},
   "source": [
    "## zero-shot 是否是横空出世的？\n",
    "\n",
    "1. zero-shot 一直是业界难点\n",
    "2. GPT-2 在 Inference 阶段采用 zero-shot 方式在多个 NLP 任务上测评\n",
    "\n",
    "需要综合 scaling-law， emergent-abilitis， in-context learning，unspervised-pretrained， transformer 等挂件因素，来考量 GPT-2/3 的影响力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d61766-b318-49e8-a290-cdb1722967d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1089b0db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d5267-d761-4011-b70e-cb2f55955974",
   "metadata": {},
   "source": [
    "## 创建语料和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4b91e9-2341-4bff-b8cc-fc847a01ba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 7, 8, 11, 12]\n",
      "我要翻译中文\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = '''我有一个梦想,翻译摘要中文英文法文会,大语言模型,小冬瓜, i have a dream, abcdefghijklmnopqrstuvwxyzQWERTYUIOPASDFGHJKLZXCVBNM,~!@#$%^&*()_+`1234567890-={}[]:\";'<>?,./\\\\\\n'''\n",
    "class SimplestTokenizer:\n",
    "    def __init__(self, text):\n",
    "        tokens = list(text)\n",
    "        self.vocab = {}\n",
    "        self.vocab_reverse = {}\n",
    "        idx = 0\n",
    "        for i in tokens:\n",
    "            if i not in self.vocab:\n",
    "                self.vocab[i] = idx\n",
    "                self.vocab_reverse[idx] = i\n",
    "                idx += 1\n",
    "    def encode(self, text, return_pt = False):\n",
    "        tokens = list(text)\n",
    "        token_ids = [ self.vocab[token] for token in tokens]\n",
    "        if return_pt:\n",
    "            token_ids = torch.tensor(token_ids, dtype = torch.long).unsqueeze(0)\n",
    "        return token_ids\n",
    "    def decode(self, ids):\n",
    "        token_list = [self.vocab_reverse[idx] for idx in ids]\n",
    "        text = ''.join(token_list)\n",
    "        return text\n",
    "        \n",
    "        \n",
    "tokenizer = SimplestTokenizer(text)\n",
    "print(tokenizer.encode('我要翻译中文'))\n",
    "print(tokenizer.decode([0, 10, 7, 8, 11, 12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b724dd-c9c6-4ab2-88dc-7535401220d8",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8b4cc0-b46d-4a2b-abff-6571d441fc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4684,  0.0700, -0.4173,  ...,  0.4209, -0.2419, -0.0283],\n",
       "         [ 0.2404, -0.0940,  0.0741,  ...,  0.3407,  0.3816,  0.2700],\n",
       "         [ 0.0018,  0.1066, -0.0899,  ..., -0.1597, -0.1091,  0.0417],\n",
       "         ...,\n",
       "         [ 0.7019,  0.2580, -0.6060,  ..., -0.5172,  0.1008, -0.1332],\n",
       "         [ 0.0083,  0.2066, -0.2648,  ...,  0.1222,  0.5677,  0.0291],\n",
       "         [ 0.3804,  0.2200, -0.6631,  ..., -0.2236, -0.4892,  0.3182]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InContextLearner(nn.Module):\n",
    "    def __init__(self, vocab_size = 100):\n",
    "        super().__init__()\n",
    "        self.dim = 512\n",
    "        self.embd = nn.Embedding(vocab_size, self.dim)\n",
    "        self.w0 = nn.Linear(self.dim, self.dim)\n",
    "        self.w1 = nn.Linear(self.dim, self.dim)\n",
    "        self.lm_head = nn.Linear(self.dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        e = self.embd(x)\n",
    "        e = torch.sin(self.w0(e).mean(dim = 1, keepdim=True)) + self.w1(e)\n",
    "        logits = self.lm_head(e)\n",
    "        return logits\n",
    "        \n",
    "vocab_size = len(tokenizer.vocab)\n",
    "model = InContextLearner(vocab_size = vocab_size)\n",
    "x = torch.randint(vocab_size, (1, 32))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd344b3-e501-40f0-8690-50b78750283f",
   "metadata": {},
   "source": [
    "## 推断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8baaf7eb-fea7-4a47-a8bd-31e150771168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我有一个梦想f!AhcZ英C语O^梦D想语O^梦D想\n"
     ]
    }
   ],
   "source": [
    "def generate(model, x, max_new_tokens = 20):\n",
    "    for i in range(max_new_tokens):\n",
    "        logits = model(x)\n",
    "        new_token = torch.argmax(logits[:, -1, :], dim = -1)\n",
    "        x = torch.cat((x, new_token.unsqueeze(1)), dim = 1)\n",
    "    return x\n",
    "\n",
    "text = '我有一个梦想'\n",
    "input_ids = tokenizer.encode(text, return_pt = True)\n",
    "result = generate(model, input_ids)[0,:].tolist()\n",
    "text = tokenizer.decode(result)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c6328-50bd-450c-9e00-f079f106ca9b",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266c92cb-289f-478e-a447-9a812ae937b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Question:中文{我有一个梦想} 中译英> \n",
      "#Answer:译D有型言J要 o&冬&冬&冬&冬&冬&\n"
     ]
    }
   ],
   "source": [
    "def format_translate(text):\n",
    "    return '中文{' + text + '} 中译英> '\n",
    "\n",
    "def forward_QA(text):\n",
    "    return '\\n#Question:' + text + '\\n#Answer:'\n",
    "\n",
    "text = '我有一个梦想'\n",
    "\n",
    "text_task = format_translate(text)\n",
    "text_task_qa = forward_QA(text_task)\n",
    "\n",
    "input_ids = tokenizer.encode(text_task_qa, return_pt = True)\n",
    "result = generate(model, input_ids)[0,:].tolist()\n",
    "text = tokenizer.decode(result)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b486e1b-5937-4806-b52c-823d0ed5aec0",
   "metadata": {},
   "source": [
    "## One-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f0d2b0-a9eb-4912-9773-55cc923fa27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文{小模型} 中译英> small model\n",
      "\n",
      "#Question:中文{我有一个梦想} 中译英> \n",
      "#Answer:译D有型言J要 o&冬&冬&冬&冬&冬&\n"
     ]
    }
   ],
   "source": [
    "def format_translate_example(text, output):\n",
    "    return '中文{' + text + '} 中译英> ' + output +'\\n'\n",
    "\n",
    "example1 = format_translate_example('小模型', 'small model')\n",
    "\n",
    "text_task_qa_1shot = example1 + text_task_qa\n",
    "\n",
    "input_ids = tokenizer.encode(text_task_qa_1shot, return_pt = True)\n",
    "result = generate(model, input_ids)[0,:].tolist()\n",
    "text = tokenizer.decode(result)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1b447-d0ee-4161-83ef-8f962c11074b",
   "metadata": {},
   "source": [
    "## Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9eb4eb8-2ce7-4c45-b4ed-99a6a8743bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文{小模型} 中译英> small model\n",
      "中文{小冬瓜} 中译英> xiaodonggua AIGC\n",
      "中文{大语言模型} 中译英> Large Language Model\n",
      "\n",
      "#Question:中文{我有一个梦想} 中译英> \n",
      "#Answer:译D有型言J要 o)6*大\"-型言J要 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "example2 = format_translate_example('小冬瓜', 'xiaodonggua AIGC')\n",
    "example3 = format_translate_example('大语言模型', 'Large Language Model')\n",
    "\n",
    "text_task_qa_fewshot = example1 + example2 + example3 + text_task_qa\n",
    "\n",
    "input_ids = tokenizer.encode(text_task_qa_fewshot, return_pt = True)\n",
    "result = generate(model, input_ids)[0,:].tolist()\n",
    "text = tokenizer.decode(result)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee16587-3f48-47d8-93df-98cc61c1b26c",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "1. 上下文学习ICL 特点：a. 改变输入 b. 不改变参数 c. Inference 使用 4.即时学习\n",
    "2. Training、Inference、ICL 他们推理形式是同源的；\n",
    "3. Pretrained任务本身也是根据 context 进行推断\n",
    "4. 表征学习视角理解 ICL ：few-shot 例子在 latent 层面**增加特定模式的推理特征**\n",
    "5. 假设 context 是无限的，那么特定文本就一定能被生成出来（如随机模型根据随机context，产生一部莎士比亚的小说）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

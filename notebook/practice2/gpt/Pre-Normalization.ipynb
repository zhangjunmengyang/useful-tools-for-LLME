{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6660d05-0c59-4828-a61d-ecf60b522250",
   "metadata": {},
   "source": [
    "# Pre-Normalization\n",
    "\n",
    "- Normalization 动机\n",
    "- Post-Norm 数据分布分析\n",
    "- Pre-Norm 数据分布分析\n",
    "- Pre-Norm 与预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688d6076-67c6-4c97-8ee1-c286178f8c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1134bcdb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b7c83-3700-43fa-b1ba-88fe045d4e88",
   "metadata": {},
   "source": [
    "## Normalization 动机\n",
    "\n",
    "深度神经网络均有 normalization，如 CV 领域常有 Batch Normalization\n",
    "\n",
    "在 Resnet 中，结合 short-cut 操作， 形如:\n",
    "\n",
    "$$\n",
    "y = \\text{Norm}(F(x) + x)\n",
    "$$\n",
    "\n",
    "将网络中的数据分布归置正态分布，以帮助模型更快收敛。经典的 Norm 归一化被称之为“后置归一化（Post-Norm）”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62868f42-03fa-4c47-b8e1-8ad1ce7d4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_mean_var(x):\n",
    "    bs, seq_len, dim = x.shape\n",
    "    batch_mean = x.reshape(bs*seq_len, dim).mean(dim = -1).mean()\n",
    "    batch_var = x.reshape(bs*seq_len, dim).var(dim = -1).mean()\n",
    "    print('batch_mean/var:', batch_mean.item(), batch_var.item())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8d4553-8842-4c4f-8862-6b4807c0bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0277)\n",
      "tensor(0.9440)\n",
      "batch_mean/var: 0.005180692300200462 0.9886613488197327\n"
     ]
    }
   ],
   "source": [
    "# 数据归一化, 统计所有token其特征维度上的 mean 和 var，的均值。\n",
    "\n",
    "x = torch.randn(2, 3, 512) # bs, seq_len, dim\n",
    "print(x[0,0,:].mean(dim = -1)) # 单个 token mean\n",
    "print(x[0,0,:].var(dim = -1)) # 单个 token var\n",
    "\n",
    "static_mean_var(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc030e1b-ac0d-4d1f-8027-9244f366badb",
   "metadata": {},
   "source": [
    "## PostNorm Model 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad21eded-1bd9-4eb9-a288-18ba62ce9c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.37144470214844\n",
      "55.3714485168457\n",
      "55.3714485168457\n",
      "55.3714485168457\n",
      "55.37144470214844\n",
      "55.37144088745117\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim = 512):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = 1e-6\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim=True)\n",
    "        var = x.var(dim = -1, keepdim=True)\n",
    "        x_ = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        x_ = x_ * self.gamma + self.beta\n",
    "        return x_\n",
    "\n",
    "class ReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (x+torch.abs(x)) * 0.5\n",
    "                   \n",
    "class PostNormBlock(nn.Module):\n",
    "    def __init__(self, dim = 512):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim)\n",
    "        self.act = ReLU()\n",
    "        self.w2 = nn.Linear(dim, dim)\n",
    "        self.norm = LayerNorm(dim = dim)\n",
    "    def forward(self, x):\n",
    "        x_w1 = self.w1(x)\n",
    "        x_act = self.act(x_w1)\n",
    "        x_w2 = self.w2(x_act)\n",
    "        y = self.norm(x_w2+x)\n",
    "        return y\n",
    "\n",
    "class PostNormModel(nn.Module):\n",
    "    def __init__(self, dim = 512, num_layers = 6, num_class = 10):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_class = 10\n",
    "        self.num_layers = num_layers\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [PostNormBlock(dim) for _ in range(self.num_layers)]\n",
    "        )\n",
    "        self.head = nn.Linear(self.dim, self.num_class)\n",
    "        \n",
    "    def forward(self, x, verbose = False):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            if verbose:\n",
    "                print(x.norm().item())\n",
    "        last_hidden_state = x\n",
    "        x = x.mean(dim = 1)\n",
    "        logits = self.head(x) # logits: bs, seq_len, num_class\n",
    "        return logits, last_hidden_state\n",
    "\n",
    "model = PostNormModel() \n",
    "\n",
    "x = torch.randn(2, 3, 512) \n",
    "\n",
    "y, last_hidden_state = model(x, verbose = True)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f000499-37ec-482d-a179-aeae386c0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mean/var: 4.113341223188627e-09 0.9999990463256836\n"
     ]
    }
   ],
   "source": [
    "# print(last_hidden_state.reshape(6, 512).mean(dim = -1).mean()) # 批量数据 mean mean\n",
    "# print(last_hidden_state.reshape(6, 512).var(dim = -1).mean()) # 批量数据 var mean\n",
    "static_mean_var(last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7592c7-f22e-477c-b873-8fa2761c5a7a",
   "metadata": {},
   "source": [
    "## PreNorm Model 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c3a94-238c-4946-9d58-dc8a263fecc1",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "y = x + F(\\text{Norm}(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b802b5-69bf-4c6c-81f7-b680cd6f1db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.251708984375 \t delta: 1.43743896484375\n",
      "59.79402160644531 \t delta: 1.5423126220703125\n",
      "60.97715759277344 \t delta: 1.183135986328125\n",
      "62.46257019042969 \t delta: 1.48541259765625\n",
      "63.61300277709961 \t delta: 1.1504325866699219\n",
      "64.91265106201172 \t delta: 1.2996482849121094\n",
      "last hidden state norm: tensor(55.3715, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.Size([2, 10])\n",
      "batch_mean/var: -1.1641532182693481e-09 0.9999993443489075\n"
     ]
    }
   ],
   "source": [
    "class PreNormBlock(nn.Module):\n",
    "    def __init__(self, dim = 512):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim)\n",
    "        self.act = ReLU()\n",
    "        self.w2 = nn.Linear(dim, dim)\n",
    "        self.norm = LayerNorm(dim = dim)\n",
    "    def forward(self, x):\n",
    "        x_ = self.norm(x)\n",
    "        x_w1 = self.w1(x_)\n",
    "        x_act = self.act(x_w1)\n",
    "        x_w2 = self.w2(x_act)\n",
    "        y = x + x_w2\n",
    "        return y\n",
    "\n",
    "class PreNormModel(nn.Module):\n",
    "    def __init__(self, dim = 512, num_layers = 6, num_class = 10):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_class = 10\n",
    "        self.num_layers = num_layers\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [PreNormBlock(dim) for _ in range(self.num_layers)]\n",
    "        )\n",
    "        # 最后一层加入 layernorm\n",
    "        self.last_norm = LayerNorm(dim = self.dim) \n",
    "        self.head = nn.Linear(self.dim, self.num_class)\n",
    "\n",
    "    def forward(self, x, add_last_norm = False, verbose=False):\n",
    "        old_norm = x.norm()\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            if verbose:\n",
    "                print(x.norm().item(), '\\t delta:',(x.norm() - old_norm).item())\n",
    "            old_norm = x.norm()\n",
    "        last_hidden_state = x\n",
    "        if add_last_norm:\n",
    "            last_hidden_state = self.last_norm(x)\n",
    "            if verbose:\n",
    "                print('last hidden state norm:', last_hidden_state.norm())\n",
    "        y = last_hidden_state.mean(dim = 1)\n",
    "        logits = self.head(y) # logits: bs, seq_len, num_class\n",
    "        return logits, last_hidden_state\n",
    "\n",
    "\n",
    "model = PreNormModel() \n",
    "x = torch.randn(2, 3, 512)\n",
    "y, last_hidden_state = model(x, add_last_norm = True, verbose = True)\n",
    "print(y.shape)\n",
    "static_mean_var(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a7c9d4-4c95-4772-ba62-4eb7df55e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.251708984375 \t delta: 1.43743896484375\n",
      "59.79402160644531 \t delta: 1.5423126220703125\n",
      "60.97715759277344 \t delta: 1.183135986328125\n",
      "62.46257019042969 \t delta: 1.48541259765625\n",
      "63.61300277709961 \t delta: 1.1504325866699219\n",
      "64.91265106201172 \t delta: 1.2996482849121094\n",
      "batch_mean/var: -0.007094651460647583 1.3694543838500977\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "y, last_hidden_state = model(x, add_last_norm = False, verbose = True)\n",
    "static_mean_var(last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df58d3c-2fab-443a-ab3a-6f4626813c45",
   "metadata": {},
   "source": [
    "综上 PreNorm 设置下，不带 last norm 的情况下，方差变大且不为1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5bc3e-07ee-4957-938a-2be415506671",
   "metadata": {},
   "source": [
    "## 分析 Norm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc37a35-b776-4c96-bc10-f0d8d5e29e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mean/var: 0.0006401605205610394 1.0001459121704102\n",
      "batch_mean/var: -0.00033171908580698073 1.0000011920928955\n",
      "batch_mean/var: 0.0003084417257923633 1.999786138534546\n",
      "batch_mean/var: 0.00021810112230014056 0.9998931884765625\n",
      "batch_mean/var: 0.00015422086289618164 0.4999465346336365\n"
     ]
    }
   ],
   "source": [
    "bs = 32\n",
    "seq_len = 256\n",
    "dim = 512\n",
    "x = torch.randn(bs, seq_len, dim)\n",
    "y = torch.randn(bs, seq_len, dim)\n",
    "xy = x+y\n",
    "xy_sqrt2 = xy / math.sqrt(2)\n",
    "xy_2 = xy / 2\n",
    "static_mean_var(x)\n",
    "static_mean_var(y)\n",
    "static_mean_var(xy)\n",
    "static_mean_var(xy_sqrt2)\n",
    "static_mean_var(xy_2) # 方差，标准差均不为1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8de056-7cd4-4c4f-bd84-166315382df2",
   "metadata": {},
   "source": [
    "## 深层数据变化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46cf8f2-aac0-49c3-8f0e-51bd733b4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.46917724609375 \t delta: 1.7455635070800781\n",
      "57.96708297729492 \t delta: 1.4979057312011719\n",
      "59.74176788330078 \t delta: 1.7746849060058594\n",
      "61.387413024902344 \t delta: 1.6456451416015625\n",
      "62.77439498901367 \t delta: 1.3869819641113281\n",
      "64.00464630126953 \t delta: 1.2302513122558594\n",
      "65.04314422607422 \t delta: 1.0384979248046875\n",
      "66.82162475585938 \t delta: 1.7784805297851562\n",
      "68.3145980834961 \t delta: 1.4929733276367188\n",
      "69.6028823852539 \t delta: 1.2882843017578125\n",
      "70.9032211303711 \t delta: 1.3003387451171875\n",
      "71.81951141357422 \t delta: 0.916290283203125\n",
      "73.65388488769531 \t delta: 1.8343734741210938\n",
      "74.57453918457031 \t delta: 0.920654296875\n",
      "75.3595962524414 \t delta: 0.7850570678710938\n",
      "76.23462677001953 \t delta: 0.875030517578125\n",
      "77.53563690185547 \t delta: 1.3010101318359375\n",
      "78.72762298583984 \t delta: 1.191986083984375\n",
      "80.00578308105469 \t delta: 1.2781600952148438\n",
      "80.94267272949219 \t delta: 0.9368896484375\n",
      "81.98242950439453 \t delta: 1.0397567749023438\n",
      "83.35127258300781 \t delta: 1.3688430786132812\n",
      "84.24541473388672 \t delta: 0.8941421508789062\n",
      "85.73395538330078 \t delta: 1.4885406494140625\n",
      "86.44859313964844 \t delta: 0.7146377563476562\n",
      "87.28376770019531 \t delta: 0.835174560546875\n",
      "88.0152359008789 \t delta: 0.7314682006835938\n",
      "89.19428253173828 \t delta: 1.179046630859375\n",
      "90.05870056152344 \t delta: 0.8644180297851562\n",
      "90.93941497802734 \t delta: 0.8807144165039062\n",
      "92.31917572021484 \t delta: 1.3797607421875\n",
      "93.11075592041016 \t delta: 0.7915802001953125\n",
      "94.11396789550781 \t delta: 1.0032119750976562\n",
      "95.898193359375 \t delta: 1.7842254638671875\n",
      "96.85228729248047 \t delta: 0.9540939331054688\n",
      "97.45289611816406 \t delta: 0.6006088256835938\n",
      "98.1111831665039 \t delta: 0.6582870483398438\n",
      "99.18614959716797 \t delta: 1.0749664306640625\n",
      "100.4725341796875 \t delta: 1.2863845825195312\n",
      "101.1808853149414 \t delta: 0.7083511352539062\n",
      "last hidden state norm: tensor(55.3715, grad_fn=<LinalgVectorNormBackward0>)\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "model = PreNormModel(num_layers = 40) \n",
    "x = torch.randn(2, 3, 512)\n",
    "y, last_hidden_state = model(x, add_last_norm = True, verbose = True)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b9fa2-926c-4c53-aa82-04ce3a4d8f53",
   "metadata": {},
   "source": [
    "随着深度变化，其方差越来越小，prenorm设定下，深层网络其层越虚，可视为宽层网络（注意：这是直觉的理解）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe778a9-fb3e-47ac-875c-2991b890e48f",
   "metadata": {},
   "source": [
    "## 如何初始化\n",
    "\n",
    "以下的实现中，哪怕没有 last norm，其 last hidden state 不会偏差太多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e481bf-5dc2-4350-8591-c4578c72ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.10917282104492 \t delta: 1.1105537414550781\n",
      "57.41032028198242 \t delta: 1.3011474609375\n",
      "58.757999420166016 \t delta: 1.3476791381835938\n",
      "60.30143356323242 \t delta: 1.5434341430664062\n",
      "61.772682189941406 \t delta: 1.4712486267089844\n",
      "63.676788330078125 \t delta: 1.9041061401367188\n",
      "64.64246368408203 \t delta: 0.9656753540039062\n",
      "65.39842987060547 \t delta: 0.7559661865234375\n",
      "66.78181457519531 \t delta: 1.3833847045898438\n",
      "68.16849517822266 \t delta: 1.3866806030273438\n",
      "69.3364028930664 \t delta: 1.16790771484375\n",
      "70.53028106689453 \t delta: 1.193878173828125\n",
      "71.57947540283203 \t delta: 1.0491943359375\n",
      "73.1681137084961 \t delta: 1.5886383056640625\n",
      "74.08837127685547 \t delta: 0.920257568359375\n",
      "75.53704071044922 \t delta: 1.44866943359375\n",
      "76.9328384399414 \t delta: 1.3957977294921875\n",
      "77.95159149169922 \t delta: 1.0187530517578125\n",
      "78.58661651611328 \t delta: 0.6350250244140625\n",
      "79.57111358642578 \t delta: 0.9844970703125\n",
      "80.55424499511719 \t delta: 0.9831314086914062\n",
      "82.01812744140625 \t delta: 1.4638824462890625\n",
      "83.23473358154297 \t delta: 1.2166061401367188\n",
      "84.26969146728516 \t delta: 1.0349578857421875\n",
      "85.26600646972656 \t delta: 0.9963150024414062\n",
      "86.322509765625 \t delta: 1.0565032958984375\n",
      "87.75630187988281 \t delta: 1.4337921142578125\n",
      "88.64421844482422 \t delta: 0.8879165649414062\n",
      "89.74353790283203 \t delta: 1.0993194580078125\n",
      "91.37188720703125 \t delta: 1.6283493041992188\n",
      "92.45801544189453 \t delta: 1.0861282348632812\n",
      "93.81951904296875 \t delta: 1.3615036010742188\n",
      "95.01560974121094 \t delta: 1.1960906982421875\n",
      "95.19140625 \t delta: 0.1757965087890625\n",
      "95.80082702636719 \t delta: 0.6094207763671875\n",
      "96.34595489501953 \t delta: 0.5451278686523438\n",
      "96.53661346435547 \t delta: 0.1906585693359375\n",
      "97.83257293701172 \t delta: 1.29595947265625\n",
      "98.60445404052734 \t delta: 0.771881103515625\n",
      "99.74002075195312 \t delta: 1.1355667114257812\n",
      "last hidden state norm: tensor(55.3715, grad_fn=<LinalgVectorNormBackward0>)\n",
      "batch_mean/var: 7.605801322085881e-09 0.9999997019767761\n"
     ]
    }
   ],
   "source": [
    "class PreNormBlockInit(nn.Module):\n",
    "    def __init__(self, dim = 512, num_layers = 6):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, dim)\n",
    "        self.act = ReLU()\n",
    "        self.w2 = nn.Linear(dim, dim)\n",
    "        self.norm = LayerNorm(dim = dim)\n",
    "\n",
    "        # reference\n",
    "        # transformers/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        \n",
    "        initializer_range = 0.02\n",
    "        std = initializer_range / math.sqrt(2 * num_layers)\n",
    "        nn.init.normal_(self.w1.weight, mean=0.0, std=std)\n",
    "        nn.init.normal_(self.w2.weight, mean=0.0, std=std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = self.norm(x)\n",
    "        x_w1 = self.w1(x_)\n",
    "        x_act = self.act(x_w1)\n",
    "        x_w2 = self.w2(x_act)\n",
    "        y = x + x_w1\n",
    "        return y\n",
    "\n",
    "class PreNormModelInit(nn.Module):\n",
    "    def __init__(self, dim = 512, num_layers = 6, num_class = 10):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_class = 10\n",
    "        self.num_layers = num_layers\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [PreNormBlockInit(dim, num_layers = self.num_layers) for i in range(self.num_layers)]\n",
    "        )\n",
    "        # 最后一层加入 layernorm\n",
    "        self.last_norm = LayerNorm(dim = self.dim) \n",
    "        self.head = nn.Linear(self.dim, self.num_class)\n",
    "\n",
    "    def forward(self, x, add_last_norm = False, verbose=False):\n",
    "        old_norm = x.norm()\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            if verbose:\n",
    "                print(x.norm().item(), '\\t delta:',(x.norm() - old_norm).item())\n",
    "            old_norm = x.norm()\n",
    "        last_hidden_state = x\n",
    "        if add_last_norm:\n",
    "            last_hidden_state = self.last_norm(x)\n",
    "            if verbose:\n",
    "                print('last hidden state norm:', last_hidden_state.norm())\n",
    "        y = last_hidden_state.mean(dim = 1)\n",
    "        logits = self.head(y) # logits: bs, seq_len, num_class\n",
    "        return logits, last_hidden_state\n",
    "\n",
    "\n",
    "model = PreNormModel(num_layers = 40) \n",
    "x = torch.randn(2, 3, 512)\n",
    "y, last_hidden_state = model(x, add_last_norm = True, verbose = True)\n",
    "static_mean_var(last_hidden_state) # 方差为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e89884f9-16aa-4339-b715-3d595e9d4b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.08007049560547 \t delta: 1.649871826171875\n",
      "57.136348724365234 \t delta: 1.0562782287597656\n",
      "58.2601203918457 \t delta: 1.1237716674804688\n",
      "59.80127716064453 \t delta: 1.5411567687988281\n",
      "61.09972381591797 \t delta: 1.2984466552734375\n",
      "62.547061920166016 \t delta: 1.4473381042480469\n",
      "63.97510528564453 \t delta: 1.4280433654785156\n",
      "65.08590698242188 \t delta: 1.1108016967773438\n",
      "65.98751068115234 \t delta: 0.9016036987304688\n",
      "67.04659271240234 \t delta: 1.05908203125\n",
      "68.25686645507812 \t delta: 1.2102737426757812\n",
      "69.205322265625 \t delta: 0.948455810546875\n",
      "70.34056854248047 \t delta: 1.1352462768554688\n",
      "71.13099670410156 \t delta: 0.7904281616210938\n",
      "72.20435333251953 \t delta: 1.0733566284179688\n",
      "73.48302459716797 \t delta: 1.2786712646484375\n",
      "75.00140380859375 \t delta: 1.5183792114257812\n",
      "76.28553771972656 \t delta: 1.2841339111328125\n",
      "77.14724731445312 \t delta: 0.8617095947265625\n",
      "78.60157012939453 \t delta: 1.4543228149414062\n",
      "79.80459594726562 \t delta: 1.2030258178710938\n",
      "80.9670639038086 \t delta: 1.1624679565429688\n",
      "82.59400177001953 \t delta: 1.6269378662109375\n",
      "83.7342300415039 \t delta: 1.140228271484375\n",
      "85.14686584472656 \t delta: 1.4126358032226562\n",
      "85.96392822265625 \t delta: 0.8170623779296875\n",
      "86.8717269897461 \t delta: 0.9077987670898438\n",
      "87.8089599609375 \t delta: 0.9372329711914062\n",
      "89.1898193359375 \t delta: 1.380859375\n",
      "90.64288330078125 \t delta: 1.45306396484375\n",
      "91.88455200195312 \t delta: 1.241668701171875\n",
      "92.5798110961914 \t delta: 0.6952590942382812\n",
      "93.47168731689453 \t delta: 0.891876220703125\n",
      "94.76266479492188 \t delta: 1.2909774780273438\n",
      "96.05581665039062 \t delta: 1.29315185546875\n",
      "96.684814453125 \t delta: 0.628997802734375\n",
      "97.58245849609375 \t delta: 0.89764404296875\n",
      "98.01251220703125 \t delta: 0.4300537109375\n",
      "98.72547912597656 \t delta: 0.7129669189453125\n",
      "99.98155975341797 \t delta: 1.2560806274414062\n",
      "batch_mean/var: 0.06068084016442299 3.2531518936157227\n"
     ]
    }
   ],
   "source": [
    "model = PreNormModel(num_layers = 40) \n",
    "x = torch.randn(2, 3, 512)\n",
    "y, last_hidden_state = model(x, add_last_norm = False, verbose = True)\n",
    "static_mean_var(last_hidden_state) # 方差为 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1faac-32cb-4287-ac2b-bea2683d8c62",
   "metadata": {},
   "source": [
    "## 补充推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257943f-717e-4f40-8e4b-d32b1f3b5b48",
   "metadata": {},
   "source": [
    "[浅谈Transformer的初始化、参数化与标准化](https://kexue.fm/archives/8620#残差连接)\n",
    "\n",
    "该文章推导了 post-norm 和 pre-norm 的数据分布的方差变化。结论：\n",
    "\n",
    "1. pre-norm 最后一层要加 norm\n",
    "2. pre-norm 每层接受的原始输入分量是一样的\n",
    "3. post-norm 随深度增加，原始输入分量会随着减少\n",
    "\n",
    "本文代码从特征维度的方差进行分析，代码按照深层 normalization 对数据分布影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8330cd1-97ee-46ac-8d51-73eb5b8757f5",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "1. PostNorm 实际上分布一直在归一化，PreNorm 在已学习好的分布上，继续强化，在深层上方差变化较小，增加深度收益变小\n",
    "2. PreNorm 在预训练中可以不 warmup，PreNorm 更容易训练\n",
    "3. PreNorm 在首层输入即进行了 norm， 对于 transformer 输入来说， 存在 E+PE，该数据不服从 0,1 正态分布\n",
    "4. PreNorm 为当前 Transformer 类网络的标准归一化手段。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

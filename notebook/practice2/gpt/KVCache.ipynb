{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c532c829-2f80-4fdb-b62c-461d61186a12",
   "metadata": {},
   "source": [
    "# KV Cache\n",
    "\n",
    "1. Attention 类模型如 transformer / GPT 的训练和推理有什么特性\n",
    "2. Decoder 预测时有什么计算特性\n",
    "3. Decoder 有什么冗余计算\n",
    "4. KV Cache 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c4a1d2-6da6-4008-b104-a64112b2dd27",
   "metadata": {},
   "source": [
    "## 1. Decoder Toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4717084-7522-414a-9ee1-5b3485b4b3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f3bcd90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b840a1-710f-4101-9bc4-aff30c2ced95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "mask = -(1 - torch.tril(torch.ones(4, 4))) * torch.tensor(float('inf'))\n",
    "mask = torch.nan_to_num(mask, nan=0.0)\n",
    "print(mask)\n",
    "p = F.softmax(mask, dim = -1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500b8913-a650-4477-95a3-db63c1f2e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 100])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim = 512):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.wq = nn.Linear(dim, dim)\n",
    "        self.wk = nn.Linear(dim, dim)\n",
    "        self.wv = nn.Linear(dim, dim)\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x, mask, verbose = False):\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        s = q@k.transpose(2,1) / math.sqrt(self.dim)\n",
    "        if verbose:\n",
    "            print('(q,k,v,s).shape:', q.shape, k.shape, v.shape, s.shape)\n",
    "        s = s+mask.unsqueeze(0)\n",
    "        p = F.softmax(s, dim = -1)\n",
    "        z = p @ v\n",
    "        return self.wo(z)\n",
    "        \n",
    "    \n",
    "class SimplesDecoder(nn.Module):\n",
    "    def __init__(self, dim = 512, vocab_size = 100, max_len = 1024):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(vocab_size, dim)\n",
    "        self.attn = Attention(dim)\n",
    "        self.lm_head = nn.Linear(dim, vocab_size)\n",
    "        self.mask = -(1 - torch.tril(torch.ones(max_len, max_len))) * torch.tensor(float('inf'))\n",
    "        self.mask = torch.nan_to_num(self.mask, nan=0.0)\n",
    "\n",
    "    def forward(self, x, verbose = False):\n",
    "        bs, seq_len = x.shape\n",
    "        X = self.embd(x)\n",
    "        X = self.attn(X, self.mask[:seq_len, :seq_len], verbose=verbose)\n",
    "        logits = self.lm_head(X)\n",
    "        return logits\n",
    "\n",
    "dim = 512,\n",
    "seq_len = 16\n",
    "vocab_size = 100\n",
    "batch_size = 2\n",
    "input_ids = torch.randint(vocab_size, [batch_size, seq_len])\n",
    "# model = SimplesDecoder(dim = dim, vocab_size = vocab_size)\n",
    "model = SimplesDecoder()\n",
    "logits = model(input_ids)\n",
    "print(logits.shape)\n",
    "# print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2865f36-7c7f-4806-8a1f-cb483066d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[69, 47]])\n",
      "tensor([[69, 47]])\n",
      "(q,k,v,s).shape: torch.Size([1, 2, 512]) torch.Size([1, 2, 512]) torch.Size([1, 2, 512]) torch.Size([1, 2, 2])\n",
      "tensor([[69, 47, 15]])\n",
      "(q,k,v,s).shape: torch.Size([1, 3, 512]) torch.Size([1, 3, 512]) torch.Size([1, 3, 512]) torch.Size([1, 3, 3])\n",
      "tensor([[69, 47, 15, 15]])\n",
      "(q,k,v,s).shape: torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 4])\n",
      "tensor([[69, 47, 15, 15, 15]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.randint(vocab_size, [1, 2])\n",
    "print(input_ids)\n",
    "def generation(\n",
    "    model = None,\n",
    "    input_ids: torch.tensor = None,\n",
    "    max_new_token: int = 100,\n",
    "):\n",
    "    for i in range(max_new_token):\n",
    "        print(input_ids)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(input_ids, verbose = True)\n",
    "        logits = logits[:, -1, :] # zhe\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        next_token_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        input_ids = torch.cat( [input_ids, next_token_idx], dim = -1 )\n",
    "    return input_ids\n",
    "\n",
    "# logits = model(input_ids)\n",
    "result = generation(model, input_ids, max_new_token = 3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106223b-7505-4f6e-b440-640b52909cdc",
   "metadata": {},
   "source": [
    "1. 在推理过程中， q,k,v 第 seq_len 维度累增，存在重复计算\n",
    "2. 在推理过程中， score 可以看出 attention 重复计算\n",
    "\n",
    "出现上述原因在于，next token prediction 在推理过程中，`input_ids` 在累增\n",
    "\n",
    "1. 在第 t=1 `forward` 时不存在重复计算\n",
    "2. 在 t>1 `forward` 时以累增的 `input_ids` 预测\n",
    "\n",
    "next token prediction 预测的本质是：第 $t=n$ 时的 $q_{\\color{red}{n}} $  与 $k_{1:n}, v_{1:n}$ 做注意力计算\n",
    "\n",
    "上述代码中实际上是 $q_{\\color{red}{1:} n}$  与 $k_{1:n}, v_{1:n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600a659-ef91-42be-a44e-331ad22169f8",
   "metadata": {},
   "source": [
    "## KV-Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9ed28e-bb14-45e3-b47d-d8199e5c6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionKVCache(nn.Module):\n",
    "    def __init__(self, dim = 512):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.wq = nn.Linear(dim, dim)\n",
    "        self.wk = nn.Linear(dim, dim)\n",
    "        self.wv = nn.Linear(dim, dim)\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.KV_cache = None\n",
    "        \n",
    "    def forward(self, x, mask, verbose = False):\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        \n",
    "        if verbose:\n",
    "            print('(q,k,v).shape:', q.shape, k.shape, v.shape,)\n",
    "            if self.KV_cache is None:\n",
    "                print('KV Cache is empty')\n",
    "            else:\n",
    "                print('KV Cache.shape:', self.KV_cache[0].shape, self.KV_cache[1].shape)\n",
    "\n",
    "\n",
    "        if self.KV_cache is None:\n",
    "            self.KV_cache = [k, v]\n",
    "        else:\n",
    "            self.KV_cache[0] = torch.cat((self.KV_cache[0], k), dim = 1) # k\n",
    "            self.KV_cache[1] = torch.cat((self.KV_cache[1], v), dim = 1) # v\n",
    "\n",
    "        \n",
    "        s = q @ self.KV_cache[0].transpose(2,1) / math.sqrt(self.dim)\n",
    "\n",
    "        if self.KV_cache is None:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        else:\n",
    "            mask = mask[-1, :].unsqueeze(0).unsqueeze(1) # 注意 mask 取最后一行 \n",
    "        s = s + mask \n",
    "\n",
    "        if verbose:\n",
    "            print('(s,mask).shape:',  s.shape, mask.shape)\n",
    "\n",
    "        p = F.softmax(s, dim = -1)\n",
    "        z = p @ self.KV_cache[1]\n",
    "        return self.wo(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b0ab81-a0d6-46c8-976d-71e6c142425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplesDecoderKVCache(nn.Module):\n",
    "    def __init__(self, dim = 512, vocab_size = 100, max_len = 1024):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(vocab_size, dim)\n",
    "        self.attn = AttentionKVCache(dim)\n",
    "        self.lm_head = nn.Linear(dim, vocab_size)\n",
    "        self.mask = -(1 - torch.tril(torch.ones(max_len, max_len))) * torch.tensor(float('inf'))\n",
    "        self.mask = torch.nan_to_num(self.mask, nan=0.0)\n",
    "\n",
    "    def forward(self, x, cur_len, verbose = False):\n",
    "        bs, seq_len = x.shape\n",
    "        X = self.embd(x)\n",
    "        X = self.attn(X, self.mask[:cur_len, :cur_len], verbose=verbose)\n",
    "        logits = self.lm_head(X)\n",
    "        return logits\n",
    "        \n",
    "model = SimplesDecoderKVCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159b6061-b4c5-4857-90f3-8b34e2ba4533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[69, 47]])\n",
      "---------- loop: 0 ----------\n",
      "input_ids: tensor([[69, 47]])\n",
      "(q,k,v).shape: torch.Size([1, 2, 512]) torch.Size([1, 2, 512]) torch.Size([1, 2, 512])\n",
      "KV Cache is empty\n",
      "(s,mask).shape: torch.Size([1, 2, 2]) torch.Size([1, 1, 2])\n",
      "---------- loop: 1 ----------\n",
      "input_ids: tensor([[65]])\n",
      "(q,k,v).shape: torch.Size([1, 1, 512]) torch.Size([1, 1, 512]) torch.Size([1, 1, 512])\n",
      "KV Cache.shape: torch.Size([1, 2, 512]) torch.Size([1, 2, 512])\n",
      "(s,mask).shape: torch.Size([1, 1, 3]) torch.Size([1, 1, 3])\n",
      "---------- loop: 2 ----------\n",
      "input_ids: tensor([[65]])\n",
      "(q,k,v).shape: torch.Size([1, 1, 512]) torch.Size([1, 1, 512]) torch.Size([1, 1, 512])\n",
      "KV Cache.shape: torch.Size([1, 3, 512]) torch.Size([1, 3, 512])\n",
      "(s,mask).shape: torch.Size([1, 1, 4]) torch.Size([1, 1, 4])\n",
      "tensor([[69, 47, 65, 65, 98]])\n"
     ]
    }
   ],
   "source": [
    "# input_ids = torch.randint(vocab_size, [1, 2])\n",
    "print(input_ids)\n",
    "def generation(\n",
    "    model = None,\n",
    "    input_ids: torch.tensor = None,\n",
    "    max_new_token: int = 100,\n",
    "):\n",
    "    input_len = input_ids.shape[1]\n",
    "    output_ids = input_ids.clone()\n",
    "    for i in range(max_new_token):\n",
    "        \n",
    "        print('-' * 10, 'loop:', i, '-' * 10)\n",
    "        print('input_ids:', input_ids)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(input_ids, cur_len = input_len+i,  verbose = True)\n",
    "        logits = logits[:, -1, :] # zhe\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        next_token_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "  \n",
    "        # input_ids = torch.cat( [input_ids, next_token_idx], dim = -1 )\n",
    "        input_ids = next_token_idx # 输入 1 个 token\n",
    "        \n",
    "        output_ids = torch.cat([output_ids, next_token_idx], dim = -1 )\n",
    "        \n",
    "    return output_ids\n",
    "\n",
    "# logits = model(input_ids)\n",
    "result = generation(model, input_ids, max_new_token = 3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cd3b7-b4f0-45db-9ce0-bba656bad612",
   "metadata": {},
   "source": [
    "## 推理分析\n",
    "\n",
    "1. 在 for loop 上，输入由 `input_ids += next_token_idx`  变为 `input_ids = next_token_idx`\n",
    "2. 在 attention 内部，遵循 1 q 和 kv-cache 进行算注意力\n",
    "3. 在 t=1 时，填充 kv-cache，此时称为 prefill 阶段， 在 t>1 时, 一个 q 与多个 kv 算注意力，此时称为 decoding 阶段\n",
    "4. prefill 与 training forward 的模式时相同的， 多 q 和 多kv， decoding 较为特殊是 1q 和 多KV\n",
    "5. prefill, forward 计算是 block-wise 的，并行度高， decoding 是 line-wise 的，需要采用 batch-decoding 提高并行度\n",
    "6. prefill, forward 有 compute-bound， decoding 有 memory-bound，例如 多层 attention 中，单个数据，需要多次访问 wq,wk,wv,wo 做投影，或者频繁加载kv cache计算\n",
    "7. 不同的计算特性需要设计对应的机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b5c88-9c9d-4626-9840-d2a05a78977b",
   "metadata": {},
   "source": [
    "## KV Cache 分析\n",
    "\n",
    "1. KV Cache 存储量: `bsz x seq_len x dim x attention_num_layers x bits`， 根据此模型可以有多种 kv-cache 存储量减少方法\n",
    "2. generation 时随 seq_len 线形递增存储量\n",
    "3. 一种 memory-efficient 方法则可以提高 cache 的管理效率， 管理的是存储过程\n",
    "4. KV Cache 是 masked-self-attention 类模型的必要技术, Transformer 解码模型实际上也可以用 KV-Cache 技术\n",
    "5. 分析 foward、prefill、decoding 的计算复杂度，从计算角度分析计算复杂度是否等同于计算效率？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

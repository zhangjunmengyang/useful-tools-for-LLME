{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfbc673-037e-4c40-9ac5-b798001924a2",
   "metadata": {},
   "source": [
    "## BPE Tokenizer\n",
    "\n",
    "BPE(Byte-Pair Encoding) Tokenizer 是基于统计规律构建子词（sub-word）分词器，被 GPT-1 采用，并成为主流的大模型分词方法。\n",
    "\n",
    "分词单位常见的有：\n",
    "\n",
    "- 字符：'a','b','!','_','好', 具有基础的字符词表，可以表示任意文本，确定是编码后的序列太长，对于 序列模型 来说编码效率低影响计算效率。\n",
    "- 单词：'hello ', 'world','!'，其形式严格，扩展性差。\n",
    "- 子词：是一种囊括字符、单词和连续文本的一种文本定义，其形式自由灵活，且对通用语种有较强的适配性。例如：'hello', 'he', 'h', 'hello world!', '!', 'ld', ... 都可以视为子词的范畴。其包含基础的字符词表，可以继承编码任意文本特性，同时，其序列长度更短，较单词又更灵活，且通用性更高。\n",
    "\n",
    "子词类分词器的缺陷是什么？\n",
    "\n",
    "- 数字：'9.11' 和 '9.8' 谁更大？ \n",
    "- 字符：'strawberry' 由几个 'r'？\n",
    "\n",
    "分词器由两个组件构成：\n",
    "\n",
    "1. 规则：规定了序列的划分方式，其划分规则具有唯一性，固定规则下产生唯一词表、编码和解码结果\n",
    "2. 词表：根据规则在语料上进行切分序列得到的子词\n",
    "\n",
    "分词器使用包含三个过程，其各阶段的输入输出分别是什么？\n",
    "\n",
    "1. 训练\n",
    "2. 编码\n",
    "3. 解码\n",
    "\n",
    "回到 BPE, Byte 指的是文本在计算机中的最小表示单位，即字符。Pair 指的是相邻 Byte 之间可以进行**融合**形成 **新的Byte**（新的 Byte 也当成是最小文本单位）。在 BPE 中，关键其实是 Pair 按照什么规则来构建，这是 BPE 算法的核心。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aca1cc-e560-4636-93e3-ffc30cf544dd",
   "metadata": {},
   "source": [
    "## Byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc477086-3c28-45ee-8309-5d2debdde6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes: b'a'\n",
      "b'a'\n",
      "b'b'\n",
      "b'1'\n",
      "\u0001\n",
      "\u0002\n",
      "\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('bytes:', bytes([97]))\n",
    "print('a'.encode(\"utf-8\"))\n",
    "print('b'.encode(\"utf-8\"))\n",
    "print('1'.encode(\"utf-8\"))\n",
    "print(b'\\x01'.decode())\n",
    "print(b'\\x02'.decode())\n",
    "print(b'\\x09'.decode())\n",
    "print(b'\\x7f'.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665a08d-5c5e-4fdc-a05f-d7b9eb332bc3",
   "metadata": {},
   "source": [
    "一段文本可以用 UTF-8 转化成 Byte 序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3358320b-b6e5-4f1a-b10f-f4a5e0fd24fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358\n",
      "byte pisition_0 : 32:\t b''\n",
      "byte pisition_1 : 32:\t b' '\n",
      "byte pisition_2 : 32:\t b'  '\n",
      "byte pisition_3 : 10:\t b'   '\n",
      "byte pisition_4 : 76:\t b'   \\n'\n",
      "byte pisition_5 : 97:\t b'   \\nL'\n",
      "byte pisition_6 : 114:\t b'   \\nLa'\n",
      "byte pisition_7 : 103:\t b'   \\nLar'\n",
      "byte pisition_8 : 101:\t b'   \\nLarg'\n",
      "byte pisition_9 : 32:\t b'   \\nLarge'\n"
     ]
    }
   ],
   "source": [
    "text = '''   \n",
    "Large Language Models is all you need,\n",
    "what can i say, manba out. \n",
    "Attention is All you need.\n",
    "Vision Transformers, \n",
    "Generative Pretrained Transformers,\n",
    "Reinforcement leraning from human feedback\n",
    "chain of thought is basic reasoning tool.\n",
    "LLMs can evaluate NLP results.\n",
    "Richard Sutton Refinforcement Learning Introduction edition 2.\n",
    "encoder-only framework\n",
    "'''\n",
    "\n",
    "text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "ids = list(text_bytes) # list of integers in range 0..255\n",
    "print(len(ids))\n",
    "for i in range(10):\n",
    "    print(f'byte pisition_{i} : {ids[i]}:\\t',text_bytes[:i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f37a60-6f66-4168-957b-d6eee3e43f34",
   "metadata": {},
   "source": [
    "## 中文Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3329dd1-f6d8-4769-abdf-0802697cca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe5\\xb0\\x8f'\n",
      "小\n",
      "b'\\xe5\\x86\\xac'\n",
      "b'\\xe5\\xb0\\x8f\\xe5\\x86\\xac\\xe7\\x93\\x9c'\n",
      "小冬瓜\n"
     ]
    }
   ],
   "source": [
    "cn_text = \"小\"\n",
    "print(cn_text.encode(\"utf-8\"))\n",
    "print(b'\\xe5\\xb0\\x8f'.decode())\n",
    "# print(b'\\xe5\\xb0'.decode()) # 报错\n",
    "# print(b'\\xe5'.decode()) # 报错\n",
    "\n",
    "cn_text = \"冬\"\n",
    "print(cn_text.encode(\"utf-8\"))\n",
    "\n",
    "cn_text = \"小冬瓜\"\n",
    "print(cn_text.encode(\"utf-8\"))\n",
    "\n",
    "print(cn_text.encode(\"utf-8\").decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b5297-5824-4b62-a072-41be2ebd5a4a",
   "metadata": {},
   "source": [
    "## 初始化 byte 词表\n",
    "\n",
    "- 词表是字典（或者是map），更具体的数据结构是双向的字典（map）\n",
    "- 初始化字典具有 常见的英文字母和标点符号。\n",
    "- BPE 原始论文或实现仅应用于英文场景，更加通用的实现是将语料里按照最基础的 bytes 切分求 set\n",
    "- 举例，如果一个字 “冬” 在初始化词表里未出现，那么 BPE 训练或编码就无法实现 merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9158cd5-0988-4d12-93e9-c246546cd811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff'}\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(size = 256):\n",
    "    merges = {}\n",
    "    vocab = {idx: bytes([idx]) for idx in range(size)}\n",
    "    for (p0, p1), idx in merges.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(size = 256)\n",
    "print(vocab) # 字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80180a-d4ab-42e7-8d72-69e75e95ccac",
   "metadata": {},
   "source": [
    "## Byte-Pair\n",
    "\n",
    "Byte Pair 包含两个步骤：\n",
    "\n",
    "1. 构建 pair\n",
    "2. 统计 pair\n",
    "3. 选择 pair: 选择出现频次最高的 相邻 byte-pair 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe3ba1d-6396-46a7-af12-13d897b60bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1] \n",
      " [2, 3, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# 构建 pair\n",
    "example = [1, 2, 3, 1, 2] \n",
    "\n",
    "print(example[:-1],'\\n',\n",
    "      example[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe072a29-7049-426e-96b5-88e0ccbb5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get stats\n",
      "{(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
      "最大对: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# 构建-统计 pair\n",
    "def get_stats(ids, counts=None):\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "print('get stats')\n",
    "example = [1, 2, 3, 1, 2] # token id 序列\n",
    "counts = get_stats(example)\n",
    "print(counts) # 相邻token出现频次\n",
    "\n",
    "pair = max(counts, key=counts.get) \n",
    "print('最大对:', pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6664b-fff1-4775-a12d-4e51f46f6994",
   "metadata": {},
   "source": [
    "## Merge Pair\n",
    "\n",
    "Merge Pair 是 BPE 的分词规则。\n",
    "\n",
    "- 筛选最大频次相邻 byte-pair，合并成新的byte\n",
    "- 更新序列\n",
    "\n",
    "输入：语料\n",
    "\n",
    "输出：\n",
    "\n",
    "- 新的byte: 'new_byte', 扩充 byte 词表\n",
    "- 新的序列: 'newids', 达到压缩序列长度目的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219f672c-c5fc-4a42-babc-eb4dc4725fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 # 相邻两个token id 匹配上Pair, 那么就进行替换\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "ids=[1, 2, 3, 1, 2]\n",
    "pair=(1, 2)\n",
    "new_byte = max(ids) + 1\n",
    "newids = merge(ids, pair, new_byte) \n",
    "print(newids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c2de4-e1fa-4fb4-86ba-310b583177b3",
   "metadata": {},
   "source": [
    "## BPE Train\n",
    "\n",
    "BPE 训练则反复的 执行 Merge Pair 操作，产生最终 词表和序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "830b1312-d9a4-4aee-b15f-6aa0ea4a64ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 0 ['a', 'b', 'c', 'a', 'b', 'a', 'b', 'c', 'a', 'a', 'b', 'c']\n",
      "loop 0 max pair (97, 98) a b -> new byte: 256 ab\n",
      "loop 0 ['ab', 'c', 'ab', 'ab', 'c', 'a', 'ab', 'c']\n",
      "--------------------\n",
      "loop 1 ['ab', 'c', 'ab', 'ab', 'c', 'a', 'ab', 'c']\n",
      "loop 1 max pair (256, 99) ab c -> new byte: 257 abc\n",
      "loop 1 ['abc', 'ab', 'abc', 'a', 'abc']\n",
      "--------------------\n",
      "loop 2 ['abc', 'ab', 'abc', 'a', 'abc']\n",
      "loop 2 max pair (257, 256) abc ab -> new byte: 258 abcab\n",
      "loop 2 ['abcab', 'abc', 'a', 'abc']\n",
      "--------------------\n",
      "loop 3 ['abcab', 'abc', 'a', 'abc']\n",
      "loop 3 max pair (258, 257) abcab abc -> new byte: 259 abcababc\n",
      "loop 3 ['abcababc', 'a', 'abc']\n",
      "--------------------\n",
      "基础词表: b'a' b'b' b'c'\n",
      "合并的 byte-pair: {(97, 98): 256, (256, 99): 257, (257, 256): 258, (258, 257): 259}\n",
      "256 b'ab'\n",
      "257 b'abc'\n",
      "258 b'abcab'\n",
      "259 b'abcababc'\n"
     ]
    }
   ],
   "source": [
    "INITIAL_VOCAB_SIZE = 256\n",
    "\n",
    "class BasicTokenizer():\n",
    "    def __init__(self):\n",
    "        # def __init__(self):\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.vocab = self.build_vocab() # int -> bytes\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        vocab = {idx: bytes([idx]) for idx in range(INITIAL_VOCAB_SIZE)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        return vocab\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= INITIAL_VOCAB_SIZE\n",
    "        num_merges = vocab_size - INITIAL_VOCAB_SIZE\n",
    "\n",
    "        text_bytes = text.encode(\"utf-8\") \n",
    "        ids = list(text_bytes) \n",
    "\n",
    "        merges = {} \n",
    "        # int -> bytes\n",
    "        vocab = {idx: bytes([idx]) for idx in range(INITIAL_VOCAB_SIZE)} \n",
    "        for i in range(num_merges):\n",
    "            pre_ids = ids\n",
    "            stats = get_stats(ids)\n",
    "            # pair(2,3),    vocab[2]='te', vocab[3]='st'\n",
    "            pair = max(stats, key=stats.get)    \n",
    "            idx = 256 + i\n",
    "            ids = merge(ids, pair, idx)\n",
    "            merges[pair] = idx\n",
    "            \n",
    "            # 原来的词不会剔除，而是在基础词表上累加，如\n",
    "            # vocab[new_id] = 'te' + 'st' -> vocab[4] = 'test'\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]] \n",
    "            \n",
    "            if verbose:\n",
    "                # print(f'loop {i}', pre_ids, '->',)\n",
    "                print(f'loop {i}', [ vocab[tmp_idx].decode() for tmp_idx in pre_ids])\n",
    "                print(f'loop {i}', 'max pair', pair, vocab[pair[0]].decode(), vocab[pair[1]].decode() , '-> new byte:', idx, vocab[idx].decode())\n",
    "                # print(f'loop {i}', ids)\n",
    "                print(f'loop {i}', [ vocab[tmp_idx].decode() for tmp_idx in ids])\n",
    "                print('-'*20)\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "\n",
    "bpe = BasicTokenizer()\n",
    "\n",
    "\n",
    "dummy_text = 'abcababcaabc'\n",
    "new_byte_nums = 4\n",
    "bpe.train(dummy_text, vocab_size = INITIAL_VOCAB_SIZE+new_byte_nums, verbose = True) # \n",
    "print('基础词表:',bpe.vocab[97], bpe.vocab[98], bpe.vocab[99])\n",
    "print('合并的 byte-pair:',bpe.merges)\n",
    "for i in range(256, INITIAL_VOCAB_SIZE+new_byte_nums,):\n",
    "    print(i, bpe.vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2d7aae-1ade-44f4-a5ab-addc09635e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: 'st'\n",
      "257: 'er'\n",
      "258: 'er '\n",
      "259: 'fa'\n",
      "260: 'fast'\n",
      "261: 'es'\n",
      "262: 'ol'\n",
      "{(115, 116): 256, (101, 114): 257, (257, 32): 258, (102, 97): 259, (259, 256): 260, (101, 115): 261, (111, 108): 262}\n"
     ]
    }
   ],
   "source": [
    "# 注意空格会参与 byte-pair, 空格本身也是一个 byte\n",
    "text = 'old older fast faster fastest best better yes desk mess'\n",
    "bpe.train(text, vocab_size = 256+7, verbose = False )\n",
    "for i in range(256, 256+7, 1):\n",
    "    print(f'{i}: \\'{bpe.vocab[i].decode()}\\'')\n",
    "print(bpe.merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6894c077-83c2-40aa-be93-b4b029ec699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: 'st'\n",
      "257: 'er'\n",
      "258: 'er '\n",
      "259: 'fa'\n",
      "260: 'fast'\n",
      "261: 'es'\n",
      "262: 'ol'\n",
      "263: 'old'\n",
      "264: 'er fast'\n",
      "265: 'est'\n",
      "266: 'est '\n",
      "267: 'est b'\n",
      "268: 'old '\n",
      "269: 'old old'\n",
      "{(115, 116): 256, (101, 114): 257, (257, 32): 258, (102, 97): 259, (259, 256): 260, (101, 115): 261, (111, 108): 262, (262, 100): 263, (258, 260): 264, (101, 256): 265, (265, 32): 266, (266, 98): 267, (263, 32): 268, (268, 263): 269}\n"
     ]
    }
   ],
   "source": [
    "# text = 'old older fast faster fastest best better yes desk mess'\n",
    "bpe.train(text, vocab_size = 270, verbose = False ) # 270-256 = 14 个新 byte\n",
    "for i in range(256, 270, 1):\n",
    "    print(f'{i}: \\'{bpe.vocab[i].decode()}\\'')\n",
    "print(bpe.merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d3a91-79a8-42e6-a299-747131878f41",
   "metadata": {},
   "source": [
    "由于示例文本较短，当预设目标合并词表数量较大时，263～279 byte序列会一直累增。\n",
    "\n",
    "用于 BPE 训练的语料模式丰富且多样化，目标合并词表数量没有那么敏感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e1902d-7a13-4621-a81c-c92956912655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: 'b'\\xe8\\xb7''\n",
      "257: 'b'\\xe8\\xb7\\x9f''\n",
      "258: 'b'\\xe8\\xb7\\x9f\\xe7''\n",
      "259: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d''\n",
      "260: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80''\n",
      "261: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5''\n",
      "262: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0''\n",
      "263: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0\\x8f''\n",
      "264: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0\\x8f\\xe5''\n",
      "265: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0\\x8f\\xe5\\x86''\n",
      "266: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0\\x8f\\xe5\\x86\\xac''\n",
      "267: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0\\x8f\\xe5\\x86\\xac\\xe7''\n",
      "268: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0\\x8f\\xe5\\x86\\xac\\xe7\\x93''\n",
      "269: 'b'\\xe8\\xb7\\x9f\\xe7\\x9d\\x80\\xe5\\xb0\\x8f\\xe5\\x86\\xac\\xe7\\x93\\x9c''\n",
      "{(115, 116): 256, (101, 114): 257, (257, 32): 258, (102, 97): 259, (259, 256): 260, (101, 115): 261, (111, 108): 262, (262, 100): 263, (258, 260): 264, (101, 256): 265, (265, 32): 266, (266, 98): 267, (263, 32): 268, (268, 263): 269}\n"
     ]
    }
   ],
   "source": [
    "chinese_text = '跟着小冬瓜AIGC一起学习LLM'\n",
    "\n",
    "cn_bpe = BasicTokenizer()\n",
    "cn_bpe.train(chinese_text, vocab_size = 270, verbose = False ) # 270-256 = 14 个新 byte\n",
    "for i in range(256, 270, 1):\n",
    "    # print(f'{i}: \\'{cn_bpe.vocab[i].decode()}\\'')\n",
    "    print(f'{i}: \\'{cn_bpe.vocab[i]}\\'')\n",
    "print(bpe.merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce54ae2-dc06-4157-bdd5-b072786011ac",
   "metadata": {},
   "source": [
    "## Encode\n",
    "\n",
    "编码：指文本经过分词处理成离散token，再经词表映射到 token id 序列。\n",
    "\n",
    "在上述词表里，请问对于一个新的 文本 `oldest`, 从单词视角，`oldest`在训练语料中未出现， 但从 byte 视角，现有词表能表示出完整文本。\n",
    "\n",
    "那么编码结果是否为：`old` `est` 即 `[263, 265]`?\n",
    "\n",
    "```\n",
    "256: 'st'\n",
    "257: 'er'\n",
    "258: 'er '\n",
    "259: 'fa'\n",
    "260: 'fast'\n",
    "261: 'es'\n",
    "262: 'ol'\n",
    "263: 'old'\n",
    "264: 'er fast'\n",
    "265: 'est'\n",
    "266: 'est '\n",
    "267: 'est b'\n",
    "268: 'old '\n",
    "269: 'old old'\n",
    "```\n",
    "\n",
    "编码如果依靠“子串匹配”，编码效率非常低。一般而言，编码与训练的分词规则是一样的。\n",
    "\n",
    "编码类似训练。先将文本转化成最小粒度的 byte， 依次合并。 `oldest` 例子所遇到的问题在于：\n",
    "\n",
    " \n",
    "`ol`,`ld`,`de`,`es`,`st`, 这些 byte-pair 选择哪对来进行 merge？\n",
    "\n",
    "已知 256: `st`, 261: `es`, 262: `ol`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d03d36-a5dc-4ba4-bf74-df4acdc1c4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 116)\n",
      "s t\n",
      "loop 0 ['o', 'l', 'd', 'e', 's', 't']\n",
      "loop 0 ['o', 'l', 'd', 'e', 'st']\n",
      "--------------------\n",
      "(111, 108)\n",
      "o l\n",
      "loop 1 ['o', 'l', 'd', 'e', 'st']\n",
      "loop 1 ['ol', 'd', 'e', 'st']\n",
      "--------------------\n",
      "(262, 100)\n",
      "ol d\n",
      "loop 2 ['ol', 'd', 'e', 'st']\n",
      "loop 2 ['old', 'e', 'st']\n",
      "--------------------\n",
      "(101, 256)\n",
      "e st\n",
      "loop 3 ['old', 'e', 'st']\n",
      "loop 3 ['old', 'est']\n",
      "--------------------\n",
      "(263, 265)\n",
      "old est\n",
      "loop 4 ['old', 'est']\n",
      "[263, 265]\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "# utf-8 token ids\n",
    "text = 'oldest'\n",
    "text_bytes = text.encode(\"utf-8\") \n",
    "\n",
    "# bpe token ids\n",
    "ids = list(text_bytes) \n",
    "i = 0\n",
    "while len(ids) >= 2:\n",
    "    stats = get_stats(ids)\n",
    "\n",
    "    # 关键代码\n",
    "    # 随后 block 讲解该行代码实现\n",
    "    pair = min(stats, key=lambda p: bpe.merges.get(p, float(\"inf\")))\n",
    "    \n",
    "    print(pair)\n",
    "    print(bpe.vocab[pair[0]].decode(), bpe.vocab[pair[1]].decode())\n",
    "    print(f'loop {i}', [ bpe.vocab[tmp_idx].decode() for tmp_idx in ids])\n",
    "    if pair not in bpe.merges:\n",
    "        break \n",
    "    idx = bpe.merges[pair] \n",
    "    ids = merge(ids, pair, idx) \n",
    "    print(f'loop {i}', [ bpe.vocab[tmp_idx].decode() for tmp_idx in ids])\n",
    "    print('-'*20)\n",
    "    i+=1\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe974590-d065-47fe-a692-843e64668bd3",
   "metadata": {},
   "source": [
    "### encode 合并顺序 trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88dfefde-5d1f-4b34-bf39-0bdbe448c943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Bob', 'age': 25}\n"
     ]
    }
   ],
   "source": [
    "people = [\n",
    "    {'name': 'Alice', 'age': 30},\n",
    "    {'name': 'Bob', 'age': 25},\n",
    "    {'name': 'Charlie', 'age': 35}\n",
    "]\n",
    "result = min(people, key=lambda person: person['age']) # 将 age 作为 key, 选择最小 age 值, 对应的 字典 item\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018e41be-4d65-4aca-9267-1f475a137e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 1, (2, 3): 1, (3, 4): 1, (4, 1): 1}\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "test_ids = [1,2,3,4,1]\n",
    "stats = get_stats(test_ids)\n",
    "print(stats)\n",
    "merges = {}\n",
    "merges[(3,4)] = 5\n",
    "merges[(1,2)] = 6\n",
    "\n",
    "pair = min(stats, key=lambda p: merges.get(p, float(\"inf\"))) \n",
    "print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a163c03-2de0-4636-b4ee-7e5a21b28729",
   "metadata": {},
   "source": [
    "为什么是min?  encode 的编码规则核心在于\n",
    "\n",
    "1. 合并是从最基础的 byte 开始 merge, 而非一步到位匹配词表里的子词\n",
    "2. 相邻对是否可以合并，需要参考 `bpe.merges` 表。其记录训练过程中，新 byte 的 pair 来源。所以 BPE 词表，除了存储 vocab， 还要存储 merges 表。\n",
    "3. merge 时会遇到多个 byte-pair 都在 `bpe.merges` 出现， 准则是 **合并后的 byte id 越小、说明训练时出现的频次越高**。故代码实现取min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940791a-a81d-4ccd-8310-333463b0297b",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2d9b683-57cf-4e7f-a9c9-bd4389c22fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[263, 265]\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad930013-dd27-4bdd-9060-baea5a083b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oldest\n"
     ]
    }
   ],
   "source": [
    "text_bytes = b\"\".join(bpe.vocab[idx] for idx in ids)\n",
    "decode_text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6856c22-276d-4609-92c7-2ec1de2974e0",
   "metadata": {},
   "source": [
    "## 中英 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa46145-1509-44f1-9f10-80fdb4266e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = '''\n",
    "Large Language Models is all you need,\n",
    "what can i say, manba out. \n",
    "Attention is All you need.\n",
    "Vision Transformers, \n",
    "Generative Pretrained Transformers,\n",
    "Reinforcement leraning from human feedback\n",
    "chain of thought is basic reasoning tool.\n",
    "LLMs can evaluate NLP results.\n",
    "Richard Sutton Refinforcement Learning Introduction edition 2.\n",
    "encoder-only framework\n",
    "大型语言模型就是所需的一切，\n",
    "我还能说什么呢，曼巴退场。\n",
    "注意力机制就是所需的一切。\n",
    "视觉变换器，\n",
    "生成式预训练变换器，\n",
    "基于人类反馈的强化学习，\n",
    "思维链是基本推理工具。\n",
    "大型语言模型能评估自然语言处理结果。\n",
    "理查德·萨顿《强化学习导论》第二版；\n",
    "仅编码器框架。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1b5b7cc-57d2-489e-b038-48bb93a66f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "[147, 3, 9, 4, 2, 148]\n",
      "reasoning\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "\n",
    "class BPETokenizer():\n",
    "    def __init__(self, text):\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.vocab = self.build_vocab(text) # int -> bytes\n",
    "        \n",
    "    def build_vocab(self, text):\n",
    "        char_counter = Counter(text)\n",
    "        sorted_chars = sorted(char_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        i = 0\n",
    "        vocab = {}\n",
    "        for k, v in sorted_chars:\n",
    "            vocab[i] = k \n",
    "            i = i + 1\n",
    "        return vocab\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        num_merges = vocab_size - len(self.vocab)\n",
    "        reverse_vocab = {}\n",
    "        for k, v in self.vocab.items():\n",
    "            reverse_vocab[v] = k\n",
    "        ids = list(text) \n",
    "        ids = [ reverse_vocab[idx] for idx in ids]\n",
    "        merges = {} \n",
    "        vocab = copy.deepcopy(self.vocab)\n",
    "        cur_vocab_size = len(self.vocab)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pre_ids = ids\n",
    "            stats = get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)  \n",
    "            idx = cur_vocab_size + i\n",
    "            ids = merge(ids, pair, idx)\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]] \n",
    "            \n",
    "        self.merges = merges \n",
    "        self.vocab = vocab   \n",
    "        self.reverse_vocab = reverse_vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        ids = list(text) \n",
    "        ids = [ self.reverse_vocab[idx] for idx in ids]\n",
    "        i = 0\n",
    "        while len(ids) >= 2:\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break \n",
    "            idx = self.merges[pair] \n",
    "            ids = merge(ids, pair, idx) \n",
    "            i+=1\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \"\".join(bpe.vocab[idx] for idx in ids)\n",
    "        return text\n",
    "\n",
    "\n",
    "bpe = BPETokenizer( corpus_text  )\n",
    "print(len(bpe.vocab))\n",
    "\n",
    "bpe.train(corpus_text, vocab_size = 150, verbose = True) \n",
    "\n",
    "input_ids = bpe.encode('reasoning')\n",
    "print(input_ids)\n",
    "\n",
    "decode_text = bpe.decode(input_ids)\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3f01e-13a5-46ba-8a41-e148cddf5767",
   "metadata": {},
   "source": [
    "## BPE 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e854d48-31b5-448f-9fed-5dc82cec204e",
   "metadata": {},
   "source": [
    "1. BPE 是基于统计的规则分词器，分词结果（token）在统计视角是合理的，哪怕分词并不符合人类日常习惯\n",
    "2. 人类主观的分词是有不一致性的\n",
    "3. 分词需要考虑 文本编码后的压缩率\n",
    "4. 词表越大，llm model 的 embedding、lmhead层，参数量随之增大\n",
    "5. 分词器扩展实现：1.词表合并 2.手动增加token 3.增加分词器规则（如标点符号不允许被合并）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bcc339b-b621-40d3-90f3-77f449b0a48b",
   "metadata": {},
   "source": [
    "# Llama\n",
    "\n",
    "Llama 诞生于 2023.02, 在22年10月 ChatGPT 发布后，并没有与之性能对标的开源模型，Llama 一经发布，在工业/学术界引起了重要的开源热潮。在 Llama-2 后放开了商业 license\n",
    "\n",
    "1. Llama 是一种 Decoder Transformer，相较 GPT-2/3 有多种改进，如RoPE、RMSNorm、SwiGLU等\n",
    "2. Llama 加速了微调、RAG、agent、inference、training、pretrained、多模态、预训练数据工程、RL 等多个研究领域的开源发展\n",
    "3. Llama-1/2/3 其网络结构稳定，是各种工具链标准适配模型\n",
    "4. Llama 可以视为一种 dense 网络结果（从FFN层定义）， 而近年流行采用 MoE（混合专家系统）来扩展 FFN 参数，与 dense 相对应，这类模型被称为 sparse 型（如：deepseek-v3）\n",
    "\n",
    "本 notebook 实现：\n",
    "\n",
    "1. Llama 整体模型\n",
    "2. KV Cache + GQA + RoPE 的注意力组件\n",
    "3. 实现 SwiGLU\n",
    "4. 实现 Llama training & inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b42f0a0-7685-4efa-b3e8-5a4fd96edffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10536cd70>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4611a9-4b55-46a0-ba96-3a38b6cd5172",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acd8330d-9175-4ee0-a497-03a06c1be708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    learning_rate: float = 0.001\n",
    "    vocab_size: int = 200\n",
    "    max_len: int = 512\n",
    "    dim: int = 512\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int = 4 # for GQA\n",
    "    num_layers: int = 6\n",
    "    position_encoding_base: float = 10000.0\n",
    "    pad_token_id: int = 0\n",
    "    attention_bias: bool = False # without bias\n",
    "config = LlamaConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3381058-631c-4859-9c2b-e029998713e8",
   "metadata": {},
   "source": [
    "## Toy Model\n",
    "\n",
    "1. 输入层没有 位置编码\n",
    "2. block 里引入 rope; block里包含模块 GQA、RMSNorm、SwiGLU、RoPE\n",
    "3. 输入层包含 norm 和 lm_heads\n",
    "4. Linear 层去除 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a64055a-8cac-4b40-a69a-9f5e767da718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32, 200])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个参数模型学习：\n",
    "\n",
    "class LlamaToyModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embd = nn.Embedding(config.vocab_size, config.dim)\n",
    "\n",
    "        # 仅用 linear 示例\n",
    "        self.RMSNorm1 = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.GQA = nn.Linear(config.dim, config.dim, bias=False) # with RoPE\n",
    "        self.RMSNorm2 = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.SwiGLU = nn.Linear(config.dim, config.dim, bias=False)\n",
    "\n",
    "        \n",
    "        self.OutputNorm = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.LM_head = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            - x dim[batch,seq]\n",
    "        '''\n",
    "        # input\n",
    "        X = self.embd(x) # 没有位置编码\n",
    "        \n",
    "        # blocks\n",
    "        X_norm = self.RMSNorm1(X)        \n",
    "        X = self.GQA(X_norm) + X\n",
    "        X_norm = self.RMSNorm2(X)        \n",
    "        h = self.SwiGLU(X_norm) + X\n",
    "\n",
    "        # lm head\n",
    "        h = self.OutputNorm(h)\n",
    "        logits = self.LM_head(h)\n",
    "        return logits\n",
    "\n",
    "model = LlamaToyModel(config)\n",
    "\n",
    "x = torch.randint(config.vocab_size, (2, 32))\n",
    "y_logits = model(x)\n",
    "\n",
    "print(x.shape) # # batch_size, seq_len\n",
    "print(y_logits.shape) # batch_size, seq_len, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb05ee4-a85f-4651-8194-57760fc970b6",
   "metadata": {},
   "source": [
    "## RMS Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3852f242-e8c6-4f4e-90c1-17416cda0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-12):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = (x**2).mean(-1, keepdim=True)\n",
    "        out_mean = x / torch.sqrt(mean + self.eps) # root mean square\n",
    "        out = self.gamma * out_mean \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de620c0b-03e9-4483-9137-65e1d4ad982c",
   "metadata": {},
   "source": [
    "## SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef75c0ea-9c86-4c46-8105-510d5a4cef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5429, -0.2782,  0.0819, -0.2592]])\n",
      "tensor([[ 0.5429, -0.2782,  0.0819, -0.2592]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,4)\n",
    "print(F.silu(a))\n",
    "print(F.sigmoid(a)*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7ab4efd-9986-464a-bddf-72553ad8169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = dim * 8 // 3\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_act = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.silu(self.w_act(x)) * self.w1(x)\n",
    "        return self.w2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83402b88-f7d8-4fc1-abee-388d957fe446",
   "metadata": {},
   "source": [
    "## RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7cb90db-eb2c-4070-97b9-6de645fdb566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n",
      "torch.Size([2, 8, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "def create_rope(max_len=1024, dim=512, base = 10000.0):\n",
    "    m = torch.arange(0, max_len, 1)\n",
    "    i = torch.arange(0, dim//2, 1) \n",
    "    theta = base ** (-2 * i / dim)\n",
    "    m_theta = torch.outer(m, theta)\n",
    "    cos = torch.zeros(max_len, dim)\n",
    "    sin = torch.zeros(max_len, dim) \n",
    "    cos[:, 0::2] = cos[:, 1::2] = torch.cos(m_theta) # cos(theta1), cos\n",
    "    sin[:, 0::2] = sin[:, 1::2] = torch.sin(m_theta) # sin, sin\n",
    "    return sin, cos\n",
    "\n",
    "def apply_rope(X, sin, cos):\n",
    "    bs, n_heads, seq_len, d = X.shape\n",
    "    X_shift = torch.zeros_like(X)\n",
    "    X_shift[..., 0::2] = -X[..., 1::2]\n",
    "    X_shift[..., 1::2] = X[..., 0::2]\n",
    "    Y = cos[None, None, :seq_len, :] * X + \\\n",
    "        sin[None, None, :seq_len, :] * X_shift\n",
    "    return Y\n",
    "\n",
    "sin, cos = create_rope(max_len=1024, dim=config.dim // config.n_heads)\n",
    "print(sin.shape)\n",
    "x = torch.randn(2, 8, 16, config.dim // config.n_heads)\n",
    "x_rope = apply_rope(x, sin, cos)\n",
    "print(x_rope.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a03b50-70cc-49b4-8845-f376ae9d8176",
   "metadata": {},
   "source": [
    "## GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bbf44f3-28da-416e-a7ef-1f7fbede818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_kv_heads \n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.share_heads = self.n_heads // self.n_kv_heads\n",
    "        self.wq = nn.Linear(self.dim, self.dim, bias=config.attention_bias)\n",
    "        self.wk = nn.Linear(self.dim, self.head_dim * self.n_kv_heads, bias=config.attention_bias) # grouped share k\n",
    "        self.wv = nn.Linear(self.dim, self.head_dim * self.n_kv_heads, bias=config.attention_bias) # grouped share v \n",
    "        self.wo = nn.Linear(self.dim, self.dim, bias=config.attention_bias)\n",
    "        \n",
    "    def forward(self, x, mask = None, sin=None, cos=None):\n",
    "        bsz, seq_len, dim = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # split\n",
    "        q = q.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.reshape(bsz, seq_len, self.n_kv_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.reshape(bsz, seq_len, self.n_kv_heads, self.head_dim).transpose(1,2)\n",
    "        k = torch.repeat_interleave(k, self.share_heads, dim=1)\n",
    "        v = torch.repeat_interleave(v, self.share_heads, dim=1)\n",
    "\n",
    "        # apply rope\n",
    "        q = apply_rope(q, sin, cos)\n",
    "        k = apply_rope(k, sin, cos)\n",
    "\n",
    "        # KV Cache after apply rope\n",
    "        \n",
    "        s = q@k.transpose(3,2) / math.sqrt(self.dim)\n",
    "        if mask is not None:\n",
    "            s = s + mask.unsqueeze(0).unsqueeze(0)\n",
    "        p = F.softmax(s, dim = -1)\n",
    "        z = p @ v\n",
    "\n",
    "        # cat\n",
    "        z = z.transpose(1,2).reshape(bsz, seq_len, self.dim)\n",
    "        \n",
    "        return self.wo(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd81672-54e9-4df9-95c3-e97b106c340c",
   "metadata": {},
   "source": [
    "## Llama Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc3aec2e-af52-4db3-b13e-21748ae4b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_kv_heads \n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "\n",
    "        self.norm1 = RMSNorm(dim = self.dim)\n",
    "        self.attn = GroupQueryAttention(config)\n",
    "        self.norm2 = RMSNorm(dim = self.dim)\n",
    "        self.ffn = SwiGLU(dim = self.dim)\n",
    "        \n",
    "    def forward(self, X, mask = None, sin=None, cos=None):\n",
    "        bsz, seq_len, _ = X.shape\n",
    "\n",
    "        # blocks\n",
    "        X_norm = self.norm1(X)        \n",
    "        X = self.attn(X_norm, mask, sin, cos) + X \n",
    "        X_norm = self.norm2(X)        \n",
    "        h = self.ffn(X_norm) + X\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06197951-e919-4e07-9cdb-42b69825f87f",
   "metadata": {},
   "source": [
    "## Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41b6e20e-6306-4447-ad5a-1b86fb82144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embd = nn.Embedding(self.config.vocab_size, \n",
    "                                 self.config.dim)\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [LlamaDecoderBlock(self.config) for _ in range(self.config.num_layers)]\n",
    "        )\n",
    "        self.ln = RMSNorm(self.config.dim)\n",
    "        self.lm_head = nn.Linear(self.config.dim, \n",
    "                                 self.config.vocab_size,\n",
    "                                 bias = False) # 不学习预训练数据分布偏置\n",
    "\n",
    "        self.cache_mask = torch.tril(torch.ones(self.config.max_len, \n",
    "                                                self.config.max_len)) \n",
    "        self.rope_sin, self.rope_cos = create_rope(self.config.max_len, self.config.dim // self.config.n_heads)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape\n",
    "        \n",
    "        add_mask = self.cache_mask[:seq_len, :seq_len]\n",
    "        \n",
    "        X = self.embd(x)\n",
    "        for block in self.decoder:\n",
    "            X = block(X, mask = add_mask, sin=self.rope_sin, cos=self.rope_cos)\n",
    "        X = self.ln(X)\n",
    "        logits = self.lm_head(X)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e833059-118f-4edd-a2a2-671a27d6008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 200])\n"
     ]
    }
   ],
   "source": [
    "model = LlamaModel(config)\n",
    "input_ids = torch.randint(config.vocab_size, (2, 32))\n",
    "y = model(input_ids)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

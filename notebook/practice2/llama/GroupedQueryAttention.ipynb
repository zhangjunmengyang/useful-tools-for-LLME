{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d3f613-0e53-4bb7-b86f-e9f8c1dbeca4",
   "metadata": {},
   "source": [
    "# Grouped Query Attention\n",
    "\n",
    "标准的多头注意力机制提供了强大的序列建模能力，其中“多头”能够捕捉多个角度的注意力关系。\n",
    "\n",
    "1. 对于当前尺寸模型，头数有64头/128头级别，多头注意力关系是否有冗余？\n",
    "2. KV-Cache 在 inference 阶段占用较多存储。 `bsz, seq_len, n_heads, head_dim, bit`, 如果在 `n_heads` 减少维度是否能保留原有精度？\n",
    "\n",
    "对于标准的注意力机制是多头 q，k，v 计算注意力，考虑从头数优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fff828-3ed5-4de9-93da-eb7b172526a7",
   "metadata": {},
   "source": [
    "## Multi Heads Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de992130-fdef-4d48-986d-d3e86a3cffec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 512])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class MultiHeadsAttention(nn.Module):\n",
    "    def __init__(self, dim = 512, n_heads = 8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.wq = nn.Linear(dim, dim)\n",
    "        self.wk = nn.Linear(dim, dim)\n",
    "        self.wv = nn.Linear(dim, dim)\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x, mask = None, verbose = False):\n",
    "        bsz, seq_len, dim = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # split\n",
    "        q = q.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        s = q@k.transpose(3,2) / math.sqrt(self.dim)\n",
    "        if mask is not None:\n",
    "            s = s + mask.unsqueeze(0).unsqueeze(0)\n",
    "        p = F.softmax(s, dim = -1)\n",
    "        z = p @ v\n",
    "\n",
    "        # cat\n",
    "        z = z.transpose(1,2).reshape(bsz, seq_len, self.dim)\n",
    "        \n",
    "        return self.wo(z)\n",
    "\n",
    "bsz = 2\n",
    "seq_len = 8\n",
    "dim = 512\n",
    "n_heads = 8\n",
    "MHA = MultiHeadsAttention(dim = dim, n_heads = n_heads)\n",
    "x = torch.randn(bsz, seq_len, dim)\n",
    "x_mha = MHA(x)\n",
    "x_mha.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c1922-2b46-4e70-9ac0-9a0ec68eeb9d",
   "metadata": {},
   "source": [
    "## Multi Query Attention\n",
    "\n",
    "1. 多query, 单kv\n",
    "2. 单kv share 成多 kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf92157c-efef-448e-986f-ba533d675e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kshape torch.Size([2, 1, 8, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, dim = 512, n_heads = 8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.wq = nn.Linear(dim, dim)\n",
    "        self.wk = nn.Linear(dim, self.head_dim) # 单头\n",
    "        self.wv = nn.Linear(dim, self.head_dim) # 单头\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x, mask = None, verbose = False):\n",
    "        bsz, seq_len, dim = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # split\n",
    "        q = q.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        # k = k.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        # v = v.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        k = k[:, None, :, :]\n",
    "        v = v[:, None, :, :]\n",
    "        if verbose:\n",
    "            print('kshape', k.shape)\n",
    "        \n",
    "        s = q@k.transpose(3,2) / math.sqrt(self.dim)\n",
    "        if mask is not None:\n",
    "            s = s + mask.unsqueeze(0).unsqueeze(0)\n",
    "        p = F.softmax(s, dim = -1)\n",
    "        z = p @ v\n",
    "\n",
    "        # cat\n",
    "        z = z.transpose(1,2).reshape(bsz, seq_len, self.dim)\n",
    "        \n",
    "        return self.wo(z)\n",
    "\n",
    "MQA = MultiQueryAttention(dim = dim, n_heads = n_heads)\n",
    "# x = torch.randn(bsz, seq_len, dim)\n",
    "x_mqa = MQA(x, verbose=True)\n",
    "x_mqa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9922ac-6c9d-4a75-8b30-cc3f4fcbc3b1",
   "metadata": {},
   "source": [
    "## Group Query Attention\n",
    "\n",
    "分组 share kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8838b5c3-3246-4de5-86bc-cf3ae8798e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 2, 2, 3, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "a_share = torch.repeat_interleave(a, 2, dim=0)\n",
    "print(a_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2229192f-ab0a-4c55-8624-136956b6afa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kshape torch.Size([2, 8, 8, 64])\n",
      "tensor([-0.9566, -0.9000,  0.2131,  0.0966, -0.8631], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.9566, -0.9000,  0.2131,  0.0966, -0.8631], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.4187,  0.6545, -0.2934, -0.4444, -0.0613], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.4187,  0.6545, -0.2934, -0.4444, -0.0613], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, dim = 512, n_heads = 8, n_kv_heads = 2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads \n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.share_heads = self.n_heads // self.n_kv_heads\n",
    "        self.wq = nn.Linear(dim, dim)\n",
    "        self.wk = nn.Linear(dim, self.head_dim * self.n_kv_heads) # grouped share k\n",
    "        self.wv = nn.Linear(dim, self.head_dim * self.n_kv_heads) # grouped share v \n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x, mask = None, verbose = False):\n",
    "        bsz, seq_len, dim = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # split\n",
    "        q = q.reshape(bsz, seq_len, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.reshape(bsz, seq_len, self.n_kv_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.reshape(bsz, seq_len, self.n_kv_heads, self.head_dim).transpose(1,2)\n",
    "        k = torch.repeat_interleave(k, self.share_heads, dim=1)\n",
    "        v = torch.repeat_interleave(v, self.share_heads, dim=1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('kshape', k.shape)\n",
    "            print(k[0, 0, 0, :5])\n",
    "            print(k[0, 1, 0, :5])\n",
    "            print(k[0, 2, 0, :5])\n",
    "            print(k[0, 3, 0, :5])\n",
    "        \n",
    "        s = q@k.transpose(3,2) / math.sqrt(self.dim)\n",
    "        if mask is not None:\n",
    "            s = s + mask.unsqueeze(0).unsqueeze(0)\n",
    "        p = F.softmax(s, dim = -1)\n",
    "        z = p @ v\n",
    "\n",
    "        # cat\n",
    "        z = z.transpose(1,2).reshape(bsz, seq_len, self.dim)\n",
    "        \n",
    "        return self.wo(z)\n",
    "\n",
    "n_kv_heads = 4\n",
    "GQA = GroupQueryAttention(dim = dim, n_heads = n_heads, n_kv_heads = n_kv_heads)\n",
    "# x = torch.randn(bsz, seq_len, dim)\n",
    "x_gqa = GQA(x, verbose=True)\n",
    "x_gqa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beda6de-f85e-464d-af8f-eac8d9960934",
   "metadata": {},
   "source": [
    "## 讨论\n",
    "\n",
    "1. 为什么 MQA/GQA 能 work？ 高维特征冗余或注意力关系冗余\n",
    "2. MQA/GQA 是否减少了计算量？ 1. 从投影角度减少 2. GQA 增加repeat计算（kernel可优化） 3.在注意力分数上，与标准的 MHA 计算量等同\n",
    "3. MQA 和 GQA 减少了多少的 KV-Cache？ q头数/kv头数\n",
    "4. 如果将 KV 变换视为一种压缩手段，压缩的维度极限在哪里？究竟应该如何压缩？ （ MLA优化）\n",
    "5. 从 SRAM 视角分析 MQA/GQA 加速？（从HBM 加载单头KV 到 SRAM，并在 SRAM 中将单头KV share到多个线程中，避免copy）\n",
    "6. 考虑分布式注意力计算情况， 如何部署参数和计算策略？*\n",
    "7. 考虑多层 attention 计算过程，虽然 GQA/MQA 会拓展头数（KV Cache），其 block 内随扩展随消除，对整体 KV-cache 量影响不大"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

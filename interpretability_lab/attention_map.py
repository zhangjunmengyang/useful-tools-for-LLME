"""
Attention - å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import torch
from interpretability_lab.interpretability_utils import (
    INTERPRETABILITY_MODELS,
    load_model_with_attention,
    get_attention_weights,
    compute_attention_entropy,
    get_attention_patterns
)


def render_attention_heatmap(
    attention: np.ndarray,
    tokens: list,
    title: str = "Attention Weights"
) -> go.Figure:
    """æ¸²æŸ“æ³¨æ„åŠ›çƒ­åŠ›å›¾"""
    fig = go.Figure(data=go.Heatmap(
        z=attention,
        x=tokens,
        y=tokens,
        colorscale='Blues',
        hoverongaps=False,
        hovertemplate='Query: %{y}<br>Key: %{x}<br>Weight: %{z:.4f}<extra></extra>'
    ))
    
    fig.update_layout(
        title=title,
        xaxis_title="Key Position",
        yaxis_title="Query Position",
        xaxis=dict(tickangle=45, side='bottom'),
        yaxis=dict(autorange='reversed'),
        height=500,
        width=600
    )
    
    return fig


def render_attention_grid(
    attention_weights: torch.Tensor,
    tokens: list,
    layer_idx: int,
    num_heads: int = 4
) -> go.Figure:
    """æ¸²æŸ“å¤šä¸ª head çš„æ³¨æ„åŠ›ç½‘æ ¼"""
    heads_to_show = min(num_heads, attention_weights.shape[0])
    
    fig = make_subplots(
        rows=1, cols=heads_to_show,
        subplot_titles=[f'Head {i}' for i in range(heads_to_show)],
        horizontal_spacing=0.05
    )
    
    for i in range(heads_to_show):
        attn = attention_weights[i].numpy()
        
        fig.add_trace(
            go.Heatmap(
                z=attn,
                colorscale='Blues',
                showscale=(i == heads_to_show - 1),
                hovertemplate='Q: %{y}<br>K: %{x}<br>W: %{z:.3f}<extra></extra>'
            ),
            row=1, col=i+1
        )
        
        fig.update_xaxes(
            tickvals=list(range(len(tokens))),
            ticktext=tokens,
            tickangle=45,
            row=1, col=i+1
        )
        fig.update_yaxes(
            tickvals=list(range(len(tokens))),
            ticktext=tokens if i == 0 else [],
            autorange='reversed',
            row=1, col=i+1
        )
    
    fig.update_layout(
        title=f"Layer {layer_idx} - Attention Heads",
        height=400,
        margin=dict(l=100, r=50, t=80, b=100)
    )
    
    return fig


def render_token_attention_flow(
    attention_weights: torch.Tensor,
    tokens: list,
    selected_token_idx: int
) -> go.Figure:
    """æ¸²æŸ“é€‰ä¸­ token çš„æ³¨æ„åŠ›æµå‘"""
    # å¹³å‡æ‰€æœ‰å±‚æ‰€æœ‰å¤´
    avg_attention = attention_weights.mean(dim=(0, 1)).numpy()
    
    # é€‰ä¸­ token ä½œä¸º query æ—¶å…³æ³¨çš„ keys
    query_attention = avg_attention[selected_token_idx, :]
    
    # é€‰ä¸­ token ä½œä¸º key æ—¶è¢«å“ªäº› queries å…³æ³¨
    key_attention = avg_attention[:, selected_token_idx]
    
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=[
            f'"{tokens[selected_token_idx]}" å…³æ³¨å“ªäº› tokens',
            f'å“ªäº› tokens å…³æ³¨ "{tokens[selected_token_idx]}"'
        ]
    )
    
    # Query â†’ Keys
    colors1 = ['#2563EB' if i == selected_token_idx else '#60A5FA' for i in range(len(tokens))]
    fig.add_trace(
        go.Bar(x=tokens, y=query_attention, marker_color=colors1, name='Query Attention'),
        row=1, col=1
    )
    
    # Keys â†’ Query
    colors2 = ['#DC2626' if i == selected_token_idx else '#F87171' for i in range(len(tokens))]
    fig.add_trace(
        go.Bar(x=tokens, y=key_attention, marker_color=colors2, name='Key Attention'),
        row=1, col=2
    )
    
    fig.update_layout(
        height=350,
        showlegend=False
    )
    
    fig.update_xaxes(tickangle=45)
    
    return fig


def render():
    """æ¸²æŸ“é¡µé¢"""
    st.markdown('<h1 class="module-title">Attention</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="tip-box">
    å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # æ¨¡å‹é€‰æ‹©
    model_choice = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        options=list(INTERPRETABILITY_MODELS.keys())
    )
    
    model_info = INTERPRETABILITY_MODELS[model_choice]
    
    with st.spinner(f"åŠ è½½ {model_choice}..."):
        model, tokenizer = load_model_with_attention(model_info['id'])
    
    if model is None:
        st.error("æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    st.success(f"âœ… æ¨¡å‹å·²åŠ è½½ ({model_info['layers']} å±‚, {model_info['heads']} å¤´)")
    
    st.markdown("---")
    
    # è¾“å…¥æ–‡æœ¬
    default_text = "The animal didn't cross the street because it was too tired"
    text = st.text_area(
        "è¾“å…¥æ–‡æœ¬",
        value=default_text,
        height=80,
        help="ç»å…¸çš„æŒ‡ä»£æ¶ˆè§£ä¾‹å­ï¼š'it' æŒ‡ä»£ 'animal' è¿˜æ˜¯ 'street'?"
    )
    
    if not text:
        st.info("è¯·è¾“å…¥æ–‡æœ¬")
        return
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    with st.spinner("è®¡ç®—æ³¨æ„åŠ›æƒé‡..."):
        attention_weights, tokens = get_attention_weights(model, tokenizer, text)
    
    st.caption(f"åºåˆ—é•¿åº¦: {len(tokens)} tokens")
    
    # æ˜¾ç¤º tokens
    with st.expander("æŸ¥çœ‹åˆ†è¯ç»“æœ"):
        st.write(tokens)
    
    # Causal Mask å¼€å…³
    use_causal = st.checkbox(
        "åº”ç”¨ Causal Mask",
        value=True,
        help="Decoder-only æ¨¡å‹ä½¿ç”¨ä¸‹ä¸‰è§’æ©ç ï¼Œæ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°ä¹‹å‰çš„ token"
    )
    
    if use_causal:
        seq_len = attention_weights.shape[-1]
        causal_mask = torch.tril(torch.ones(seq_len, seq_len))
        # æ³¨æ„ï¼šattention å·²ç»æ˜¯ softmax åçš„ï¼Œè¿™é‡Œåªæ˜¯å¯è§†åŒ–æ•ˆæœ
        attention_display = attention_weights.clone()
        # å°† mask å¤–çš„è®¾ä¸º 0
        attention_display = attention_display * causal_mask.unsqueeze(0).unsqueeze(0)
    else:
        attention_display = attention_weights
    
    # Tab åˆ‡æ¢
    tab1, tab2, tab3 = st.tabs(["çƒ­åŠ›å›¾", "Token åˆ†æ", "æ¨¡å¼åˆ†æ"])
    
    with tab1:
        col_layer, col_head = st.columns(2)
        
        with col_layer:
            layer_idx = st.selectbox(
                "é€‰æ‹©å±‚",
                options=list(range(model_info['layers'])),
                format_func=lambda x: f"Layer {x}"
            )
        
        with col_head:
            head_idx = st.selectbox(
                "é€‰æ‹© Head",
                options=["All Heads"] + list(range(model_info['heads'])),
                format_func=lambda x: f"Head {x}" if isinstance(x, int) else x
            )
        
        if head_idx == "All Heads":
            # æ˜¾ç¤ºå¤šä¸ª head çš„ç½‘æ ¼
            fig = render_attention_grid(
                attention_display[layer_idx],
                tokens,
                layer_idx,
                num_heads=4
            )
            st.plotly_chart(fig, width='stretch')
        else:
            # æ˜¾ç¤ºå•ä¸ª head çš„çƒ­åŠ›å›¾
            attn = attention_display[layer_idx, head_idx].numpy()
            fig = render_attention_heatmap(
                attn,
                tokens,
                f"Layer {layer_idx}, Head {head_idx}"
            )
            st.plotly_chart(fig, width='stretch')
        
        # Causal Mask å¯¹æ¯”
        st.markdown("### Causal Mask vs Full Attention")
        
        col_a, col_b = st.columns(2)
        
        with col_a:
            st.markdown("**Decoder-only (Causal Mask)**")
            st.markdown("""
            ```
            1 0 0 0 0
            1 1 0 0 0
            1 1 1 0 0
            1 1 1 1 0
            1 1 1 1 1
            ```
            æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ token
            """)
        
        with col_b:
            st.markdown("**Encoder (Full Attention)**")
            st.markdown("""
            ```
            1 1 1 1 1
            1 1 1 1 1
            1 1 1 1 1
            1 1 1 1 1
            1 1 1 1 1
            ```
            æ¯ä¸ªä½ç½®å¯ä»¥çœ‹åˆ°æ‰€æœ‰ token
            """)
    
    with tab2:
        st.markdown("### Token æ³¨æ„åŠ›åˆ†æ")
        
        # é€‰æ‹© token
        selected_idx = st.selectbox(
            "é€‰æ‹©è¦åˆ†æçš„ Token",
            options=list(range(len(tokens))),
            format_func=lambda x: f'{x}: "{tokens[x]}"'
        )
        
        # æ˜¾ç¤ºæ³¨æ„åŠ›æµå‘
        fig = render_token_attention_flow(attention_display, tokens, selected_idx)
        st.plotly_chart(fig, width='stretch')
        
        # æŒ‡ä»£æ¶ˆè§£ç¤ºä¾‹
        if "it" in text.lower() and any(word in text.lower() for word in ["animal", "street", "he", "she"]):
            st.markdown("""
            <div class="warning-box">
            ğŸ’¡ <b>æŒ‡ä»£æ¶ˆè§£</b>: é€‰æ‹© "it" æˆ–ä»£è¯ï¼Œè§‚å¯Ÿæ¨¡å‹ä¸»è¦å…³æ³¨å“ªä¸ªåè¯ã€‚
            æ³¨æ„åŠ›æƒé‡é«˜çš„ token å¯èƒ½å°±æ˜¯ä»£è¯çš„æŒ‡ä»£å¯¹è±¡ã€‚
            </div>
            """, unsafe_allow_html=True)
        
        # å„å±‚æ³¨æ„åŠ›å˜åŒ–
        st.markdown("### å„å±‚å¯¹é€‰ä¸­ Token çš„æ³¨æ„åŠ›")
        
        layer_attention = []
        for l in range(model_info['layers']):
            # å¹³å‡æ‰€æœ‰å¤´
            avg_attn = attention_display[l].mean(dim=0).numpy()
            # é€‰ä¸­ token ä½œä¸º query æ—¶çš„æ³¨æ„åŠ›åˆ†å¸ƒ
            layer_attention.append(avg_attn[selected_idx, :])
        
        layer_attn_matrix = np.array(layer_attention)
        
        fig_layers = go.Figure(data=go.Heatmap(
            z=layer_attn_matrix,
            x=tokens,
            y=[f'Layer {i}' for i in range(model_info['layers'])],
            colorscale='Viridis'
        ))
        
        fig_layers.update_layout(
            title=f'"{tokens[selected_idx]}" åœ¨å„å±‚çš„æ³¨æ„åŠ›åˆ†å¸ƒ',
            xaxis_title="Key Token",
            yaxis_title="Layer",
            height=400,
            xaxis=dict(tickangle=45)
        )
        
        st.plotly_chart(fig_layers, width='stretch')
    
    with tab3:
        st.markdown("### æ³¨æ„åŠ›æ¨¡å¼åˆ†æ")
        
        # åˆ†æå„å±‚å„å¤´çš„æ¨¡å¼
        patterns_data = []
        
        for l in range(model_info['layers']):
            for h in range(model_info['heads']):
                attn = attention_display[l, h].numpy()
                patterns = get_attention_patterns(torch.tensor(attn))
                patterns_data.append({
                    'Layer': l,
                    'Head': h,
                    'å¯¹è§’çº¿': patterns['diagonal'],
                    'é¦– Token': patterns['first_token'],
                    'å±€éƒ¨': patterns['local'],
                    'å…¨å±€': patterns['global']
                })
        
        df_patterns = pd.DataFrame(patterns_data)
        
        # æ±‡æ€»ç»Ÿè®¡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("å¹³å‡å¯¹è§’çº¿æ³¨æ„åŠ›", f"{df_patterns['å¯¹è§’çº¿'].mean():.2%}")
        with col2:
            st.metric("å¹³å‡é¦– Token æ³¨æ„åŠ›", f"{df_patterns['é¦– Token'].mean():.2%}")
        with col3:
            st.metric("å¹³å‡å±€éƒ¨æ³¨æ„åŠ›", f"{df_patterns['å±€éƒ¨'].mean():.2%}")
        with col4:
            st.metric("å¹³å‡å…¨å±€æ³¨æ„åŠ›", f"{df_patterns['å…¨å±€'].mean():.2%}")
        
        # çƒ­åŠ›å›¾å±•ç¤ºå„å¤´çš„æ¨¡å¼
        st.markdown("### å„ Head çš„æ³¨æ„åŠ›æ¨¡å¼")
        
        # é‡å¡‘ä¸º (layers, heads) çš„çŸ©é˜µ
        diagonal_matrix = df_patterns.pivot(index='Layer', columns='Head', values='å¯¹è§’çº¿')
        
        fig_pattern = go.Figure(data=go.Heatmap(
            z=diagonal_matrix.values,
            x=[f'H{i}' for i in range(model_info['heads'])],
            y=[f'L{i}' for i in range(model_info['layers'])],
            colorscale='RdBu',
            zmid=0.5,
            hovertemplate='Layer %{y}, Head %{x}<br>Diagonal Attention: %{z:.2%}<extra></extra>'
        ))
        
        fig_pattern.update_layout(
            title="å¯¹è§’çº¿æ³¨æ„åŠ›å¼ºåº¦ (æ¯ä¸ª token å…³æ³¨è‡ªå·±çš„ç¨‹åº¦)",
            xaxis_title="Head",
            yaxis_title="Layer",
            height=400
        )
        
        st.plotly_chart(fig_pattern, width='stretch')
        
        # ç†µåˆ†æ
        st.markdown("### æ³¨æ„åŠ›ç†µåˆ†æ")
        st.markdown("ç†µè¶Šé«˜è¡¨ç¤ºæ³¨æ„åŠ›è¶Šåˆ†æ•£ï¼Œç†µè¶Šä½è¡¨ç¤ºæ³¨æ„åŠ›è¶Šé›†ä¸­")
        
        entropy_data = []
        for l in range(model_info['layers']):
            for h in range(model_info['heads']):
                attn = attention_display[l, h]
                entropy = compute_attention_entropy(attn).mean().item()
                entropy_data.append({
                    'Layer': l,
                    'Head': h,
                    'Entropy': entropy
                })
        
        df_entropy = pd.DataFrame(entropy_data)
        entropy_matrix = df_entropy.pivot(index='Layer', columns='Head', values='Entropy')
        
        fig_entropy = go.Figure(data=go.Heatmap(
            z=entropy_matrix.values,
            x=[f'H{i}' for i in range(model_info['heads'])],
            y=[f'L{i}' for i in range(model_info['layers'])],
            colorscale='Plasma'
        ))
        
        fig_entropy.update_layout(
            title="æ³¨æ„åŠ›ç†µ (å€¼è¶Šå¤§è¡¨ç¤ºæ³¨æ„åŠ›è¶Šåˆ†æ•£)",
            xaxis_title="Head",
            yaxis_title="Layer",
            height=400
        )
        
        st.plotly_chart(fig_entropy, width='stretch')


"""
Logits - å¯è§†åŒ– Next Token é¢„æµ‹
å±•ç¤º Logitsã€Temperatureã€Top-P/Top-K é‡‡æ ·ç­–ç•¥
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import torch
from generation_lab.generation_utils import (
    DEMO_MODELS,
    load_model_and_tokenizer,
    get_next_token_logits,
    apply_temperature,
    apply_top_k,
    apply_top_p,
    get_sampling_distribution
)


def render_probability_bar_chart(
    tokens: list,
    probabilities: list,
    title: str = "Token æ¦‚ç‡åˆ†å¸ƒ",
    cutoff_line: float = None,
    cutoff_label: str = None,
    top_k_cutoff: int = None
):
    """æ¸²æŸ“æ¦‚ç‡æŸ±çŠ¶å›¾"""
    fig = go.Figure()
    
    # ä¸»æŸ±çŠ¶å›¾
    colors = ['#2563EB' if i < (top_k_cutoff or len(tokens)) else '#E5E7EB' 
              for i in range(len(tokens))]
    
    fig.add_trace(go.Bar(
        x=tokens,
        y=probabilities,
        marker_color=colors,
        text=[f'{p:.2%}' for p in probabilities],
        textposition='outside',
        textfont=dict(size=10)
    ))
    
    # Top-P æˆªæ–­çº¿
    if cutoff_line is not None:
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡å¯¹åº”çš„ä½ç½®
        cumsum = np.cumsum(probabilities)
        cutoff_idx = np.searchsorted(cumsum, cutoff_line)
        if cutoff_idx < len(tokens):
            fig.add_hline(
                y=probabilities[cutoff_idx],
                line_dash="dash",
                line_color="#DC2626",
                annotation_text=cutoff_label or f"Top-P æˆªæ–­ ({cutoff_line:.0%})",
                annotation_position="right"
            )
    
    # Top-K æˆªæ–­çº¿
    if top_k_cutoff is not None and top_k_cutoff < len(tokens):
        fig.add_vline(
            x=top_k_cutoff - 0.5,
            line_dash="dash",
            line_color="#D97706",
            annotation_text=f"Top-K={top_k_cutoff}",
            annotation_position="top"
        )
    
    fig.update_layout(
        title=title,
        xaxis_title="Token",
        yaxis_title="æ¦‚ç‡",
        yaxis_tickformat='.1%',
        height=400,
        margin=dict(l=50, r=50, t=50, b=100),
        showlegend=False
    )
    
    fig.update_xaxes(tickangle=45)
    
    return fig


def render_temperature_comparison(logits: torch.Tensor, temperatures: list, tokenizer, top_k: int = 10):
    """æ¸²æŸ“ä¸åŒæ¸©åº¦ä¸‹çš„æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”"""
    fig = go.Figure()
    
    colors = px.colors.qualitative.Set2
    
    for idx, temp in enumerate(temperatures):
        scaled_logits = logits.clone() / temp if temp > 0 else logits.clone()
        probs = torch.softmax(scaled_logits, dim=-1)
        top_probs, top_indices = torch.topk(probs, top_k)
        
        tokens = [tokenizer.decode([i.item()]) for i in top_indices]
        prob_values = top_probs.tolist()
        
        fig.add_trace(go.Bar(
            name=f'T={temp}',
            x=tokens,
            y=prob_values,
            marker_color=colors[idx % len(colors)],
            opacity=0.8
        ))
    
    fig.update_layout(
        title="Temperature å¯¹æ¦‚ç‡åˆ†å¸ƒçš„å½±å“",
        xaxis_title="Token",
        yaxis_title="æ¦‚ç‡",
        yaxis_tickformat='.1%',
        barmode='group',
        height=450,
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )
    
    return fig


def render_entropy_gauge(probs: torch.Tensor) -> go.Figure:
    """æ¸²æŸ“ç†µå€¼ä»ªè¡¨ç›˜"""
    # è®¡ç®—ç†µ
    probs_np = probs.numpy()
    probs_np = probs_np[probs_np > 0]  # è¿‡æ»¤é›¶æ¦‚ç‡
    entropy = -np.sum(probs_np * np.log2(probs_np))
    max_entropy = np.log2(len(probs))
    normalized_entropy = entropy / max_entropy
    
    fig = go.Figure(go.Indicator(
        mode="gauge+number+delta",
        value=entropy,
        domain={'x': [0, 1], 'y': [0, 1]},
        title={'text': "åˆ†å¸ƒç†µ (bits)", 'font': {'size': 14}},
        delta={'reference': max_entropy / 2, 'increasing': {'color': "#D97706"}},
        gauge={
            'axis': {'range': [0, max_entropy], 'tickwidth': 1},
            'bar': {'color': "#2563EB"},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "gray",
            'steps': [
                {'range': [0, max_entropy * 0.3], 'color': '#D1FAE5'},  # ä½ç†µ - ç¡®å®š
                {'range': [max_entropy * 0.3, max_entropy * 0.7], 'color': '#FEF3C7'},  # ä¸­ç†µ
                {'range': [max_entropy * 0.7, max_entropy], 'color': '#FEE2E2'}  # é«˜ç†µ - ä¸ç¡®å®š
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': max_entropy
            }
        }
    ))
    
    fig.update_layout(
        height=250,
        margin=dict(l=20, r=20, t=40, b=20)
    )
    
    return fig


def render():
    """æ¸²æŸ“é¡µé¢"""
    st.markdown('<h1 class="module-title">Logits</h1>', unsafe_allow_html=True)
    
    # æ¨¡å‹é€‰æ‹©
    col_model, col_status = st.columns([3, 1])
    with col_model:
        model_choice = st.selectbox(
            "é€‰æ‹©æ¼”ç¤ºæ¨¡å‹",
            options=list(DEMO_MODELS.keys()),
            help="é€‰æ‹©ä¸€ä¸ªè½»é‡çº§æ¨¡å‹ç”¨äºæ¼”ç¤ºï¼ˆé¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½ï¼‰"
        )
    
    model_info = DEMO_MODELS[model_choice]
    
    with col_status:
        st.caption(f"ğŸ“¦ {model_info['description']}")
    
    # åŠ è½½æ¨¡å‹
    with st.spinner(f"åŠ è½½ {model_choice}..."):
        model, tokenizer = load_model_and_tokenizer(model_info['id'])
    
    if model is None or tokenizer is None:
        st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é€‰æ‹©å…¶ä»–æ¨¡å‹")
        return
    
    st.success(f"âœ… æ¨¡å‹å·²åŠ è½½: {model_info['id']}")
    
    st.markdown("---")
    
    # è¾“å…¥åŒºåŸŸ
    prompt = st.text_area(
        "è¾“å…¥ Prompt",
        value="The quick brown fox jumps over the",
        height=100,
        placeholder="è¾“å…¥æ–‡æœ¬ï¼Œæ¨¡å‹å°†é¢„æµ‹ä¸‹ä¸€ä¸ª token..."
    )
    
    if not prompt:
        st.info("è¯·è¾“å…¥ Prompt æ–‡æœ¬")
        return
    
    # è·å–åŸå§‹ logits
    with st.spinner("è®¡ç®—ä¸­..."):
        token_candidates = get_next_token_logits(model, tokenizer, prompt, top_k=50)
    
    # åˆ›å»º tabs
    tab1, tab2, tab3 = st.tabs(["æ¦‚ç‡åˆ†å¸ƒ", "Temperature å®éªŒ", "Top-K/Top-P æˆªæ–­"])
    
    with tab1:
        st.markdown("### Next Token å€™é€‰è¯ Top-50")
        
        # å±•ç¤ºæŒ‡æ ‡
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Top-1 Token", f'"{token_candidates[0]["token_str"]}"')
        with col2:
            st.metric("Top-1 æ¦‚ç‡", f'{token_candidates[0]["probability"]:.2%}')
        with col3:
            st.metric("Top-1 Logit", f'{token_candidates[0]["logit"]:.2f}')
        with col4:
            # è®¡ç®— Top-5 ç´¯ç§¯æ¦‚ç‡
            top5_prob = sum(t['probability'] for t in token_candidates[:5])
            st.metric("Top-5 ç´¯ç§¯æ¦‚ç‡", f'{top5_prob:.2%}')
        
        # æ¦‚ç‡æŸ±çŠ¶å›¾
        tokens_display = [t['token_str'][:10] + ('...' if len(t['token_str']) > 10 else '') 
                        for t in token_candidates[:20]]
        probs = [t['probability'] for t in token_candidates[:20]]
        
        fig = render_probability_bar_chart(tokens_display, probs, "Top-20 Token æ¦‚ç‡åˆ†å¸ƒ")
        st.plotly_chart(fig, width='stretch')
        
        # è¯¦ç»†è¡¨æ ¼
        with st.expander("è¯¦ç»†æ•°æ® (Top-50)"):
            df = pd.DataFrame([{
                "æ’å": t['rank'],
                "Token": repr(t['token_str']),
                "Raw Token": t['raw_token'],
                "Token ID": t['token_id'],
                "Logit": f"{t['logit']:.4f}",
                "æ¦‚ç‡": f"{t['probability']:.4%}"
            } for t in token_candidates])
            st.dataframe(df, width="stretch", hide_index=True)
    
    with tab2:
        
        # æ¸©åº¦æ»‘å—
        temperature = st.slider(
            "Temperature",
            min_value=0.1,
            max_value=3.0,
            value=1.0,
            step=0.1,
            help="è°ƒæ•´æ¸©åº¦å‚æ•°è§‚å¯Ÿæ¦‚ç‡åˆ†å¸ƒå˜åŒ–"
        )
        
        # è·å–åŸå§‹ logits tensor
        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits[0, -1, :]
        
        # åº”ç”¨æ¸©åº¦
        scaled_logits = logits / temperature
        scaled_probs = torch.softmax(scaled_logits, dim=-1)
        original_probs = torch.softmax(logits, dim=-1)
        
        # å¯¹æ¯”æ˜¾ç¤º
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown(f"**å½“å‰æ¸©åº¦ T={temperature}**")
            top_scaled_probs, top_indices = torch.topk(scaled_probs, 10)
            tokens = [tokenizer.decode([i.item()]) for i in top_indices]
            
            fig1 = render_probability_bar_chart(
                tokens, 
                top_scaled_probs.tolist(),
                f"T={temperature} æ¦‚ç‡åˆ†å¸ƒ"
            )
            st.plotly_chart(fig1, width='stretch')
        
        with col2:
            st.markdown("**åŸå§‹åˆ†å¸ƒ T=1.0**")
            top_orig_probs, top_orig_indices = torch.topk(original_probs, 10)
            orig_tokens = [tokenizer.decode([i.item()]) for i in top_orig_indices]
            
            fig2 = render_probability_bar_chart(
                orig_tokens,
                top_orig_probs.tolist(),
                "T=1.0 åŸå§‹åˆ†å¸ƒ"
            )
            st.plotly_chart(fig2, width='stretch')
        
        # å¤šæ¸©åº¦å¯¹æ¯”
        st.markdown("### å¤šæ¸©åº¦å¯¹æ¯”")
        temps = [0.3, 0.7, 1.0, 1.5, 2.0]
        fig_compare = render_temperature_comparison(logits, temps, tokenizer, top_k=8)
        st.plotly_chart(fig_compare, width='stretch')
        
        # ç†µå€¼å±•ç¤º
        st.markdown("### åˆ†å¸ƒç†µ")
        col_a, col_b = st.columns([1, 2])
        with col_a:
            fig_entropy = render_entropy_gauge(scaled_probs)
            st.plotly_chart(fig_entropy, width='stretch')
        with col_b:
            st.markdown("""
            **ç†µå€¼è§£è¯»**ï¼š
            - ğŸŸ¢ **ä½ç†µ** (ç»¿è‰²åŒº): æ¨¡å‹éå¸¸ç¡®å®šï¼Œæ¦‚ç‡é›†ä¸­åœ¨å°‘æ•° token
            - ğŸŸ¡ **ä¸­ç†µ** (é»„è‰²åŒº): æ¨¡å‹æœ‰ä¸€å®šä¸ç¡®å®šæ€§
            - ğŸ”´ **é«˜ç†µ** (çº¢è‰²åŒº): æ¨¡å‹éå¸¸ä¸ç¡®å®šï¼Œæ¦‚ç‡åˆ†æ•£
            
            é«˜æ¸©åº¦ä¼šå¢åŠ ç†µå€¼ï¼Œä½¿åˆ†å¸ƒæ›´å‡åŒ€ï¼›ä½æ¸©åº¦åˆ™é™ä½ç†µå€¼ï¼Œä½¿åˆ†å¸ƒæ›´é›†ä¸­ã€‚
            """)
    
    with tab3:
        st.markdown("### Top-K / Top-P é‡‡æ ·æˆªæ–­")
        
        col_k, col_p = st.columns(2)
        
        with col_k:
            top_k = st.slider(
                "Top-K",
                min_value=1,
                max_value=50,
                value=10,
                help="åªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ª token ä¸­é‡‡æ ·"
            )
        
        with col_p:
            top_p = st.slider(
                "Top-P (Nucleus)",
                min_value=0.1,
                max_value=1.0,
                value=0.9,
                step=0.05,
                help="åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° P çš„ token ä¸­é‡‡æ ·"
            )
        
        # åº”ç”¨é‡‡æ ·ç­–ç•¥
        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits[0, -1, :].clone()
        
        original_probs = torch.softmax(logits, dim=-1)
        top_probs, top_indices = torch.topk(original_probs, 50)
        
        # è®¡ç®— Top-P æˆªæ–­ä½ç½®
        cumsum_probs = torch.cumsum(top_probs, dim=-1)
        top_p_cutoff = torch.searchsorted(cumsum_probs, top_p).item() + 1
        
        # å®é™…æˆªæ–­ä½ç½®ï¼ˆå– Top-K å’Œ Top-P çš„è¾ƒå°å€¼ï¼‰
        effective_cutoff = min(top_k, top_p_cutoff)
        
        st.markdown(f"""
        **é‡‡æ ·ç»“æœ**:
        - Top-K æˆªæ–­: ä¿ç•™å‰ **{top_k}** ä¸ª token
        - Top-P æˆªæ–­: ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° **{top_p:.0%}** éœ€è¦ **{top_p_cutoff}** ä¸ª token
        - æœ‰æ•ˆé‡‡æ ·èŒƒå›´: **{effective_cutoff}** ä¸ª token
        """)
        
        # å¯è§†åŒ–
        tokens_display = [tokenizer.decode([i.item()])[:8] for i in top_indices[:30]]
        probs_list = top_probs[:30].tolist()
        
        fig = go.Figure()
        
        # æ ¹æ®æ˜¯å¦åœ¨æˆªæ–­èŒƒå›´å†…è®¾ç½®é¢œè‰²
        colors = []
        for i in range(30):
            if i < effective_cutoff:
                colors.append('#2563EB')  # è“è‰² - åœ¨é‡‡æ ·èŒƒå›´å†…
            elif i < top_k:
                colors.append('#D97706')  # æ©™è‰² - åœ¨ Top-K ä½†ä¸åœ¨ Top-P
            else:
                colors.append('#E5E7EB')  # ç°è‰² - è¢«æˆªæ–­
        
        fig.add_trace(go.Bar(
            x=tokens_display,
            y=probs_list,
            marker_color=colors,
            text=[f'{p:.1%}' for p in probs_list],
            textposition='outside',
            textfont=dict(size=9)
        ))
        
        # Top-K çº¿
        if top_k <= 30:
            fig.add_vline(x=top_k - 0.5, line_dash="dash", line_color="#D97706",
                         annotation_text=f"Top-K={top_k}", annotation_position="top")
        
        # Top-P çº¿
        if top_p_cutoff <= 30:
            fig.add_vline(x=top_p_cutoff - 0.5, line_dash="dot", line_color="#DC2626",
                         annotation_text=f"Top-P={top_p}", annotation_position="bottom")
        
        fig.update_layout(
            title="é‡‡æ ·æˆªæ–­å¯è§†åŒ–",
            xaxis_title="Token",
            yaxis_title="æ¦‚ç‡",
            yaxis_tickformat='.1%',
            height=450,
            margin=dict(l=50, r=50, t=60, b=100)
        )
        fig.update_xaxes(tickangle=45)
        
        st.plotly_chart(fig, width='stretch')
        
        # å›¾ä¾‹è¯´æ˜
        st.markdown("""
        <div style="display: flex; gap: 20px; margin-top: 10px;">
            <span><span style="color: #2563EB;">â– </span> æœ‰æ•ˆé‡‡æ ·èŒƒå›´</span>
            <span><span style="color: #D97706;">â– </span> Top-K å†…ä½†è¶…å‡º Top-P</span>
            <span><span style="color: #E5E7EB;">â– </span> è¢«æˆªæ–­ (ä¸å‚ä¸é‡‡æ ·)</span>
        </div>
        """, unsafe_allow_html=True)
        
        # ç´¯ç§¯æ¦‚ç‡æ›²çº¿
        st.markdown("### ç´¯ç§¯æ¦‚ç‡æ›²çº¿ (CDF)")
        
        fig_cdf = go.Figure()
        fig_cdf.add_trace(go.Scatter(
            x=list(range(1, 51)),
            y=cumsum_probs.tolist(),
            mode='lines+markers',
            name='ç´¯ç§¯æ¦‚ç‡',
            marker=dict(size=6),
            line=dict(color='#2563EB')
        ))
        
        fig_cdf.add_hline(y=top_p, line_dash="dash", line_color="#DC2626",
                        annotation_text=f"Top-P={top_p}")
        
        fig_cdf.update_layout(
            title="Top-50 Token ç´¯ç§¯æ¦‚ç‡åˆ†å¸ƒ",
            xaxis_title="Token æ•°é‡",
            yaxis_title="ç´¯ç§¯æ¦‚ç‡",
            yaxis_tickformat='.0%',
            height=350
        )
        
        st.plotly_chart(fig_cdf, width='stretch')


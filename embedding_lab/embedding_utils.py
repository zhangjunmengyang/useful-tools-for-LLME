"""
EmbeddingLab - Embedding å·¥å…·å‡½æ•°
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Any
import streamlit as st

# Word2Vec é¢„è®­ç»ƒæ¨¡å‹çš„è¯æ±‡è¡¨ï¼ˆç²¾ç®€ç‰ˆï¼Œç”¨äºæ¼”ç¤ºï¼‰
# å®é™…ä½¿ç”¨æ—¶ä¼šä¸‹è½½å®Œæ•´æ¨¡å‹
DEMO_WORD_VECTORS = None


@st.cache_resource
def load_word2vec_model():
    """åŠ è½½ Word2Vec æ¨¡å‹ï¼ˆä½¿ç”¨ gensimï¼‰"""
    try:
        import gensim.downloader as api
        model = api.load("glove-wiki-gigaword-100")  # 100ç»´çš„GloVeæ¨¡å‹ï¼Œè¾ƒå°
        return model
    except Exception as e:
        st.error(f"åŠ è½½ Word2Vec æ¨¡å‹å¤±è´¥: {e}")
        return None


@st.cache_resource
def load_sentence_transformer(model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
    """åŠ è½½ Sentence Transformer æ¨¡å‹"""
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(model_name)
        return model
    except Exception as e:
        st.error(f"åŠ è½½ Sentence Transformer å¤±è´¥: {e}")
        return None


def get_word_vector(model, word: str) -> Optional[np.ndarray]:
    """è·å–å•è¯çš„å‘é‡è¡¨ç¤º"""
    try:
        return model[word]
    except KeyError:
        return None


def vector_arithmetic(model, positive: List[str], negative: List[str], topn: int = 10) -> List[Tuple[str, float]]:
    """
    æ‰§è¡Œå‘é‡è¿ç®—ï¼špositive - negative
    ä¾‹å¦‚ï¼šKing - Man + Woman = ? (positive=['king', 'woman'], negative=['man'])
    """
    try:
        result = model.most_similar(positive=positive, negative=negative, topn=topn)
        return result
    except Exception as e:
        return []


def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return float(np.dot(vec1, vec2) / (norm1 * norm2))


def compute_similarity_matrix(vectors1: List[np.ndarray], vectors2: List[np.ndarray]) -> np.ndarray:
    """è®¡ç®—ä¸¤ç»„å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ"""
    matrix = np.zeros((len(vectors1), len(vectors2)))
    for i, v1 in enumerate(vectors1):
        for j, v2 in enumerate(vectors2):
            matrix[i, j] = cosine_similarity(v1, v2)
    return matrix


class DimensionReductionError(Exception):
    """é™ç»´ç®—æ³•çš„è‡ªå®šä¹‰é”™è¯¯ï¼ŒåŒ…å«ç”¨æˆ·å‹å¥½çš„æç¤ºä¿¡æ¯"""
    pass


# å„ç®—æ³•çš„æœ€å°æ ·æœ¬æ•°è¦æ±‚
MIN_SAMPLES = {
    "pca": 2,
    "tsne": 6,   # t-SNE å»ºè®® perplexity >= 5ï¼Œæ‰€ä»¥éœ€è¦è‡³å°‘ 6 ä¸ªæ ·æœ¬
    "umap": 15,  # UMAP é»˜è®¤ n_neighbors=15ï¼Œéœ€è¦è‡³å°‘ 15 ä¸ªæ ·æœ¬æ‰èƒ½ç¨³å®šå·¥ä½œ
}


def reduce_dimensions(vectors: np.ndarray, method: str = "pca", n_components: int = 2) -> np.ndarray:
    """
    é™ç»´å¤„ç†
    
    Args:
        vectors: è¾“å…¥å‘é‡çŸ©é˜µ
        method: é™ç»´æ–¹æ³• (pca, tsne, umap)
        n_components: ç›®æ ‡ç»´åº¦
        
    Returns:
        é™ç»´åçš„åæ ‡
        
    Raises:
        DimensionReductionError: å½“æ ·æœ¬æ•°ä¸æ»¡è¶³ç®—æ³•è¦æ±‚æ—¶
    """
    n_samples = len(vectors)
    
    if n_samples == 0:
        raise DimensionReductionError("æ²¡æœ‰æ•°æ®ç‚¹å¯ä»¥å¯è§†åŒ–")
    
    min_required = MIN_SAMPLES.get(method, 2)
    
    if method == "pca":
        if n_samples < 2:
            raise DimensionReductionError(
                f"PCA è‡³å°‘éœ€è¦ 2 ä¸ªæ•°æ®ç‚¹ï¼Œå½“å‰åªæœ‰ {n_samples} ä¸ª"
            )
        from sklearn.decomposition import PCA
        n_components = min(n_components, n_samples, vectors.shape[1] if len(vectors.shape) > 1 else 1)
        reducer = PCA(n_components=n_components)
        
    elif method == "tsne":
        if n_samples < 6:
            raise DimensionReductionError(
                f"t-SNE è‡³å°‘éœ€è¦ 6 ä¸ªæ•°æ®ç‚¹ï¼ˆperplexity å‚æ•°è¦æ±‚ï¼‰ï¼Œå½“å‰åªæœ‰ {n_samples} ä¸ªã€‚\n"
                f"ğŸ’¡ å»ºè®®ï¼šæ·»åŠ æ›´å¤šæ•°æ®ç‚¹ï¼Œæˆ–ä½¿ç”¨ PCA ç®—æ³•"
            )
        from sklearn.manifold import TSNE
        # perplexity å¿…é¡» < n_samples
        perplexity = min(30, n_samples - 1)
        reducer = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)
        
    elif method == "umap":
        if n_samples < 15:
            raise DimensionReductionError(
                f"UMAP è‡³å°‘éœ€è¦ 15 ä¸ªæ•°æ®ç‚¹ï¼ˆn_neighbors å‚æ•°è¦æ±‚ï¼‰ï¼Œå½“å‰åªæœ‰ {n_samples} ä¸ªã€‚\n"
                f"ğŸ’¡ å»ºè®®ï¼šæ·»åŠ æ›´å¤šæ•°æ®ç‚¹ï¼Œæˆ–ä½¿ç”¨ PCA/t-SNE ç®—æ³•"
            )
        import umap
        n_neighbors = min(15, n_samples - 1)
        reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, random_state=42)
        
    else:
        raise ValueError(f"æœªçŸ¥çš„é™ç»´æ–¹æ³•: {method}")
    
    return reducer.fit_transform(vectors)


# ================================
# Embedding æ¨¡å‹é…ç½®
# ================================

EMBEDDING_MODELS = {
    "Dense Models": {
        "paraphrase-multilingual-MiniLM-L12-v2": {
            "name": "Multilingual MiniLM",
            "type": "dense",
            "dim": 384,
            "description": "å¤šè¯­è¨€è½»é‡çº§æ¨¡å‹"
        },
        "all-MiniLM-L6-v2": {
            "name": "MiniLM-L6",
            "type": "dense",
            "dim": 384,
            "description": "è‹±æ–‡è½»é‡çº§æ¨¡å‹"
        },
        "BAAI/bge-small-zh-v1.5": {
            "name": "BGE-Small-ZH",
            "type": "dense",
            "dim": 512,
            "description": "ä¸­æ–‡å°æ¨¡å‹"
        },
    },
    "Classical Models": {
        "tfidf": {
            "name": "TF-IDF",
            "type": "sparse",
            "dim": "variable",
            "description": "è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡"
        },
        "bm25": {
            "name": "BM25",
            "type": "sparse",
            "dim": "variable",
            "description": "æ¦‚ç‡æ£€ç´¢æ¨¡å‹"
        },
    }
}


def get_text_embedding(text: str, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2") -> Optional[np.ndarray]:
    """è·å–æ–‡æœ¬çš„ embedding"""
    if model_name in ["tfidf", "bm25"]:
        return None  # è¿™äº›éœ€è¦æ•´ä¸ªè¯­æ–™åº“
    
    model = load_sentence_transformer(model_name)
    if model is None:
        return None
    
    embedding = model.encode(text, convert_to_numpy=True)
    return embedding


def get_batch_embeddings(texts: List[str], model_name: str = "paraphrase-multilingual-MiniLM-L12-v2") -> Optional[np.ndarray]:
    """æ‰¹é‡è·å–æ–‡æœ¬çš„ embeddings"""
    if model_name in ["tfidf", "bm25"]:
        return compute_sparse_embeddings(texts, model_name)
    
    model = load_sentence_transformer(model_name)
    if model is None:
        return None
    
    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=False)
    return embeddings


def chinese_tokenizer(text: str) -> List[str]:
    """ä¸­æ–‡åˆ†è¯å™¨ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆ"""
    import jieba
    import re
    
    # ä½¿ç”¨ jieba åˆ†è¯
    tokens = jieba.lcut(text)
    
    # è¿‡æ»¤æ‰ç©ºç™½å’Œæ ‡ç‚¹ç¬¦å·
    tokens = [t.strip() for t in tokens if t.strip() and not re.match(r'^[\s\W]+$', t)]
    
    return tokens


def compute_sparse_embeddings(texts: List[str], method: str = "tfidf") -> np.ndarray:
    """è®¡ç®—ç¨€ç–å‘é‡ï¼ˆTF-IDF æˆ– BM25ï¼‰ï¼Œæ”¯æŒä¸­æ–‡åˆ†è¯"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡
    import re
    has_chinese = any(re.search(r'[\u4e00-\u9fff]', text) for text in texts)
    
    # å¦‚æœåŒ…å«ä¸­æ–‡ï¼Œä½¿ç”¨ jieba åˆ†è¯
    tokenizer = chinese_tokenizer if has_chinese else None
    
    if method == "tfidf":
        vectorizer = TfidfVectorizer(tokenizer=tokenizer, token_pattern=None if tokenizer else r'(?u)\b\w+\b')
        vectors = vectorizer.fit_transform(texts).toarray()
        return vectors
    elif method == "bm25":
        # ç®€åŒ–çš„ BM25 å®ç°ï¼Œä½¿ç”¨ TF-IDF çš„å˜ä½“
        vectorizer = TfidfVectorizer(
            tokenizer=tokenizer, 
            token_pattern=None if tokenizer else r'(?u)\b\w+\b',
            use_idf=True, 
            norm='l2', 
            sublinear_tf=True
        )
        vectors = vectorizer.fit_transform(texts).toarray()
        return vectors
    
    return np.array([])


def compute_anisotropy(vectors: np.ndarray, sample_size: int = 100) -> Tuple[float, float]:
    """
    è®¡ç®—å‘é‡ç©ºé—´çš„å„å‘å¼‚æ€§
    è¿”å›ï¼š(å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦, æ ‡å‡†å·®)
    """
    n = len(vectors)
    if n < 2:
        return 0.0, 0.0
    
    # éšæœºé‡‡æ ·
    if n > sample_size:
        indices = np.random.choice(n, sample_size, replace=False)
        sampled = vectors[indices]
    else:
        sampled = vectors
    
    similarities = []
    for i in range(len(sampled)):
        for j in range(i + 1, len(sampled)):
            sim = cosine_similarity(sampled[i], sampled[j])
            similarities.append(sim)
    
    if not similarities:
        return 0.0, 0.0
    
    return float(np.mean(similarities)), float(np.std(similarities))


def whitening_transform(vectors: np.ndarray) -> np.ndarray:
    """
    å¯¹å‘é‡è¿›è¡Œç™½åŒ–å¤„ç†ï¼Œç¼“è§£å„å‘å¼‚æ€§é—®é¢˜
    """
    # ä¸­å¿ƒåŒ–
    mean = np.mean(vectors, axis=0)
    centered = vectors - mean
    
    # è®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov = np.cov(centered.T)
    
    # ç‰¹å¾å€¼åˆ†è§£
    eigenvalues, eigenvectors = np.linalg.eigh(cov)
    
    # é˜²æ­¢é™¤é›¶
    eigenvalues = np.maximum(eigenvalues, 1e-10)
    
    # ç™½åŒ–å˜æ¢
    D_inv_sqrt = np.diag(1.0 / np.sqrt(eigenvalues))
    whitening_matrix = eigenvectors @ D_inv_sqrt @ eigenvectors.T
    
    whitened = centered @ whitening_matrix
    return whitened


# ================================
# é¢„ç½®æ•°æ®é›†
# ================================

PRESET_DATASETS = {
    "news_categories": {
        "name": "æ–°é—»åˆ†ç±»",
        "description": "ä¸åŒç±»åˆ«çš„æ–°é—»æ ‡é¢˜",
        "data": [
            {"text": "è‚¡å¸‚ä»Šæ—¥å¤§æ¶¨ï¼Œä¸Šè¯æŒ‡æ•°çªç ´3500ç‚¹", "label": "è´¢ç»"},
            {"text": "å¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹", "label": "è´¢ç»"},
            {"text": "æ¯”ç‰¹å¸ä»·æ ¼åˆ›å†å²æ–°é«˜", "label": "è´¢ç»"},
            {"text": "æ–°èƒ½æºæ±½è½¦é”€é‡åŒæ¯”å¢é•¿50%", "label": "è´¢ç»"},
            {"text": "å›½è¶³ä¸–é¢„èµ›2-0æˆ˜èƒœå¯¹æ‰‹", "label": "ä½“è‚²"},
            {"text": "NBAå­£åèµ›ç«çƒ­è¿›è¡Œä¸­", "label": "ä½“è‚²"},
            {"text": "å¥¥è¿ä¼šå¼€å¹•å¼ç²¾å½©çº·å‘ˆ", "label": "ä½“è‚²"},
            {"text": "ç½‘çƒå…¬å¼€èµ›å†³èµ›ä»Šæ™šä¸Šæ¼”", "label": "ä½“è‚²"},
            {"text": "æ–°æ¬¾iPhoneæ­£å¼å‘å¸ƒ", "label": "ç§‘æŠ€"},
            {"text": "äººå·¥æ™ºèƒ½çªç ´è¯­è¨€ç†è§£éšœç¢", "label": "ç§‘æŠ€"},
            {"text": "é‡å­è®¡ç®—æœºå–å¾—é‡å¤§è¿›å±•", "label": "ç§‘æŠ€"},
            {"text": "SpaceXæˆåŠŸå‘å°„æ˜Ÿé“¾å«æ˜Ÿ", "label": "ç§‘æŠ€"},
            {"text": "çŸ¥åæ¼”å‘˜æ–°ç”µå½±ä¸Šæ˜ ", "label": "å¨±ä¹"},
            {"text": "çƒ­é—¨ç»¼è‰ºèŠ‚ç›®æ”¶è§†ç‡åˆ›æ–°é«˜", "label": "å¨±ä¹"},
            {"text": "æµè¡Œæ­Œæ‰‹å‘å¸ƒæ–°ä¸“è¾‘", "label": "å¨±ä¹"},
            {"text": "ç”µå½±èŠ‚é¢å¥–å…¸ç¤¼ä¸¾è¡Œ", "label": "å¨±ä¹"},
        ]
    },
    "sentiment": {
        "name": "æƒ…æ„Ÿåˆ†æ",
        "description": "æ­£é¢/è´Ÿé¢æƒ…æ„Ÿæ–‡æœ¬",
        "data": [
            {"text": "è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œå¼ºçƒˆæ¨èï¼", "label": "æ­£é¢"},
            {"text": "æœåŠ¡éå¸¸å‘¨åˆ°ï¼Œå¾ˆæ»¡æ„", "label": "æ­£é¢"},
            {"text": "è´¨é‡å¾ˆå¥½ï¼Œç‰©è¶…æ‰€å€¼", "label": "æ­£é¢"},
            {"text": "ä½“éªŒå¾ˆæ£’ï¼Œä¸‹æ¬¡è¿˜ä¼šæ¥", "label": "æ­£é¢"},
            {"text": "å¤ªå¤±æœ›äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·", "label": "è´Ÿé¢"},
            {"text": "æœåŠ¡æ€åº¦å¤ªå·®ï¼Œä¸ä¼šå†æ¥", "label": "è´Ÿé¢"},
            {"text": "è´¨é‡å¾ˆå·®ï¼Œæµªè´¹é’±", "label": "è´Ÿé¢"},
            {"text": "ç­‰äº†å¾ˆä¹…ï¼Œä½“éªŒå¾ˆç³Ÿç³•", "label": "è´Ÿé¢"},
            {"text": "è¿˜è¡Œå§ï¼Œä¸€èˆ¬èˆ¬", "label": "ä¸­æ€§"},
            {"text": "æ²¡ä»€ä¹ˆç‰¹åˆ«çš„æ„Ÿè§‰", "label": "ä¸­æ€§"},
            {"text": "é©¬é©¬è™è™ï¼Œå‡‘åˆç”¨", "label": "ä¸­æ€§"},
            {"text": "æ™®æ™®é€šé€šï¼Œä¸å¥½ä¸å", "label": "ä¸­æ€§"},
        ]
    },
    "multilingual": {
        "name": "ä¸­è‹±æ··åˆ",
        "description": "ä¸­è‹±æ–‡è¯­ä¹‰å¯¹åº”",
        "data": [
            {"text": "æˆ‘çˆ±ä½ ", "label": "ä¸­æ–‡"},
            {"text": "I love you", "label": "è‹±æ–‡"},
            {"text": "ä»Šå¤©å¤©æ°”å¾ˆå¥½", "label": "ä¸­æ–‡"},
            {"text": "The weather is nice today", "label": "è‹±æ–‡"},
            {"text": "äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ", "label": "ä¸­æ–‡"},
            {"text": "Artificial intelligence changes the world", "label": "è‹±æ–‡"},
            {"text": "å­¦ä¹ æ˜¯ç»ˆèº«çš„äº‹ä¸š", "label": "ä¸­æ–‡"},
            {"text": "Learning is a lifelong journey", "label": "è‹±æ–‡"},
            {"text": "æ—¶é—´å°±æ˜¯é‡‘é’±", "label": "ä¸­æ–‡"},
            {"text": "Time is money", "label": "è‹±æ–‡"},
            {"text": "çŸ¥è¯†å°±æ˜¯åŠ›é‡", "label": "ä¸­æ–‡"},
            {"text": "Knowledge is power", "label": "è‹±æ–‡"},
        ]
    }
}


# ================================
# é¢œè‰²é…ç½®
# ================================

LABEL_COLORS = {
    # æ–°é—»åˆ†ç±»
    "è´¢ç»": "#2563EB",
    "ä½“è‚²": "#059669",
    "ç§‘æŠ€": "#7C3AED",
    "å¨±ä¹": "#DC2626",
    # æƒ…æ„Ÿåˆ†æ
    "æ­£é¢": "#059669",
    "è´Ÿé¢": "#DC2626",
    "ä¸­æ€§": "#6B7280",
    # å¤šè¯­è¨€
    "ä¸­æ–‡": "#2563EB",
    "è‹±æ–‡": "#D97706",
}


def get_label_color(label: str) -> str:
    """è·å–æ ‡ç­¾å¯¹åº”çš„é¢œè‰²"""
    return LABEL_COLORS.get(label, "#6B7280")


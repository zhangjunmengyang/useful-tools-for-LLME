"""
Lab 2: æ¨¡å‹å¯¹æ¯” - å¯¹æ¯”ä¸åŒ Embedding æ¨¡å‹çš„ç‰¹æ€§
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from typing import List, Dict, Tuple

from embedding_lab.embedding_utils import (
    get_batch_embeddings,
    compute_sparse_embeddings,
    cosine_similarity,
    load_sentence_transformer,
    EMBEDDING_MODELS
)


def compute_similarity_scores(query_embedding: np.ndarray, candidate_embeddings: np.ndarray) -> List[float]:
    """è®¡ç®— query ä¸æ‰€æœ‰ candidates çš„ç›¸ä¼¼åº¦"""
    scores = []
    for emb in candidate_embeddings:
        scores.append(cosine_similarity(query_embedding, emb))
    return scores


def create_comparison_chart(
    candidates: List[str],
    scores_dict: Dict[str, List[float]],
    query: str
) -> go.Figure:
    """åˆ›å»ºæ¨¡å‹å¯¹æ¯”æŸ±çŠ¶å›¾"""
    fig = go.Figure()
    
    colors = ['#2563EB', '#059669', '#D97706', '#7C3AED', '#DC2626']
    
    for i, (model_name, scores) in enumerate(scores_dict.items()):
        fig.add_trace(go.Bar(
            name=model_name,
            x=candidates,
            y=scores,
            marker_color=colors[i % len(colors)],
            text=[f'{s:.3f}' for s in scores],
            textposition='outside',
            textfont=dict(size=11, color='#111827')
        ))
    
    fig.update_layout(
        title=dict(
            text=f'Query: "{query}"',
            font=dict(size=14, color='#6B7280')
        ),
        barmode='group',
        plot_bgcolor='#FFFFFF',
        paper_bgcolor='#FFFFFF',
        font=dict(color='#111827', family='Inter, sans-serif'),
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        margin=dict(l=40, r=40, t=80, b=80),
        height=400,
        xaxis=dict(
            tickangle=-20,
            gridcolor='#E5E7EB'
        ),
        yaxis=dict(
            title='ç›¸ä¼¼åº¦',
            range=[0, 1],
            gridcolor='#E5E7EB'
        )
    )
    
    return fig


def render_score_table(candidates: List[str], scores_dict: Dict[str, List[float]]):
    """æ¸²æŸ“è¯¦ç»†åˆ†æ•°è¡¨æ ¼"""
    data = {'å€™é€‰æ–‡æœ¬': candidates}
    for model_name, scores in scores_dict.items():
        data[model_name] = [f'{s:.4f}' for s in scores]
    
    df = pd.DataFrame(data)
    
    # é«˜äº®æœ€é«˜åˆ†
    def highlight_max(s):
        if s.name == 'å€™é€‰æ–‡æœ¬':
            return [''] * len(s)
        numeric_vals = [float(v) for v in s]
        is_max = [v == max(numeric_vals) for v in numeric_vals]
        return ['background-color: #D1FAE5' if v else '' for v in is_max]
    
    styled_df = df.style.apply(highlight_max)
    st.dataframe(styled_df, width="stretch", hide_index=True)


def render_token_attention_hint():
    """æ¸²æŸ“ Token çº§åˆ«çš„æ³¨æ„åŠ›æç¤º"""
    st.markdown("""
    <div style="background: #DBEAFE; border: 1px solid #93C5FD; border-radius: 6px; 
                padding: 12px; margin-top: 16px;">
        <p style="color: #1E40AF; margin: 0; font-size: 13px;">
            <strong>ğŸ” æ·±å…¥ç†è§£</strong><br/>
            Dense æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ä½¿ç”¨ Attention æœºåˆ¶æ•æ‰ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚<br/>
            Sparse æ¨¡å‹ï¼ˆå¦‚ TF-IDFï¼‰ä»…åŸºäºè¯é¢‘ç»Ÿè®¡ï¼Œæ— æ³•ç†è§£åŒä¹‰è¯å’Œä¸Šä¸‹æ–‡ã€‚
        </p>
    </div>
    """, unsafe_allow_html=True)


def render():
    """æ¸²æŸ“ Lab 2: æ¨¡å‹å¯¹æ¯” é¡µé¢"""
    st.markdown('<h1 class="module-title">æ¨¡å‹å¯¹æ¯”</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div style="background: linear-gradient(135deg, #D1FAE5 0%, #DBEAFE 100%); 
                border-radius: 8px; padding: 16px; margin-bottom: 24px; border: 1px solid #A7F3D0;">
        <p style="color: #065F46; margin: 0; font-size: 14px;">
            <strong>ğŸ”¬ å®éªŒç›®æ ‡</strong>ï¼šå¯¹æ¯”è¯æ³•åŒ¹é…ï¼ˆTF-IDF/BM25ï¼‰ä¸è¯­ä¹‰åŒ¹é…ï¼ˆDense Embeddingï¼‰çš„å·®å¼‚ã€‚<br/>
            è§‚å¯Ÿä¸åŒæ¨¡å‹å¯¹åŒä¸€ Query çš„ç›¸ä¼¼åº¦æ’åºæœ‰ä½•ä¸åŒã€‚
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # æ¨¡å‹é€‰æ‹©
    st.markdown("### é€‰æ‹©å¯¹æ¯”æ¨¡å‹")
    
    available_models = {
        "TF-IDF": "tfidf",
        "BM25": "bm25",
        "Multilingual MiniLM": "paraphrase-multilingual-MiniLM-L12-v2",
        "MiniLM-L6 (è‹±æ–‡)": "all-MiniLM-L6-v2",
    }
    
    model_cols = st.columns(len(available_models))
    selected_models = []
    
    for i, (name, model_id) in enumerate(available_models.items()):
        with model_cols[i]:
            if st.checkbox(name, value=(i < 2), key=f"model_{model_id}"):
                selected_models.append((name, model_id))
    
    if len(selected_models) < 1:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
        return
    
    st.markdown("---")
    
    # è¾“å…¥åŒºåŸŸ
    st.markdown("### è¾“å…¥æµ‹è¯•æ•°æ®")
    
    col_query, col_candidates = st.columns([1, 2])
    
    with col_query:
        st.markdown("#### Query")
        query = st.text_input("æŸ¥è¯¢æ–‡æœ¬", value="è‹¹æœ", key="comparison_query",
                             help="è¾“å…¥è¦æœç´¢çš„å…³é”®è¯æˆ–å¥å­")
    
    with col_candidates:
        st.markdown("#### Candidates")
        default_candidates = "æ°´æœ\næ‰‹æœº\nä¹”å¸ƒæ–¯\nçº¢è‰²çš„çƒ\nè‹¹æœå‘å¸ƒæ–°äº§å“\næˆ‘å–œæ¬¢åƒè‹¹æœ"
        candidates_text = st.text_area(
            "å€™é€‰æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
            value=default_candidates,
            height=150,
            key="comparison_candidates",
            help="è¾“å…¥å¤šä¸ªå€™é€‰æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ª"
        )
    
    # é¢„è®¾æ¡ˆä¾‹
    st.markdown("#### é¢„è®¾æ¡ˆä¾‹")
    presets = [
        ("ğŸ è‹¹æœæ­§ä¹‰", "è‹¹æœ", "æ°´æœ\næ‰‹æœº\nä¹”å¸ƒæ–¯\nçº¢è‰²çš„çƒ\nè‹¹æœå‘å¸ƒæ–°äº§å“\næˆ‘å–œæ¬¢åƒè‹¹æœ"),
        ("ğŸ¦ é“¶è¡Œæ­§ä¹‰", "é“¶è¡Œ", "é‡‘èæœºæ„\næ²³è¾¹\nå­˜æ¬¾å–æ¬¾\né“¶è¡Œå¡\næ²³å²¸é£æ™¯"),
        ("ğŸš— ç‰¹æ–¯æ‹‰", "ç‰¹æ–¯æ‹‰", "ç”µåŠ¨æ±½è½¦\nç§‘å­¦å®¶\né©¬æ–¯å…‹\nç”µç£æ„Ÿåº”\nModel 3"),
        ("ğŸ” è¯­ä¹‰æœç´¢", "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹", "ç¼–ç¨‹å…¥é—¨æ•™ç¨‹\nå­¦ä¹ Python\nä»£ç æ€ä¹ˆå†™\nç¨‹åºå‘˜æˆé•¿\nè½¯ä»¶å¼€å‘"),
    ]
    
    def set_comparison_preset(q: str, c: str):
        """å›è°ƒå‡½æ•°ï¼šè®¾ç½®é¢„è®¾å€¼"""
        st.session_state.comparison_query = q
        st.session_state.comparison_candidates = c
    
    preset_cols = st.columns(len(presets))
    for i, (label, q, c) in enumerate(presets):
        with preset_cols[i]:
            st.button(
                label, 
                key=f"preset_compare_{i}", 
                width="stretch",
                on_click=set_comparison_preset,
                args=(q, c)
            )
    
    st.markdown("---")
    
    # æ‰§è¡Œå¯¹æ¯”
    if query and candidates_text:
        candidates = [c.strip() for c in candidates_text.strip().split('\n') if c.strip()]
        
        if not candidates:
            st.warning("è¯·è¾“å…¥å€™é€‰æ–‡æœ¬")
            return
        
        all_texts = [query] + candidates
        scores_dict = {}
        
        with st.spinner("è®¡ç®— Embeddings..."):
            for model_name, model_id in selected_models:
                try:
                    if model_id in ["tfidf", "bm25"]:
                        # ç¨€ç–å‘é‡
                        embeddings = compute_sparse_embeddings(all_texts, model_id)
                        query_emb = embeddings[0]
                        candidate_embs = embeddings[1:]
                    else:
                        # Dense å‘é‡
                        embeddings = get_batch_embeddings(all_texts, model_id)
                        if embeddings is None:
                            st.warning(f"æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥")
                            continue
                        query_emb = embeddings[0]
                        candidate_embs = embeddings[1:]
                    
                    scores = compute_similarity_scores(query_emb, candidate_embs)
                    scores_dict[model_name] = scores
                    
                except Exception as e:
                    st.warning(f"æ¨¡å‹ {model_name} è®¡ç®—å¤±è´¥: {e}")
        
        if not scores_dict:
            st.error("æ‰€æœ‰æ¨¡å‹è®¡ç®—å¤±è´¥")
            return
        
        # æ˜¾ç¤ºç»“æœ
        st.markdown("### å¯¹æ¯”ç»“æœ")
        
        # æŸ±çŠ¶å›¾
        fig = create_comparison_chart(candidates, scores_dict, query)
        st.plotly_chart(fig, width="stretch")
        
        # è¯¦ç»†è¡¨æ ¼
        with st.expander("è¯¦ç»†åˆ†æ•°", expanded=True):
            render_score_table(candidates, scores_dict)
        
        # æ’åºå¯¹æ¯”
        st.markdown("### æ’åºå·®å¼‚åˆ†æ")
        
        rank_cols = st.columns(len(scores_dict))
        for i, (model_name, scores) in enumerate(scores_dict.items()):
            with rank_cols[i]:
                st.markdown(f"**{model_name}** æ’åº")
                
                # æŒ‰åˆ†æ•°æ’åº
                sorted_indices = np.argsort(scores)[::-1]
                
                # ä½¿ç”¨å®¹å™¨æ˜¾ç¤ºæ’åºç»“æœ
                with st.container():
                    for rank, idx in enumerate(sorted_indices, 1):
                        score = scores[idx]
                        text = candidates[idx][:20] + ('...' if len(candidates[idx]) > 20 else '')
                        
                        # é«˜äº®å‰ä¸‰å
                        if rank == 1:
                            badge = 'ğŸ¥‡'
                            st.success(f"{badge} {text} â€” {score:.3f}")
                        elif rank == 2:
                            badge = 'ğŸ¥ˆ'
                            st.info(f"{badge} {text} â€” {score:.3f}")
                        elif rank == 3:
                            badge = 'ğŸ¥‰'
                            st.warning(f"{badge} {text} â€” {score:.3f}")
                        else:
                            st.text(f"{rank}. {text} â€” {score:.3f}")
        
        # æ´å¯Ÿåˆ†æ
        st.markdown("### ğŸ’¡ æ´å¯Ÿ")
        
        # æ‰¾å‡ºå·®å¼‚æœ€å¤§çš„æ¡ˆä¾‹
        if len(scores_dict) >= 2:
            model_names = list(scores_dict.keys())
            scores_1 = scores_dict[model_names[0]]
            scores_2 = scores_dict[model_names[1]]
            
            rank_diff = []
            for i, candidate in enumerate(candidates):
                rank_1 = sorted(range(len(scores_1)), key=lambda k: scores_1[k], reverse=True).index(i) + 1
                rank_2 = sorted(range(len(scores_2)), key=lambda k: scores_2[k], reverse=True).index(i) + 1
                rank_diff.append((candidate, abs(rank_1 - rank_2), rank_1, rank_2))
            
            rank_diff.sort(key=lambda x: x[1], reverse=True)
            
            if rank_diff[0][1] > 0:
                most_diff = rank_diff[0]
                st.warning(f"""
**æœ€å¤§æ’åºå·®å¼‚**

æ–‡æœ¬ ã€Œ{most_diff[0][:30]}...ã€ åœ¨ **{model_names[0]}** ä¸­æ’ç¬¬ **{most_diff[2]}** åï¼Œåœ¨ **{model_names[1]}** ä¸­æ’ç¬¬ **{most_diff[3]}** åã€‚

_è¿™è¯´æ˜ä¸åŒæ¨¡å‹å¯¹è¯­ä¹‰çš„ç†è§£å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚_
""")
        
        render_token_attention_hint()
        
        # æ¨¡å‹ç‰¹ç‚¹è¯´æ˜
        with st.expander("ğŸ“š æ¨¡å‹ç‰¹ç‚¹è¯´æ˜"):
            st.markdown("""
            | æ¨¡å‹ç±»å‹ | ä»£è¡¨ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
            |---------|------|------|---------|
            | **ç¨€ç–å‘é‡** | TF-IDF, BM25 | åŸºäºè¯é¢‘ç»Ÿè®¡ï¼Œå¯è§£é‡Šæ€§å¼º | å…³é”®è¯æœç´¢ã€ç²¾ç¡®åŒ¹é… |
            | **é™æ€å‘é‡** | Word2Vec, GloVe | æ¯ä¸ªè¯ä¸€ä¸ªå›ºå®šå‘é‡ | è¯ç›¸ä¼¼åº¦ã€ç®€å•è¯­ä¹‰ |
            | **ä¸Šä¸‹æ–‡å‘é‡** | BERT, BGE | è€ƒè™‘ä¸Šä¸‹æ–‡çš„åŠ¨æ€å‘é‡ | è¯­ä¹‰æœç´¢ã€é—®ç­”åŒ¹é… |
            
            **å…³é”®å·®å¼‚**ï¼š
            - TF-IDF/BM25 åªçœ‹ã€Œæ˜¯å¦åŒ…å«ç›¸åŒçš„è¯ã€
            - Dense æ¨¡å‹èƒ½ç†è§£ã€Œè‹¹æœã€åœ¨ä¸åŒè¯­å¢ƒä¸‹æŒ‡æ°´æœè¿˜æ˜¯å…¬å¸
            """)


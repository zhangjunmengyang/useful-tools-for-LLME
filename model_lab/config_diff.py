"""
Config å·®å¼‚å¯¹æ¯” - å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ config.json
æ”¯æŒä» HuggingFace Hub å®æ—¶è¯»å–é…ç½®
"""

import streamlit as st
import json
from huggingface_hub import hf_hub_download
from model_lab.model_utils import extract_from_url


# æŒ‰å‚å•†/ç³»åˆ—åˆ†ç±»çš„é¢„è®¾æ¨¡å‹
MODEL_CATEGORIES = {
    "Meta (Llama)": {
        "models": [
            ("Llama-2-7B", "meta-llama/Llama-2-7b-hf"),
            ("Llama-3.1-8B", "meta-llama/Llama-3.1-8B"),
            ("Llama-3.2-3B", "meta-llama/Llama-3.2-3B"),
            ("Llama-4-Scout-17B", "meta-llama/Llama-4-Scout-17B-16E-Instruct"),
        ],
    },
    "Alibaba (Qwen)": {
        "models": [
            ("Qwen2.5-7B", "Qwen/Qwen2.5-7B"),
            ("Qwen2.5-3B", "Qwen/Qwen2.5-3B"),
            ("Qwen3-8B", "Qwen/Qwen3-8B"),
        ],
    },
    "DeepSeek": {
        "models": [
            ("DeepSeek-V3", "deepseek-ai/DeepSeek-V3"),
            ("DeepSeek-R1", "deepseek-ai/DeepSeek-R1"),
        ],
    },
    "Mistral": {
        "models": [
            ("Mistral-7B-v0.1", "mistralai/Mistral-7B-v0.1"),
            ("Mistral-7B-v0.3", "mistralai/Mistral-7B-v0.3"),
        ],
    },
    "Google": {
        "models": [
            ("Gemma-2-2B", "google/gemma-2-2b"),
            ("Gemma-7B", "google/gemma-7b"),
        ],
    },
    "Microsoft": {
        "models": [
            ("Phi-3-mini-4k", "microsoft/Phi-3-mini-4k-instruct"),
            ("Phi-4", "microsoft/phi-4"),
        ],
    },
}

# ç”Ÿæˆæ‰å¹³çš„é¢„è®¾åˆ—è¡¨ï¼ˆç”¨äºå…¼å®¹ï¼‰
def _build_preset_list():
    """æ„å»º (display_name, model_id) çš„æ‰å¹³åˆ—è¡¨"""
    result = []
    for category_data in MODEL_CATEGORIES.values():
        for item in category_data["models"]:
            result.append(item)
    return result

PRESET_MODELS = _build_preset_list()

# å…³é”®é…ç½®é¡¹è¯´æ˜
KEY_DESCRIPTIONS = {
    "hidden_size": "éšè—å±‚ç»´åº¦",
    "num_hidden_layers": "Transformer å±‚æ•°",
    "num_attention_heads": "æ³¨æ„åŠ›å¤´æ•° (Q)",
    "num_key_value_heads": "KV å¤´æ•° (GQA)",
    "intermediate_size": "FFN ä¸­é—´ç»´åº¦",
    "max_position_embeddings": "æœ€å¤§ä½ç½®ç¼–ç ",
    "rope_theta": "RoPE Base",
    "vocab_size": "è¯è¡¨å¤§å°",
    "sliding_window": "æ»‘åŠ¨çª—å£å¤§å°",
    "head_dim": "æ³¨æ„åŠ›å¤´ç»´åº¦",
    "rms_norm_eps": "RMSNorm epsilon",
    "tie_word_embeddings": "å…±äº«è¯åµŒå…¥",
    "torch_dtype": "é»˜è®¤ç²¾åº¦",
    "architectures": "æ¨¡å‹æ¶æ„",
    "model_type": "æ¨¡å‹ç±»å‹",
    "hidden_act": "æ¿€æ´»å‡½æ•°",
    "attention_dropout": "æ³¨æ„åŠ› Dropout",
    "attention_bias": "æ³¨æ„åŠ›åç½®",
    "mlp_bias": "MLP åç½®",
}

# é‡ç‚¹å±•ç¤ºçš„é…ç½®é¡¹
KEY_CONFIGS = [
    "hidden_size",
    "num_hidden_layers", 
    "num_attention_heads",
    "num_key_value_heads",
    "intermediate_size",
    "max_position_embeddings",
    "rope_theta",
    "vocab_size",
    "head_dim",
    "hidden_act",
    "tie_word_embeddings",
]


@st.cache_data(show_spinner=False, ttl=3600)
def load_config_from_hub(model_name: str, token: str = None) -> dict:
    """
    ä» HuggingFace Hub åŠ è½½æ¨¡å‹é…ç½®
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ– URL
        token: HF API tokenï¼ˆå¯é€‰ï¼‰
        
    Returns:
        é…ç½®å­—å…¸
    """
    model_name = extract_from_url(model_name)
    
    # ç›´æ¥ä¸‹è½½ config.json æ–‡ä»¶
    config_path = hf_hub_download(
        repo_id=model_name,
        filename="config.json",
        token=token if token else None
    )
    
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    
    return config


def render_model_selector(col, key_prefix: str, default_category_idx: int = 0, default_model_idx: int = 0):
    """
    æ¸²æŸ“æ¨¡å‹é€‰æ‹©å™¨ï¼ˆæ”¯æŒæŒ‰å‚å•†åˆ†ç±»ï¼‰
    
    Args:
        col: streamlit column
        key_prefix: ç”¨äºåŒºåˆ† A/B çš„å‰ç¼€
        default_category_idx: é»˜è®¤é€‰æ‹©çš„å‚å•†ç´¢å¼•
        default_model_idx: é»˜è®¤é€‰æ‹©çš„æ¨¡å‹ç´¢å¼•
        
    Returns:
        (model_id, display_name, token)
    """
    with col:
        # è¾“å…¥æ–¹å¼é€‰æ‹©
        input_mode = st.radio(
            "è¾“å…¥æ–¹å¼",
            ["é¢„è®¾æ¨¡å‹", "è‡ªå®šä¹‰æ¨¡å‹"],
            key=f"{key_prefix}_mode",
            horizontal=True
        )
        
        model_name = None
        display_name = None
        
        if input_mode == "é¢„è®¾æ¨¡å‹":
            # å‚å•†é€‰æ‹©
            categories = list(MODEL_CATEGORIES.keys())
            selected_category = st.selectbox(
                "é€‰æ‹©å‚å•†",
                categories,
                index=default_category_idx,
                key=f"{key_prefix}_category"
            )
            
            # æ¨¡å‹é€‰æ‹©
            models = MODEL_CATEGORIES[selected_category]["models"]
            model_names = [m[0] for m in models]
            selected_model = st.selectbox(
                "é€‰æ‹©æ¨¡å‹",
                model_names,
                index=min(default_model_idx, len(model_names) - 1),
                key=f"{key_prefix}_model"
            )
            
            # æ‰¾åˆ°å¯¹åº”çš„ model_id
            for name, model_id in models:
                if name == selected_model:
                    model_name = model_id
                    display_name = name
                    break
            
            st.caption(f"ğŸ“¦ `{model_name}`")
        else:
            model_name = st.text_input(
                "æ¨¡å‹åç§°æˆ– URL",
                placeholder="ä¾‹å¦‚: meta-llama/Llama-2-7b-hf",
                key=f"{key_prefix}_custom"
            )
            display_name = model_name.split("/")[-1] if model_name else None
        
        # Token è¾“å…¥ï¼ˆå¯é€‰ï¼‰
        token = st.text_input(
            "HF Token (å¯é€‰ï¼Œç”¨äºç§æœ‰æ¨¡å‹)",
            type="password",
            key=f"{key_prefix}_token"
        )
        
        return model_name, display_name, token


def render():
    """æ¸²æŸ“é¡µé¢"""
    st.markdown('<h1 class="module-title">Config å·®å¼‚å¯¹æ¯”</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="tip-box">
    å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ¶æ„é…ç½®ï¼Œæ”¯æŒä» HuggingFace Hub å®æ—¶è¯»å– config.jsonã€‚
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    # æ¨¡å‹ A (é»˜è®¤: Meta/Llama-2-7B)
    with col1:
        st.markdown("### æ¨¡å‹ A")
    model_a_name, display_a, token_a = render_model_selector(col1, "model_a", 0, 0)
    
    # æ¨¡å‹ B (é»˜è®¤: Meta/Llama-3.1-8B)
    with col2:
        st.markdown("### æ¨¡å‹ B")
    model_b_name, display_b, token_b = render_model_selector(col2, "model_b", 0, 1)
    
    # åŠ è½½æŒ‰é’®
    st.markdown("---")
    
    if not model_a_name or not model_b_name:
        st.warning("è¯·é€‰æ‹©æˆ–è¾“å…¥ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”")
        return
    
    col_btn, col_opt = st.columns([1, 3])
    with col_btn:
        load_clicked = st.button("åŠ è½½é…ç½®", type="primary", width="stretch")
    with col_opt:
        show_all = st.checkbox("æ˜¾ç¤ºå…¨éƒ¨é…ç½®é¡¹", value=False)
    
    # åŠ è½½é…ç½®
    if load_clicked:
        config_a = None
        config_b = None
        
        with st.spinner(f"æ­£åœ¨åŠ è½½ {display_a} çš„é…ç½®..."):
            try:
                config_a = load_config_from_hub(model_a_name, token_a)
                st.session_state["config_a"] = config_a
                st.session_state["display_a"] = display_a
            except Exception as e:
                st.error(f"åŠ è½½ {display_a} å¤±è´¥: {str(e)}")
                return
        
        with st.spinner(f"æ­£åœ¨åŠ è½½ {display_b} çš„é…ç½®..."):
            try:
                config_b = load_config_from_hub(model_b_name, token_b)
                st.session_state["config_b"] = config_b
                st.session_state["display_b"] = display_b
            except Exception as e:
                st.error(f"åŠ è½½ {display_b} å¤±è´¥: {str(e)}")
                return
        
        st.success("âœ… é…ç½®åŠ è½½æˆåŠŸï¼")
    
    # ä» session state è·å–é…ç½®
    config_a = st.session_state.get("config_a")
    config_b = st.session_state.get("config_b")
    display_a = st.session_state.get("display_a", "æ¨¡å‹ A")
    display_b = st.session_state.get("display_b", "æ¨¡å‹ B")
    
    if not config_a or not config_b:
        return
    
    # æ˜¾ç¤ºé…ç½®å¯¹æ¯”
    st.markdown("### é…ç½®å¯¹æ¯”")
    
    # å†³å®šè¦æ˜¾ç¤ºçš„ key
    if show_all:
        all_keys = set(config_a.keys()) | set(config_b.keys())
        # è¿‡æ»¤æ‰ä¸€äº›ä¸å¤ªæœ‰ç”¨çš„ key
        exclude_keys = {"_name_or_path", "transformers_version", "_commit_hash", "auto_map"}
        all_keys = all_keys - exclude_keys
    else:
        all_keys = set(KEY_CONFIGS)
    
    # æ„å»ºå¯¹æ¯”è¡¨
    diff_data = []
    for key in sorted(all_keys):
        val_a = config_a.get(key, "N/A")
        val_b = config_b.get(key, "N/A")
        
        # æ ¼å¼åŒ–å€¼
        val_a_str = format_value(val_a)
        val_b_str = format_value(val_b)
        
        # åˆ¤æ–­æ˜¯å¦æœ‰å·®å¼‚
        is_diff = val_a != val_b
        
        diff_data.append({
            "é…ç½®é¡¹": key,
            "è¯´æ˜": KEY_DESCRIPTIONS.get(key, ""),
            display_a: val_a_str,
            display_b: val_b_str,
            "å·®å¼‚": "âš ï¸" if is_diff else "âœ…"
        })
    
    # æ˜¾ç¤ºè¡¨æ ¼
    st.dataframe(diff_data, hide_index=True, width="stretch")
    
    # å…³é”®å·®å¼‚åˆ†æ
    render_analysis(config_a, config_b, display_a, display_b)
    
    # åŸå§‹ JSON å±•ç¤º
    with st.expander("æŸ¥çœ‹åŸå§‹é…ç½® JSON"):
        col_json1, col_json2 = st.columns(2)
        with col_json1:
            st.markdown(f"**{display_a}**")
            st.json(config_a)
        with col_json2:
            st.markdown(f"**{display_b}**")
            st.json(config_b)


def format_value(val):
    """æ ¼å¼åŒ–é…ç½®å€¼ä¸ºå­—ç¬¦ä¸²"""
    if isinstance(val, list):
        return ", ".join(str(v) for v in val)
    elif isinstance(val, bool):
        return "âœ“" if val else "âœ—"
    elif isinstance(val, (int, float)) and val >= 10000:
        return f"{val:,}"
    return str(val)


def render_analysis(config_a: dict, config_b: dict, name_a: str, name_b: str):
    """æ¸²æŸ“å…³é”®å·®å¼‚åˆ†æ"""
    st.markdown("### å…³é”®å·®å¼‚åˆ†æ")
    
    # GQA åˆ†æ
    gqa_a = config_a.get("num_key_value_heads", config_a.get("num_attention_heads"))
    gqa_b = config_b.get("num_key_value_heads", config_b.get("num_attention_heads"))
    heads_a = config_a.get("num_attention_heads", 32)
    heads_b = config_b.get("num_attention_heads", 32)
    
    col_a, col_b = st.columns(2)
    
    with col_a:
        st.markdown(f"**{name_a}**")
        if gqa_a and heads_a:
            if gqa_a == heads_a:
                st.info("ä½¿ç”¨ MHA (Multi-Head Attention)")
            elif gqa_a == 1:
                st.warning("ä½¿ç”¨ MQA (Multi-Query Attention)")
            else:
                ratio = heads_a // gqa_a if gqa_a else 1
                st.success(f"ä½¿ç”¨ GQA, KV å¤´æ•°å‹ç¼© {ratio}x")
    
    with col_b:
        st.markdown(f"**{name_b}**")
        if gqa_b and heads_b:
            if gqa_b == heads_b:
                st.info("ä½¿ç”¨ MHA (Multi-Head Attention)")
            elif gqa_b == 1:
                st.warning("ä½¿ç”¨ MQA (Multi-Query Attention)")
            else:
                ratio = heads_b // gqa_b if gqa_b else 1
                st.success(f"ä½¿ç”¨ GQA, KV å¤´æ•°å‹ç¼© {ratio}x")
    
    # RoPE åˆ†æ
    rope_a = config_a.get("rope_theta")
    rope_b = config_b.get("rope_theta")
    
    if rope_a and rope_b and rope_a != rope_b:
        st.markdown(f"""
        **RoPE Base å·®å¼‚**:
        - {name_a}: `{rope_a:,}`
        - {name_b}: `{rope_b:,}`
        - æ›´å¤§çš„ base æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡å¤–æ¨
        """)
    
    # å‚æ•°é‡ä¼°ç®—
    st.markdown("---")
    st.markdown("### ğŸ“ å‚æ•°é‡ä¼°ç®—")
    
    params_a = estimate_params(config_a)
    params_b = estimate_params(config_b)
    
    col_1, col_2 = st.columns(2)
    with col_1:
        if params_a:
            st.metric(f"{name_a} ä¼°ç®—å‚æ•°", f"~{params_a:.2f}B")
        else:
            st.metric(f"{name_a} ä¼°ç®—å‚æ•°", "N/A")
    with col_2:
        if params_b:
            st.metric(f"{name_b} ä¼°ç®—å‚æ•°", f"~{params_b:.2f}B")
        else:
            st.metric(f"{name_b} ä¼°ç®—å‚æ•°", "N/A")


def estimate_params(config: dict) -> float:
    """
    ä¼°ç®—æ¨¡å‹å‚æ•°é‡ï¼ˆå•ä½ï¼šBï¼‰
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸
        
    Returns:
        å‚æ•°é‡ï¼ˆå•ä½ï¼šåäº¿ï¼‰
    """
    try:
        d = config.get('hidden_size')
        L = config.get('num_hidden_layers')
        V = config.get('vocab_size')
        ff = config.get('intermediate_size')
        
        if not all([d, L, V, ff]):
            return None
        
        # è€ƒè™‘ GQA
        num_heads = config.get('num_attention_heads', 32)
        num_kv_heads = config.get('num_key_value_heads', num_heads)
        head_dim = config.get('head_dim', d // num_heads)
        
        # Attention: Q + K + V + O
        q_proj = d * (num_heads * head_dim)
        kv_proj = d * (num_kv_heads * head_dim) * 2  # K + V
        o_proj = (num_heads * head_dim) * d
        attention = (q_proj + kv_proj + o_proj) * L
        
        # FFN: gate, up, down (for SwiGLU)
        ffn = 3 * d * ff * L
        
        # Embedding
        tie_embeddings = config.get('tie_word_embeddings', False)
        embed = V * d * (1 if tie_embeddings else 2)
        
        # Layer norms
        ln = 2 * d * L + d  # æ¯å±‚ 2 ä¸ª LN + æœ€åä¸€ä¸ª LN
        
        total = (attention + ffn + embed + ln) / 1e9
        return total
    except Exception:
        return None

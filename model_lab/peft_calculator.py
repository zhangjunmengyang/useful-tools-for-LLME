"""
PEFT å‚æ•°è®¡ç®—å™¨ - è®¡ç®— LoRA/QLoRA çš„å¯è®­ç»ƒå‚æ•°é‡
"""

import streamlit as st
import pandas as pd

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "Llama-2-7B": {"hidden_size": 4096, "num_layers": 32, "num_heads": 32, "intermediate_size": 11008},
    "Llama-2-13B": {"hidden_size": 5120, "num_layers": 40, "num_heads": 40, "intermediate_size": 13824},
    "Llama-2-70B": {"hidden_size": 8192, "num_layers": 80, "num_heads": 64, "intermediate_size": 28672},
    "Llama-3-8B": {"hidden_size": 4096, "num_layers": 32, "num_heads": 32, "intermediate_size": 14336},
    "Qwen-7B": {"hidden_size": 4096, "num_layers": 32, "num_heads": 32, "intermediate_size": 11008},
    "Mistral-7B": {"hidden_size": 4096, "num_layers": 32, "num_heads": 32, "intermediate_size": 14336},
}

# å¯è®­ç»ƒæ¨¡å—
TARGET_MODULES = {
    "q_proj": "Query æŠ•å½±",
    "k_proj": "Key æŠ•å½±",
    "v_proj": "Value æŠ•å½±",
    "o_proj": "Output æŠ•å½±",
    "gate_proj": "FFN Gate",
    "up_proj": "FFN Up",
    "down_proj": "FFN Down",
}


def calculate_lora_params(hidden_size: int, rank: int, num_layers: int, modules: list) -> dict:
    """è®¡ç®— LoRA å‚æ•°é‡"""
    params_per_layer = 0
    details = []
    
    for module in modules:
        if module in ["q_proj", "k_proj", "v_proj", "o_proj"]:
            # Attention æ¨¡å—: hidden_size -> hidden_size
            module_params = 2 * hidden_size * rank  # A + B
            params_per_layer += module_params
            details.append({"æ¨¡å—": module, "æ¯å±‚å‚æ•°": module_params})
        elif module in ["gate_proj", "up_proj", "down_proj"]:
            # FFN æ¨¡å—æ¯”è¾ƒå¤æ‚ï¼Œç®€åŒ–å¤„ç†
            module_params = 2 * hidden_size * rank
            params_per_layer += module_params
            details.append({"æ¨¡å—": module, "æ¯å±‚å‚æ•°": module_params})
    
    total_params = params_per_layer * num_layers
    
    return {
        "total_params": total_params,
        "params_per_layer": params_per_layer,
        "details": details
    }


def render():
    """æ¸²æŸ“é¡µé¢"""
    st.markdown('<h1 class="module-title">PEFT å‚æ•°è®¡ç®—å™¨</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="tip-box">
    ğŸ’¡ è®¡ç®— LoRA/QLoRA çš„å¯è®­ç»ƒå‚æ•°é‡ï¼Œè¯„ä¼°å¾®è°ƒæˆæœ¬ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("### æ¨¡å‹é…ç½®")
        
        model_choice = st.selectbox("é€‰æ‹©æ¨¡å‹", list(MODEL_CONFIGS.keys()))
        config = MODEL_CONFIGS[model_choice]
        
        st.info(f"""
        **{model_choice}**
        - Hidden: {config['hidden_size']}
        - Layers: {config['num_layers']}
        - Heads: {config['num_heads']}
        """)
        
        st.markdown("### LoRA å‚æ•°")
        
        rank = st.slider("Rank (r)", 4, 256, 16, help="LoRA ä½ç§©ç»´åº¦")
        alpha = st.slider("Alpha (Î±)", 8, 512, 32, help="ç¼©æ”¾å› å­")
        
        st.markdown("### ç›®æ ‡æ¨¡å—")
        
        selected_modules = []
        for module_id, module_name in TARGET_MODULES.items():
            if st.checkbox(module_name, value=module_id in ["q_proj", "v_proj"], key=f"mod_{module_id}"):
                selected_modules.append(module_id)
    
    with col2:
        st.markdown("### è®¡ç®—ç»“æœ")
        
        if selected_modules:
            result = calculate_lora_params(
                config['hidden_size'], rank, config['num_layers'], selected_modules
            )
            
            # ä¼°ç®—åŸå§‹æ¨¡å‹å‚æ•°é‡ (ç®€åŒ–)
            base_params = config['hidden_size'] * config['hidden_size'] * 4 * config['num_layers']  # ç®€åŒ–ä¼°ç®—
            trainable_ratio = result['total_params'] / base_params * 100
            
            col_a, col_b, col_c = st.columns(3)
            with col_a:
                st.metric("LoRA å‚æ•°é‡", f"{result['total_params']:,}")
            with col_b:
                st.metric("å‚æ•°é‡ (MB)", f"{result['total_params'] * 2 / 1024 / 1024:.2f}")
            with col_c:
                st.metric("å¯è®­ç»ƒæ¯”ä¾‹", f"~{trainable_ratio:.3f}%")
            
            # è¯¦ç»†è¡¨æ ¼
            st.markdown("### å‚æ•°åˆ†å¸ƒ")
            df = pd.DataFrame(result['details'])
            df['æ€»å‚æ•°'] = df['æ¯å±‚å‚æ•°'] * config['num_layers']
            st.dataframe(df, hide_index=True)
            
            # å…¬å¼è¯´æ˜
            st.markdown("""
            ### ğŸ“ è®¡ç®—å…¬å¼
            
            ```
            LoRA å‚æ•° = 2 Ã— hidden_size Ã— rank Ã— num_modules Ã— num_layers
            
            å…¶ä¸­:
            - A çŸ©é˜µ: hidden_size Ã— rank
            - B çŸ©é˜µ: rank Ã— hidden_size
            - scaling = Î± / r
            ```
            """)
        else:
            st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç›®æ ‡æ¨¡å—")
    
    # QLoRA è¯´æ˜
    st.markdown("---")
    st.markdown("""
    ### ğŸ”§ QLoRA ç‰¹ç‚¹
    
    | ç‰¹æ€§ | LoRA | QLoRA |
    |------|------|-------|
    | åŸºåº§æ¨¡å‹ç²¾åº¦ | FP16/BF16 | INT4 (NF4) |
    | LoRA æƒé‡ç²¾åº¦ | FP16 | BF16 |
    | æ˜¾å­˜å ç”¨ | ~16GB (7B) | ~6GB (7B) |
    | è®­ç»ƒé€Ÿåº¦ | å¿« | ç¨æ…¢ (åé‡åŒ–) |
    
    QLoRA = 4-bit é‡åŒ– + LoRA + Double Quantization + Paged Optimizer
    """)


"""
åˆ†è¯ç¼–ç  - äº¤äº’å¼åˆ†è¯ã€ç¼–è§£ç ä¸å­—èŠ‚åˆ†æ
"""

import streamlit as st
import pandas as pd
from utils.tokenizer_utils import (
    load_tokenizer, 
    get_token_info, 
    get_tokenizer_info,
    decode_token_ids,
    calculate_compression_stats,
    get_normalization_info,
    get_unicode_info,
    get_special_tokens_map,
    get_model_categories,
    get_models_by_category,
    MODEL_CATEGORIES,
    TOKEN_COLORS
)


def render_token_display(token_info_list: list, show_ids: bool = True) -> None:
    """æ¸²æŸ“åˆ†è¯ç»“æœ"""
    if not token_info_list:
        st.info("è¯·è¾“å…¥æ–‡æœ¬")
        return
    
    html_parts = ['<div style="background-color: #F3F4F6; border: 1px solid #E5E7EB; border-radius: 6px; padding: 16px; line-height: 2.2; font-family: \'JetBrains Mono\', monospace;">']
    
    for idx, info in enumerate(token_info_list):
        color = TOKEN_COLORS[idx % len(TOKEN_COLORS)]
        token_str = info['token_str']
        token_id = info['token_id']
        raw_token = info.get('raw_token', token_str)
        byte_seq = info.get('byte_sequence', 'N/A')
        is_special = info.get('is_special', False)
        is_byte_fallback = info.get('is_byte_fallback', False)
        is_byte_token = info.get('is_byte_token', False)
        
        display_str = token_str.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        display_str = display_str.replace(' ', '\u2423')
        if '\n' in display_str:
            display_str = display_str.replace('\n', '\u21b5')
        if '\t' in display_str:
            display_str = display_str.replace('\t', '\u2192')
        
        if not display_str.strip():
            display_str = repr(token_str)[1:-1] if token_str else '[EMPTY]'
        
        border_style = ""
        label = ""
        if is_special:
            border_style = "border: 2px solid #DC2626;"
            label = '<small style="color:#DC2626;font-size:10px;margin-left:2px;">[S]</small>'
        elif is_byte_fallback:
            border_style = "border: 2px dashed #D97706;"
            label = '<small style="color:#D97706;font-size:10px;margin-left:2px;">[B]</small>'
        elif is_byte_token:
            border_style = "border: 1px solid #9CA3AF;"
        
        tooltip_text = f"Token: {raw_token}  |  ID: {token_id}  |  Bytes: {byte_seq}"
        id_html = f'<sub style="font-size: 11px; color: #6B7280; margin-left: 2px;">{token_id}</sub>' if show_ids else ''
        
        html_parts.append(
            f'<span title="{tooltip_text}" '
            f'style="background-color: {color}; color: #111827; padding: 4px 8px; margin: 2px; '
            f'border-radius: 4px; display: inline-block; font-size: 13px; cursor: default; {border_style}">'
            f'{display_str}{id_html}{label}</span>'
        )
    
    html_parts.append('</div>')
    st.markdown(''.join(html_parts), unsafe_allow_html=True)


def render_unicode_table(text: str):
    """æ¸²æŸ“ Unicode è¡¨"""
    if not text:
        return
    
    data = []
    for char in text:
        info = get_unicode_info(char)
        data.append({
            "å­—ç¬¦": char,
            "åç§°": info.get('name', 'N/A'),
            "åˆ†ç±»": info.get('category', 'N/A'),
            "ç ç‚¹": info.get('codepoint', 'N/A'),
            "åè¿›åˆ¶": info.get('decimal', 'N/A'),
            "UTF-8": info.get('utf8_bytes', 'N/A')
        })
    
    st.dataframe(pd.DataFrame(data), width="stretch", hide_index=True)


def render():
    """æ¸²æŸ“é¡µé¢"""
    st.markdown('<h1 class="module-title">åˆ†è¯ç¼–ç </h1>', unsafe_allow_html=True)
    
    # ä¸¤çº§è”åŠ¨æ¨¡å‹é€‰æ‹©
    st.markdown("#### é€‰æ‹©æ¨¡å‹")
    
    # è·å–å‚å•†åˆ—è¡¨
    categories = list(MODEL_CATEGORIES.keys())
    category_display = [cat for cat in categories]
    
    col_provider, col_model = st.columns([1, 2])
    
    with col_provider:
        # å‚å•†é€‰æ‹©
        selected_category_display = st.selectbox(
            "æ¨¡å‹å‚å•†",
            options=category_display,
            index=0,
            key="model_provider",
            help="é€‰æ‹©æ¨¡å‹æä¾›å•†"
        )
        # æå–å®é™…çš„åˆ†ç±»åç§°ï¼ˆå»æ‰å›¾æ ‡ï¼‰
        selected_category = categories[category_display.index(selected_category_display)]
    
    with col_model:
        # è·å–è¯¥å‚å•†ä¸‹çš„æ¨¡å‹
        models = get_models_by_category(selected_category)
        model_display = [name for name, _ in models]
        model_ids = [model_id for _, model_id in models]
        
        # æ¨¡å‹é€‰æ‹©
        selected_model_display = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=model_display,
            index=0,
            key="model_name",
            help="é€‰æ‹©è¯¥å‚å•†ä¸‹çš„å…·ä½“æ¨¡å‹"
        )
        
        # è·å–å®é™…çš„ model_id
        if model_display:
            model_idx = model_display.index(selected_model_display)
            model_name = model_ids[model_idx]
        else:
            model_name = None
    
    # æ˜¾ç¤ºé€‰ä¸­çš„æ¨¡å‹ ID
    if model_name:
        st.caption(f"ğŸ“¦ Tokenizer: `{model_name}`")
    
    tokenizer = load_tokenizer(model_name) if model_name else None
    
    if not tokenizer:
        st.warning("è¯·é€‰æ‹©æ¨¡å‹")
        return
    
    # æ¨¡å‹ä¿¡æ¯
    with st.expander("æ¨¡å‹ä¿¡æ¯", expanded=False):
        info = get_tokenizer_info(tokenizer)
        col_a, col_b, col_c = st.columns(3)
        with col_a:
            st.metric("è¯è¡¨å¤§å°", f"{info['vocab_size']:,}")
        with col_b:
            st.metric("æœ€å¤§é•¿åº¦", str(info['model_max_length']))
        with col_c:
            st.metric("ç®—æ³•ç±»å‹", info['algorithm'].split('(')[0].strip())
        
        special_tokens = []
        for name in ['bos_token', 'eos_token', 'pad_token', 'unk_token']:
            token = info.get(name)
            if token:
                special_tokens.append(f"**{name}**: `{token}`")
        if special_tokens:
            st.markdown(" | ".join(special_tokens))
    
    st.markdown("---")
    
    # ä¸»è¦åŠŸèƒ½ Tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ç¼–ç ", "è§£ç ", "å­—èŠ‚åˆ†æ", "Unicode", "ç‰¹æ®Š Token"])
    
    # ========== ç¼–ç  Tab ==========
    with tab1:
        input_text = st.text_area(
            "è¾“å…¥æ–‡æœ¬",
            value="",
            height=120,
            placeholder="è¾“å…¥æ–‡æœ¬ï¼Œç‚¹å‡»å¤–éƒ¨æˆ–æŒ‰ Cmd+Enter æŸ¥çœ‹ç»“æœ",
            key="encode_input"
        )
        
        opt_col1, opt_col2 = st.columns(2)
        with opt_col1:
            show_ids = st.checkbox("æ˜¾ç¤º Token ID", value=True, key="show_ids")
        with opt_col2:
            add_special = st.checkbox("æ·»åŠ ç‰¹æ®Š Token", value=False, key="add_special")
        
        if input_text:
            if add_special:
                encoding = tokenizer(input_text, add_special_tokens=True)
                token_ids = encoding["input_ids"]
                token_info_list = []
                special_ids = set(tokenizer.all_special_ids) if hasattr(tokenizer, 'all_special_ids') else set()
                for idx, tid in enumerate(token_ids):
                    raw_token = tokenizer.convert_ids_to_tokens([tid])[0] if hasattr(tokenizer, 'convert_ids_to_tokens') else ""
                    token_str = tokenizer.decode([tid])
                    
                    is_byte_token = '\ufffd' in token_str
                    display_token_str = raw_token if (is_byte_token and raw_token) else token_str
                    
                    try:
                        byte_seq = ' '.join([f'0x{b:02X}' for b in (raw_token or token_str).encode('utf-8')])
                    except:
                        byte_seq = "N/A"
                    token_info_list.append({
                        "token_str": display_token_str,
                        "decoded_str": token_str,
                        "raw_token": raw_token,
                        "token_id": tid,
                        "byte_sequence": byte_seq,
                        "is_special": tid in special_ids,
                        "is_byte_fallback": raw_token.startswith('<0x') and raw_token.endswith('>'),
                        "is_byte_token": is_byte_token,
                        "index": idx
                    })
            else:
                token_info_list = get_token_info(tokenizer, input_text)
            
            stats = calculate_compression_stats(input_text, len(token_info_list))
            
            stat_cols = st.columns(4)
            with stat_cols[0]:
                st.metric("Token æ•°", stats['token_count'])
            with stat_cols[1]:
                st.metric("å­—ç¬¦æ•°", stats['char_count'])
            with stat_cols[2]:
                st.metric("å­—ç¬¦/Token", stats['chars_per_token'])
            with stat_cols[3]:
                st.metric("å­—èŠ‚/Token", stats['bytes_per_token'])
            
            st.markdown("#### åˆ†è¯ç»“æœ")
            render_token_display(token_info_list, show_ids)
            
            with st.expander("Token ID åºåˆ—"):
                ids = [info['token_id'] for info in token_info_list]
                st.code(str(ids), language="python")
            
            with st.expander("è¯¦ç»†ä¿¡æ¯"):
                df_data = []
                for info in token_info_list:
                    df_data.append({
                        "Index": info['index'],
                        "Token": info['raw_token'],
                        "Decoded": repr(info['token_str']),
                        "ID": info['token_id'],
                        "Bytes": info['byte_sequence'],
                        "Special": "Yes" if info.get('is_special') else "",
                        "Fallback": "Yes" if info.get('is_byte_fallback') else ""
                    })
                st.dataframe(pd.DataFrame(df_data), width="stretch", hide_index=True)
    
    # ========== è§£ç  Tab ==========
    with tab2:
        id_input = st.text_input(
            "Token IDs",
            placeholder="ä¾‹å¦‚: 128000, 50256 æˆ– [128000, 50256]",
            key="decode_input"
        )
        
        if id_input:
            try:
                cleaned = id_input.strip().strip('[]')
                token_ids = [int(x.strip()) for x in cleaned.split(',') if x.strip()]
                
                if token_ids:
                    decoded_text, individual_tokens = decode_token_ids(tokenizer, token_ids)
                    
                    st.markdown("#### è§£ç ç»“æœ")
                    st.code(decoded_text, language=None)
                    
                    st.markdown("#### Token è¯¦æƒ…")
                    for tok in individual_tokens:
                        st.markdown(
                            f"**ID {tok['token_id']}** â†’ `{tok['raw_token']}` â†’ \"{tok['token_str']}\""
                        )
            except ValueError as e:
                st.error(f"æ ¼å¼é”™è¯¯: {str(e)}")
    
    # ========== å­—èŠ‚åˆ†æ Tab ==========
    with tab3:
        byte_input = st.text_area("è¾“å…¥æ–‡æœ¬", value="", height=80, 
                                   placeholder="è¾“å…¥ Emojiã€ç”Ÿåƒ»å­—ç­‰ç‰¹æ®Šå­—ç¬¦æŸ¥çœ‹å­—èŠ‚çº§åˆ†è¯", key="byte_input")
        
        if byte_input:
            token_info = get_token_info(tokenizer, byte_input)
            
            total = len(token_info)
            fallback_count = sum(1 for t in token_info if t.get('is_byte_fallback', False))
            byte_token_count = sum(1 for t in token_info if t.get('is_byte_token', False))
            special_count = sum(1 for t in token_info if t.get('is_special', False))
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€» Token", total)
            with col2:
                st.metric("Byte Fallback", fallback_count, 
                         delta="éœ€è¦å›é€€" if fallback_count > 0 else None, delta_color="off")
            with col3:
                st.metric("å­—èŠ‚çº§ BPE", byte_token_count)
            with col4:
                st.metric("ç‰¹æ®Š Token", special_count)
            
            st.markdown("#### åˆ†è¯ç»“æœ")
            render_token_display(token_info, show_ids=True)
            
            if fallback_count > 0:
                st.markdown("#### Byte Fallback è¯¦æƒ…")
                fallback_data = [{"Index": t['index'], "Token": t['raw_token'], "ID": t['token_id'], 
                                 "Byte": t.get('byte_sequence', 'N/A')} 
                                for t in token_info if t.get('is_byte_fallback')]
                if fallback_data:
                    st.dataframe(pd.DataFrame(fallback_data), width="stretch", hide_index=True)
    
    # ========== Unicode Tab ==========
    with tab4:
        norm_input = st.text_input("è¾“å…¥æ–‡æœ¬", value="", placeholder="è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬", key="norm_input")
        
        if norm_input:
            norm_info = get_normalization_info(norm_input)
            
            st.markdown("#### è§„èŒƒåŒ–å¯¹æ¯”")
            norm_data = [
                {"å½¢å¼": "åŸå§‹", "é•¿åº¦": norm_info['original_len'], "æ–‡æœ¬": norm_info['original'], "ç›¸åŒ": "-"},
                {"å½¢å¼": "NFC", "é•¿åº¦": norm_info['NFC_len'], "æ–‡æœ¬": norm_info['NFC'], "ç›¸åŒ": "Yes" if norm_info['nfc_equal'] else "No"},
                {"å½¢å¼": "NFD", "é•¿åº¦": norm_info['NFD_len'], "æ–‡æœ¬": norm_info['NFD'], "ç›¸åŒ": "Yes" if norm_info['nfd_equal'] else "No"},
                {"å½¢å¼": "NFKC", "é•¿åº¦": norm_info['NFKC_len'], "æ–‡æœ¬": norm_info['NFKC'], "ç›¸åŒ": "-"},
                {"å½¢å¼": "NFKD", "é•¿åº¦": norm_info['NFKD_len'], "æ–‡æœ¬": norm_info['NFKD'], "ç›¸åŒ": "-"},
            ]
            st.dataframe(pd.DataFrame(norm_data), width="stretch", hide_index=True)
            
            st.markdown("#### Unicode è¯¦æƒ…")
            render_unicode_table(norm_input[:50])
    
    # ========== ç‰¹æ®Š Token Tab ==========
    with tab5:
        special_map = get_special_tokens_map(tokenizer)
        
        if special_map:
            st.markdown("#### æ ‡å‡†ç‰¹æ®Š Token")
            standard = [{"åç§°": n, "Token": special_map[n]['token'], "ID": special_map[n]['id']} 
                       for n in ['bos_token', 'eos_token', 'pad_token', 'unk_token', 'cls_token', 'sep_token', 'mask_token'] 
                       if n in special_map]
            if standard:
                st.dataframe(pd.DataFrame(standard), width="stretch", hide_index=True)
            else:
                st.info("æ— æ ‡å‡†ç‰¹æ®Š Token")
            
            if 'additional_special_tokens' in special_map and special_map['additional_special_tokens']:
                st.markdown("#### é¢å¤–ç‰¹æ®Š Token")
                additional = special_map['additional_special_tokens']
                st.dataframe(pd.DataFrame([{"Token": t['token'], "ID": t['id']} for t in additional[:20]]), 
                            width="stretch", hide_index=True)
                if len(additional) > 20:
                    st.caption(f"å…± {len(additional)} ä¸ª")
        else:
            st.info("æ— æ³•è·å–ç‰¹æ®Š Token")

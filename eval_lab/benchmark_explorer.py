"""
Benchmark Explorer - Benchmark æ•°æ®å¯è§†åŒ–å¯¹æ¯”

æä¾›ä¸»æµ LLM benchmark ç»“æœçš„äº¤äº’å¼å¯è§†åŒ–ï¼ŒåŒ…æ‹¬ï¼š
- å¤šæ¨¡å‹é›·è¾¾å›¾å¯¹æ¯”
- åˆ†ç±»åˆ«æŸ±çŠ¶å›¾å±•ç¤º  
- æ”¯æŒé€‰æ‹©æ¨¡å‹å’Œbenchmarkç»´åº¦
"""

import gradio as gr
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
from typing import List, Dict, Any, Tuple
import json


# å†…ç½® benchmark æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®æ•°æ®ï¼‰
BENCHMARK_DATA = {
    'GPT-4': {
        'MMLU': 86.4,
        'HumanEval': 67.0, 
        'GSM8K': 92.0,
        'HellaSwag': 95.3,
        'ARC': 96.3,
        'TruthfulQA': 59.0,
        'Winogrande': 87.5,
        'MATH': 42.5
    },
    'Claude-3.5-Sonnet': {
        'MMLU': 88.7,
        'HumanEval': 92.0,
        'GSM8K': 96.4,
        'HellaSwag': 89.0,
        'ARC': 96.0,
        'TruthfulQA': 71.3,
        'Winogrande': 89.6,
        'MATH': 71.1
    },
    'GPT-3.5-Turbo': {
        'MMLU': 70.0,
        'HumanEval': 48.1,
        'GSM8K': 57.1,
        'HellaSwag': 85.5,
        'ARC': 85.2,
        'TruthfulQA': 47.0,
        'Winogrande': 81.6,
        'MATH': 34.1
    },
    'Llama-2-70B': {
        'MMLU': 69.8,
        'HumanEval': 29.9,
        'GSM8K': 56.8,
        'HellaSwag': 87.3,
        'ARC': 67.3,
        'TruthfulQA': 52.8,
        'Winogrande': 80.2,
        'MATH': 13.5
    },
    'Llama-3-70B': {
        'MMLU': 82.0,
        'HumanEval': 81.7,
        'GSM8K': 93.0,
        'HellaSwag': 88.0,
        'ARC': 93.0,
        'TruthfulQA': 63.2,
        'Winogrande': 83.5,
        'MATH': 50.4
    },
    'Qwen-72B': {
        'MMLU': 77.4,
        'HumanEval': 35.9,
        'GSM8K': 78.9,
        'HellaSwag': 86.0,
        'ARC': 88.8,
        'TruthfulQA': 54.1,
        'Winogrande': 82.3,
        'MATH': 35.7
    },
    'Gemini-Pro': {
        'MMLU': 71.8,
        'HumanEval': 67.7,
        'GSM8K': 86.5,
        'HellaSwag': 87.8,
        'ARC': 87.2,
        'TruthfulQA': 45.7,
        'Winogrande': 87.0,
        'MATH': 32.6
    },
    'PaLM-2-L': {
        'MMLU': 78.3,
        'HumanEval': 26.2,
        'GSM8K': 80.7,
        'HellaSwag': 86.8,
        'ARC': 89.7,
        'TruthfulQA': 54.6,
        'Winogrande': 83.7,
        'MATH': 34.3
    }
}

# Benchmark æè¿°ä¿¡æ¯
BENCHMARK_INFO = {
    'MMLU': 'Massive Multitask Language Understanding - æ¶µç›–57ä¸ªå­¦ç§‘çš„ç»¼åˆçŸ¥è¯†è¯„æµ‹',
    'HumanEval': 'Python ä»£ç ç”Ÿæˆèƒ½åŠ›è¯„æµ‹ï¼ŒåŒ…å«164ä¸ªç¼–ç¨‹é—®é¢˜',
    'GSM8K': 'å°å­¦æ•°å­¦åº”ç”¨é¢˜æ¨ç†ï¼Œæµ‹è¯•åŸºç¡€æ•°å­¦æ¨ç†èƒ½åŠ›', 
    'HellaSwag': 'å¸¸è¯†æ¨ç†è¯„æµ‹ï¼Œé€‰æ‹©æœ€åˆç†çš„å¥å­ç»“å°¾',
    'ARC': 'AI2 Reasoning Challenge - ç§‘å­¦å¸¸è¯†æ¨ç†',
    'TruthfulQA': 'è¯„æµ‹æ¨¡å‹å›ç­”çš„çœŸå®æ€§å’Œå‡†ç¡®æ€§',
    'Winogrande': 'Winograd Schema Challenge çš„æ‰©å±•ç‰ˆæœ¬ï¼Œå¸¸è¯†æ¨ç†',
    'MATH': 'é«˜éš¾åº¦æ•°å­¦ç«èµ›é¢˜ç›®ï¼Œæµ‹è¯•é«˜çº§æ•°å­¦æ¨ç†èƒ½åŠ›'
}

# é¢œè‰²æ–¹æ¡ˆ
MODEL_COLORS = [
    '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'
]


def create_radar_chart(selected_models: List[str], selected_benchmarks: List[str]) -> go.Figure:
    """
    åˆ›å»ºé›·è¾¾å›¾å¯¹æ¯”é€‰ä¸­çš„æ¨¡å‹åœ¨å„benchmarksä¸Šçš„è¡¨ç°
    
    Args:
        selected_models: é€‰æ‹©çš„æ¨¡å‹åˆ—è¡¨
        selected_benchmarks: é€‰æ‹©çš„benchmarkåˆ—è¡¨
    
    Returns:
        Plotly é›·è¾¾å›¾
    """
    if not selected_models or not selected_benchmarks:
        fig = go.Figure()
        fig.add_annotation(
            text="è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªBenchmark",
            xref="paper", yref="paper",
            x=0.5, y=0.5, xanchor='center', yanchor='middle',
            font=dict(size=16, color="gray")
        )
        return fig
    
    fig = go.Figure()
    
    # ä¸ºæ¯ä¸ªé€‰ä¸­çš„æ¨¡å‹æ·»åŠ é›·è¾¾å›¾è½¨è¿¹
    for i, model in enumerate(selected_models):
        if model in BENCHMARK_DATA:
            # è·å–è¯¥æ¨¡å‹åœ¨é€‰ä¸­benchmarksä¸Šçš„åˆ†æ•°
            scores = []
            categories = []
            
            for benchmark in selected_benchmarks:
                if benchmark in BENCHMARK_DATA[model]:
                    scores.append(BENCHMARK_DATA[model][benchmark])
                    categories.append(benchmark)
            
            if scores:
                # æ·»åŠ é›·è¾¾å›¾è½¨è¿¹
                fig.add_trace(go.Scatterpolar(
                    r=scores + [scores[0]],  # é—­åˆå›¾å½¢
                    theta=categories + [categories[0]],
                    fill='toself',
                    name=model,
                    line=dict(color=MODEL_COLORS[i % len(MODEL_COLORS)], width=2),
                    fillcolor=MODEL_COLORS[i % len(MODEL_COLORS)],
                    opacity=0.3
                ))
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100],
                tickfont=dict(size=12),
                gridcolor='lightgray'
            ),
            angularaxis=dict(
                tickfont=dict(size=12, color='black'),
                rotation=90,
                direction='counterclockwise'
            )
        ),
        showlegend=True,
        title=dict(
            text="æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾å¯¹æ¯”",
            x=0.5,
            font=dict(size=18, color='black')
        ),
        font=dict(family="Arial, sans-serif"),
        width=700,
        height=600,
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1,
            xanchor="left", 
            x=1.02
        )
    )
    
    return fig


def create_bar_chart(selected_models: List[str], selected_benchmarks: List[str]) -> go.Figure:
    """
    åˆ›å»ºæŸ±çŠ¶å›¾å±•ç¤ºå„æ¨¡å‹åœ¨ä¸åŒbenchmarksä¸Šçš„å¾—åˆ†
    
    Args:
        selected_models: é€‰æ‹©çš„æ¨¡å‹åˆ—è¡¨
        selected_benchmarks: é€‰æ‹©çš„benchmarkåˆ—è¡¨
    
    Returns:
        Plotly æŸ±çŠ¶å›¾
    """
    if not selected_models or not selected_benchmarks:
        fig = go.Figure()
        fig.add_annotation(
            text="è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªBenchmark",
            xref="paper", yref="paper",
            x=0.5, y=0.5, xanchor='center', yanchor='middle',
            font=dict(size=16, color="gray")
        )
        return fig
    
    fig = go.Figure()
    
    # ä¸ºæ¯ä¸ªé€‰ä¸­çš„æ¨¡å‹åˆ›å»ºæŸ±çŠ¶å›¾
    for i, model in enumerate(selected_models):
        if model in BENCHMARK_DATA:
            scores = []
            benchmarks = []
            
            for benchmark in selected_benchmarks:
                if benchmark in BENCHMARK_DATA[model]:
                    scores.append(BENCHMARK_DATA[model][benchmark])
                    benchmarks.append(benchmark)
            
            if scores:
                fig.add_trace(go.Bar(
                    x=benchmarks,
                    y=scores,
                    name=model,
                    marker_color=MODEL_COLORS[i % len(MODEL_COLORS)],
                    text=[f'{score:.1f}' for score in scores],
                    textposition='outside'
                ))
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        title=dict(
            text="Benchmark å¾—åˆ†å¯¹æ¯”",
            x=0.5,
            font=dict(size=18, color='black')
        ),
        xaxis_title="Benchmark",
        yaxis_title="å¾—åˆ†",
        barmode='group',
        width=800,
        height=500,
        font=dict(family="Arial, sans-serif"),
        yaxis=dict(range=[0, 100]),
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )
    
    return fig


def create_heatmap(selected_models: List[str], selected_benchmarks: List[str]) -> go.Figure:
    """
    åˆ›å»ºçƒ­åŠ›å›¾å±•ç¤ºæ¨¡å‹-benchmarkçŸ©é˜µ
    
    Args:
        selected_models: é€‰æ‹©çš„æ¨¡å‹åˆ—è¡¨
        selected_benchmarks: é€‰æ‹©çš„benchmarkåˆ—è¡¨
    
    Returns:
        Plotly çƒ­åŠ›å›¾
    """
    if not selected_models or not selected_benchmarks:
        fig = go.Figure()
        fig.add_annotation(
            text="è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªBenchmark",
            xref="paper", yref="paper",
            x=0.5, y=0.5, xanchor='center', yanchor='middle',
            font=dict(size=16, color="gray")
        )
        return fig
    
    # æ„å»ºå¾—åˆ†çŸ©é˜µ
    matrix = []
    model_labels = []
    
    for model in selected_models:
        if model in BENCHMARK_DATA:
            row = []
            for benchmark in selected_benchmarks:
                score = BENCHMARK_DATA[model].get(benchmark, 0)
                row.append(score)
            matrix.append(row)
            model_labels.append(model)
    
    if not matrix:
        fig = go.Figure()
        fig.add_annotation(
            text="æ²¡æœ‰å¯ç”¨çš„æ•°æ®",
            xref="paper", yref="paper",
            x=0.5, y=0.5, xanchor='center', yanchor='middle',
            font=dict(size=16, color="gray")
        )
        return fig
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    fig = go.Figure(data=go.Heatmap(
        z=matrix,
        x=selected_benchmarks,
        y=model_labels,
        colorscale='RdYlBu_r',
        text=[[f'{val:.1f}' for val in row] for row in matrix],
        texttemplate="%{text}",
        textfont={"size": 12},
        colorbar=dict(title="å¾—åˆ†")
    ))
    
    fig.update_layout(
        title=dict(
            text="æ¨¡å‹-Benchmark å¾—åˆ†çƒ­åŠ›å›¾",
            x=0.5,
            font=dict(size=18, color='black')
        ),
        width=700,
        height=400 + len(model_labels) * 30,
        font=dict(family="Arial, sans-serif")
    )
    
    return fig


def get_benchmark_details(selected_benchmark: str) -> str:
    """
    è·å–é€‰ä¸­benchmarkçš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        selected_benchmark: é€‰æ‹©çš„benchmarkåç§°
    
    Returns:
        Benchmarkè¯¦ç»†ä¿¡æ¯çš„Markdownæ–‡æœ¬
    """
    if not selected_benchmark or selected_benchmark not in BENCHMARK_INFO:
        return "è¯·é€‰æ‹©ä¸€ä¸ªBenchmarkæŸ¥çœ‹è¯¦æƒ…"
    
    info = BENCHMARK_INFO[selected_benchmark]
    
    # è·å–è¯¥benchmarkä¸Šæ‰€æœ‰æ¨¡å‹çš„æ’å
    scores = []
    for model, benchmarks in BENCHMARK_DATA.items():
        if selected_benchmark in benchmarks:
            scores.append((model, benchmarks[selected_benchmark]))
    
    # æŒ‰åˆ†æ•°æ’åº
    scores.sort(key=lambda x: x[1], reverse=True)
    
    # æ„å»ºè¯¦æƒ…æ–‡æœ¬
    details = f"## {selected_benchmark}\n\n"
    details += f"**æè¿°**: {info}\n\n"
    details += "### æ’å\n\n"
    details += "| æ’å | æ¨¡å‹ | å¾—åˆ† |\n"
    details += "|------|------|------|\n"
    
    for i, (model, score) in enumerate(scores, 1):
        details += f"| {i} | {model} | {score:.1f} |\n"
    
    return details


def create_benchmark_overview_table() -> pd.DataFrame:
    """
    åˆ›å»ºbenchmarkæ¦‚è§ˆè¡¨æ ¼
    
    Returns:
        åŒ…å«æ‰€æœ‰æ¨¡å‹å’Œbenchmarkçš„DataFrame
    """
    # è½¬æ¢æ•°æ®æ ¼å¼ä»¥é€‚é…è¡¨æ ¼å±•ç¤º
    rows = []
    for model, benchmarks in BENCHMARK_DATA.items():
        row = {'æ¨¡å‹': model}
        for benchmark in BENCHMARK_INFO.keys():
            score = benchmarks.get(benchmark, None)
            row[benchmark] = f"{score:.1f}" if score is not None else "-"
        rows.append(row)
    
    df = pd.DataFrame(rows)
    return df


def render():
    """
    æ¸²æŸ“ Benchmark Explorer é¡µé¢
    
    Returns:
        load äº‹ä»¶é…ç½® (å¦‚æœéœ€è¦)
    """
    with gr.Column():
        gr.HTML("""
        <div class="main-header">
            <h1 style="color: #1f2937; margin-bottom: 8px;">ğŸ† Benchmark Explorer</h1>
            <p style="color: #6b7280; font-size: 1.1rem; margin: 0;">ä¸»æµ LLM Benchmark å¯è§†åŒ–å¯¹æ¯”å·¥å…·</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # æ¨¡å‹é€‰æ‹©
                model_selector = gr.CheckboxGroup(
                    choices=list(BENCHMARK_DATA.keys()),
                    value=['GPT-4', 'Claude-3.5-Sonnet', 'Llama-3-70B'],
                    label="é€‰æ‹©æ¨¡å‹",
                    info="é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹"
                )
                
                # Benchmarké€‰æ‹©
                benchmark_selector = gr.CheckboxGroup(
                    choices=list(BENCHMARK_INFO.keys()),
                    value=['MMLU', 'HumanEval', 'GSM8K', 'HellaSwag'],
                    label="é€‰æ‹©Benchmark",
                    info="é€‰æ‹©è¦å¯¹æ¯”çš„è¯„æµ‹æŒ‡æ ‡"
                )
                
                # Benchmarkè¯¦æƒ…æŸ¥çœ‹
                benchmark_detail_selector = gr.Dropdown(
                    choices=list(BENCHMARK_INFO.keys()),
                    value='MMLU',
                    label="æŸ¥çœ‹Benchmarkè¯¦æƒ…",
                    info="é€‰æ‹©æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"
                )
            
            with gr.Column(scale=2):
                # å¯è§†åŒ–tabs
                with gr.Tabs():
                    with gr.Tab("é›·è¾¾å›¾å¯¹æ¯”"):
                        radar_plot = gr.Plot(label="é›·è¾¾å›¾")
                    
                    with gr.Tab("æŸ±çŠ¶å›¾å¯¹æ¯”"):  
                        bar_plot = gr.Plot(label="æŸ±çŠ¶å›¾")
                    
                    with gr.Tab("çƒ­åŠ›å›¾"):
                        heatmap_plot = gr.Plot(label="çƒ­åŠ›å›¾")
                    
                    with gr.Tab("æ•°æ®è¡¨æ ¼"):
                        overview_table = gr.Dataframe(
                            value=create_benchmark_overview_table(),
                            label="å®Œæ•´æ•°æ®è¡¨æ ¼",
                            interactive=False
                        )
        
        # Benchmarkè¯¦æƒ…å±•ç¤º
        with gr.Row():
            benchmark_details = gr.Markdown(
                value=get_benchmark_details('MMLU'),
                label="Benchmark è¯¦æƒ…"
            )
        
        # äº‹ä»¶ç»‘å®š
        def update_plots(selected_models, selected_benchmarks):
            """æ›´æ–°æ‰€æœ‰å›¾è¡¨"""
            radar = create_radar_chart(selected_models, selected_benchmarks)
            bar = create_bar_chart(selected_models, selected_benchmarks) 
            heatmap = create_heatmap(selected_models, selected_benchmarks)
            return radar, bar, heatmap
        
        # ç»‘å®šæ¨¡å‹å’Œbenchmarké€‰æ‹©å˜åŒ–äº‹ä»¶
        model_selector.change(
            fn=update_plots,
            inputs=[model_selector, benchmark_selector],
            outputs=[radar_plot, bar_plot, heatmap_plot]
        )
        
        benchmark_selector.change(
            fn=update_plots,
            inputs=[model_selector, benchmark_selector],
            outputs=[radar_plot, bar_plot, heatmap_plot]
        )
        
        # ç»‘å®šbenchmarkè¯¦æƒ…æŸ¥çœ‹
        benchmark_detail_selector.change(
            fn=get_benchmark_details,
            inputs=[benchmark_detail_selector],
            outputs=[benchmark_details]
        )
        
        # åˆå§‹åŒ–å›¾è¡¨
        initial_radar = create_radar_chart(['GPT-4', 'Claude-3.5-Sonnet', 'Llama-3-70B'], 
                                         ['MMLU', 'HumanEval', 'GSM8K', 'HellaSwag'])
        initial_bar = create_bar_chart(['GPT-4', 'Claude-3.5-Sonnet', 'Llama-3-70B'],
                                     ['MMLU', 'HumanEval', 'GSM8K', 'HellaSwag'])  
        initial_heatmap = create_heatmap(['GPT-4', 'Claude-3.5-Sonnet', 'Llama-3-70B'],
                                       ['MMLU', 'HumanEval', 'GSM8K', 'HellaSwag'])
        
        radar_plot.value = initial_radar
        bar_plot.value = initial_bar
        heatmap_plot.value = initial_heatmap


if __name__ == "__main__":
    # æµ‹è¯•é¡µé¢
    with gr.Blocks(title="Benchmark Explorer") as demo:
        render()
    
    demo.launch()
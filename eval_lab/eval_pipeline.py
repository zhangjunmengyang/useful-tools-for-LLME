"""
è‡ªåŠ¨åŒ–è¯„æµ‹Pipeline - æ‰¹é‡è¯„æµ‹å’Œç»“æœåˆ†æ

æä¾›å®Œæ•´çš„è¯„æµ‹æµæ°´çº¿åŠŸèƒ½ï¼š
- æ•°æ®é›†ä¸Šä¼ å’Œè§£æ
- å¤šæŒ‡æ ‡æ‰¹é‡è¯„æµ‹
- ç»“æœå¯è§†åŒ–åˆ†æ
- è¯„æµ‹æŠ¥å‘Šå¯¼å‡º
"""

import gradio as gr
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
import json
import io
import numpy as np
from eval_lab.eval_utils import (
    calculate_all_metrics, 
    format_metrics_table,
    generate_eval_report
)


# å†…ç½®ç¤ºä¾‹æ•°æ®é›†
SAMPLE_DATASETS = {
    'QAæ•°æ®é›†ç¤ºä¾‹': {
        'description': 'é—®ç­”ä»»åŠ¡ç¤ºä¾‹æ•°æ®ï¼ŒåŒ…å«é—®é¢˜ã€å‚è€ƒç­”æ¡ˆå’Œæ¨¡å‹é¢„æµ‹',
        'data': [
            {
                'question': 'Pythonä¸­å¦‚ä½•è¯»å–CSVæ–‡ä»¶ï¼Ÿ',
                'reference': 'ä½¿ç”¨pandasåº“çš„read_csv()å‡½æ•°å¯ä»¥æ–¹ä¾¿åœ°è¯»å–CSVæ–‡ä»¶ã€‚',
                'prediction': 'import pandas as pd\ndf = pd.read_csv("file.csv")\nå¯ä»¥ä½¿ç”¨pandasçš„read_csvå‡½æ•°è¯»å–CSVæ–‡ä»¶ã€‚'
            },
            {
                'question': 'ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
                'reference': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºé€šè¿‡æ•°æ®å­¦ä¹ æ¨¡å¼å’Œåšå‡ºé¢„æµ‹ã€‚',
                'prediction': 'æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹ï¼Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚'
            },
            {
                'question': 'å¦‚ä½•æé«˜ä»£ç è´¨é‡ï¼Ÿ',
                'reference': 'é€šè¿‡ä»£ç å®¡æŸ¥ã€å•å…ƒæµ‹è¯•ã€éµå¾ªç¼–ç è§„èŒƒã€é‡æ„ç­‰æ–¹å¼å¯ä»¥æé«˜ä»£ç è´¨é‡ã€‚',
                'prediction': 'å†™æ³¨é‡Šã€åšæµ‹è¯•ã€code reviewã€ç”¨linteræ£€æŸ¥æ ¼å¼ã€‚'
            },
            {
                'question': 'SQLä¸­çš„JOINæœ‰å“ªäº›ç±»å‹ï¼Ÿ',
                'reference': 'ä¸»è¦æœ‰INNER JOINã€LEFT JOINã€RIGHT JOINã€FULL OUTER JOINç­‰ç±»å‹ã€‚',
                'prediction': 'SQLçš„JOINåŒ…æ‹¬å†…è¿æ¥(INNER JOIN)ã€å·¦è¿æ¥(LEFT JOIN)ã€å³è¿æ¥(RIGHT JOIN)å’Œå…¨å¤–è¿æ¥(FULL OUTER JOIN)ã€‚'
            },
            {
                'question': 'ä»€ä¹ˆæ˜¯REST APIï¼Ÿ',
                'reference': 'RESTæ˜¯ä¸€ç§è½¯ä»¶æ¶æ„é£æ ¼ï¼Œç”¨äºè®¾è®¡ç½‘ç»œåº”ç”¨çš„APIï¼ŒåŸºäºHTTPåè®®ã€‚',
                'prediction': 'REST APIæ˜¯åŸºäºHTTPçš„WebæœåŠ¡æ¥å£ï¼Œä½¿ç”¨GET/POST/PUT/DELETEç­‰æ–¹æ³•è¿›è¡Œèµ„æºæ“ä½œã€‚'
            }
        ]
    },
    'ä»£ç ç”Ÿæˆç¤ºä¾‹': {
        'description': 'ä»£ç ç”Ÿæˆä»»åŠ¡ç¤ºä¾‹ï¼Œè¯„æµ‹ä»£ç çš„æ­£ç¡®æ€§å’Œè´¨é‡',
        'data': [
            {
                'question': 'å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹',
                'reference': 'def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)',
                'prediction': 'def fib(n):\n    if n < 2:\n        return n\n    a, b = 0, 1\n    for i in range(2, n+1):\n        a, b = b, a + b\n    return b'
            },
            {
                'question': 'å®ç°ä¸€ä¸ªå‡½æ•°åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ä¸ºå›æ–‡',
                'reference': 'def is_palindrome(s):\n    s = s.lower().replace(" ", "")\n    return s == s[::-1]',
                'prediction': 'def palindrome_check(text):\n    clean = "".join(text.split()).lower()\n    return clean == clean[::-1]'
            },
            {
                'question': 'å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•',
                'reference': 'def quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quicksort(left) + middle + quicksort(right)',
                'prediction': 'def quick_sort(array):\n    if len(array) < 2:\n        return array\n    pivot = array[0]\n    less = [i for i in array[1:] if i <= pivot]\n    greater = [i for i in array[1:] if i > pivot]\n    return quick_sort(less) + [pivot] + quick_sort(greater)'
            }
        ]
    },
    'ç¿»è¯‘è´¨é‡è¯„æµ‹': {
        'description': 'æœºå™¨ç¿»è¯‘è´¨é‡è¯„æµ‹ç¤ºä¾‹',
        'data': [
            {
                'question': 'Hello, how are you today?',
                'reference': 'ä½ å¥½ï¼Œä½ ä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ',
                'prediction': 'æ‚¨å¥½ï¼Œæ‚¨ä»Šå¤©å¥½å—ï¼Ÿ'
            },
            {
                'question': 'The weather is very nice today.',
                'reference': 'ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚',
                'prediction': 'ä»Šå¤©çš„å¤©æ°”éå¸¸ä¸é”™ã€‚'
            },
            {
                'question': 'I love programming and artificial intelligence.',
                'reference': 'æˆ‘å–œæ¬¢ç¼–ç¨‹å’Œäººå·¥æ™ºèƒ½ã€‚',
                'prediction': 'æˆ‘çƒ­çˆ±ç¼–ç¨‹å’ŒAIæŠ€æœ¯ã€‚'
            }
        ]
    }
}

# æ¨¡å‹é…ç½®
MOCK_MODELS = ['GPT-4', 'Claude-3.5', 'GPT-3.5', 'Llama-3-70B', 'Qwen-72B']


def parse_uploaded_file(file_content: str, file_type: str) -> List[Dict[str, str]]:
    """
    è§£æä¸Šä¼ çš„æ–‡ä»¶å†…å®¹
    
    Args:
        file_content: æ–‡ä»¶å†…å®¹
        file_type: æ–‡ä»¶ç±»å‹ ('json' æˆ– 'csv')
    
    Returns:
        è§£æåçš„æ•°æ®åˆ—è¡¨
    """
    try:
        if file_type == 'json':
            data = json.loads(file_content)
            if isinstance(data, list):
                return data
            else:
                return [data]
        elif file_type == 'csv':
            # ä½¿ç”¨pandasè§£æCSV
            df = pd.read_csv(io.StringIO(file_content))
            return df.to_dict('records')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
    except Exception as e:
        raise ValueError(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")


def validate_dataset(data: List[Dict[str, str]]) -> Tuple[bool, str]:
    """
    éªŒè¯æ•°æ®é›†æ ¼å¼
    
    Args:
        data: æ•°æ®é›†
    
    Returns:
        (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
    """
    if not data:
        return False, "æ•°æ®é›†ä¸ºç©º"
    
    required_fields = {'question', 'reference', 'prediction'}
    
    for i, item in enumerate(data):
        if not isinstance(item, dict):
            return False, f"ç¬¬{i+1}è¡Œæ•°æ®æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºå­—å…¸æ ¼å¼"
        
        missing_fields = required_fields - set(item.keys())
        if missing_fields:
            return False, f"ç¬¬{i+1}è¡Œç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_fields)}"
        
        # æ£€æŸ¥å­—æ®µæ˜¯å¦ä¸ºå­—ç¬¦ä¸²
        for field in required_fields:
            if not isinstance(item[field], str):
                return False, f"ç¬¬{i+1}è¡Œ{field}å­—æ®µåº”ä¸ºå­—ç¬¦ä¸²"
    
    return True, ""


def run_batch_evaluation(data: List[Dict[str, str]], selected_metrics: List[str]) -> Dict[str, Any]:
    """
    è¿è¡Œæ‰¹é‡è¯„æµ‹
    
    Args:
        data: è¯„æµ‹æ•°æ®
        selected_metrics: é€‰æ‹©çš„è¯„æµ‹æŒ‡æ ‡
    
    Returns:
        è¯„æµ‹ç»“æœ
    """
    if not data:
        return {'error': 'æ²¡æœ‰æ•°æ®è¿›è¡Œè¯„æµ‹'}
    
    # æå–é¢„æµ‹å’Œå‚è€ƒç­”æ¡ˆ
    predictions = [item['prediction'] for item in data]
    references = [item['reference'] for item in data]
    
    # è®¡ç®—æŒ‡æ ‡
    try:
        all_metrics = calculate_all_metrics(predictions, references)
        
        # è¿‡æ»¤é€‰æ‹©çš„æŒ‡æ ‡
        if selected_metrics:
            filtered_metrics = {k: v for k, v in all_metrics.items() if k in selected_metrics}
        else:
            filtered_metrics = all_metrics
        
        # æ ·æœ¬çº§åˆ«çš„è¯¦ç»†ç»“æœ
        sample_results = []
        for i, (pred, ref) in enumerate(zip(predictions, references)):
            sample_metrics = calculate_all_metrics([pred], [ref])
            sample_results.append({
                'index': i,
                'question': data[i]['question'],
                'prediction': pred,
                'reference': ref,
                'metrics': sample_metrics
            })
        
        return {
            'overall_metrics': filtered_metrics,
            'sample_results': sample_results,
            'total_samples': len(data)
        }
    
    except Exception as e:
        return {'error': f'è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}'}


def create_metrics_comparison_chart(results_dict: Dict[str, Dict[str, float]]) -> go.Figure:
    """
    åˆ›å»ºå¤šä¸ªè¯„æµ‹ç»“æœçš„å¯¹æ¯”å›¾è¡¨
    
    Args:
        results_dict: {æ¨¡å‹åç§°: æŒ‡æ ‡ç»“æœ} çš„å­—å…¸
    
    Returns:
        Plotlyå›¾è¡¨
    """
    if not results_dict:
        fig = go.Figure()
        fig.add_annotation(text="æ²¡æœ‰æ•°æ®", x=0.5, y=0.5, showarrow=False)
        return fig
    
    # æå–æ‰€æœ‰æŒ‡æ ‡åç§°
    all_metrics = set()
    for metrics in results_dict.values():
        all_metrics.update(metrics.keys())
    all_metrics = sorted(list(all_metrics))
    
    fig = go.Figure()
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹æ·»åŠ æŸ±çŠ¶å›¾
    colors = px.colors.qualitative.Set3
    for i, (model_name, metrics) in enumerate(results_dict.items()):
        scores = [metrics.get(metric, 0) for metric in all_metrics]
        
        fig.add_trace(go.Bar(
            name=model_name,
            x=all_metrics,
            y=scores,
            marker_color=colors[i % len(colors)],
            text=[f'{score:.3f}' for score in scores],
            textposition='outside'
        ))
    
    fig.update_layout(
        title="å¤šæ¨¡å‹è¯„æµ‹ç»“æœå¯¹æ¯”",
        xaxis_title="è¯„æµ‹æŒ‡æ ‡",
        yaxis_title="åˆ†æ•°",
        barmode='group',
        height=500,
        yaxis=dict(range=[0, 1])
    )
    
    return fig


def create_metrics_radar_chart(metrics: Dict[str, float]) -> go.Figure:
    """
    åˆ›å»ºå•ä¸ªè¯„æµ‹ç»“æœçš„é›·è¾¾å›¾
    
    Args:
        metrics: æŒ‡æ ‡ç»“æœ
    
    Returns:
        Plotlyé›·è¾¾å›¾
    """
    if not metrics:
        fig = go.Figure()
        fig.add_annotation(text="æ²¡æœ‰æ•°æ®", x=0.5, y=0.5, showarrow=False)
        return fig
    
    categories = list(metrics.keys())
    values = list(metrics.values())
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=values + [values[0]],
        theta=categories + [categories[0]],
        fill='toself',
        name='è¯„æµ‹ç»“æœ',
        line=dict(color='#1f77b4', width=2),
        fillcolor='#1f77b4',
        opacity=0.3
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1],
                tickfont=dict(size=10)
            )
        ),
        showlegend=False,
        title=dict(text="è¯„æµ‹æŒ‡æ ‡é›·è¾¾å›¾", x=0.5, font=dict(size=16)),
        width=500,
        height=500
    )
    
    return fig


def create_sample_analysis_chart(sample_results: List[Dict]) -> go.Figure:
    """
    åˆ›å»ºæ ·æœ¬çº§åˆ«çš„åˆ†æå›¾è¡¨
    
    Args:
        sample_results: æ ·æœ¬ç»“æœåˆ—è¡¨
    
    Returns:
        Plotlyå›¾è¡¨
    """
    if not sample_results:
        fig = go.Figure()
        fig.add_annotation(text="æ²¡æœ‰æ ·æœ¬æ•°æ®", x=0.5, y=0.5, showarrow=False)
        return fig
    
    # æå–æ¯ä¸ªæ ·æœ¬çš„F1åˆ†æ•°ç”¨äºå±•ç¤º
    indices = [res['index'] + 1 for res in sample_results]  # 1-based indexing
    f1_scores = [res['metrics'].get('f1', 0) for res in sample_results]
    
    fig = go.Figure()
    
    # æ·»åŠ æ•£ç‚¹å›¾
    fig.add_trace(go.Scatter(
        x=indices,
        y=f1_scores,
        mode='markers+lines',
        marker=dict(
            size=8,
            color=f1_scores,
            colorscale='RdYlBu_r',
            colorbar=dict(title="F1 Score"),
            line=dict(width=1, color='white')
        ),
        line=dict(width=2, color='gray'),
        name='F1 Score',
        text=[f"æ ·æœ¬{i}: {score:.3f}" for i, score in zip(indices, f1_scores)],
        hovertemplate='<b>æ ·æœ¬ %{x}</b><br>F1 Score: %{y:.3f}<extra></extra>'
    ))
    
    # æ·»åŠ å¹³å‡çº¿
    avg_f1 = np.mean(f1_scores)
    fig.add_hline(
        y=avg_f1,
        line_dash="dash",
        line_color="red",
        annotation_text=f"å¹³å‡ F1: {avg_f1:.3f}",
        annotation_position="top right"
    )
    
    fig.update_layout(
        title="æ ·æœ¬çº§åˆ«è¯„æµ‹ç»“æœåˆ†æ",
        xaxis_title="æ ·æœ¬ç¼–å·",
        yaxis_title="F1 Score",
        height=400,
        yaxis=dict(range=[0, 1])
    )
    
    return fig


def format_sample_results_table(sample_results: List[Dict]) -> pd.DataFrame:
    """
    æ ¼å¼åŒ–æ ·æœ¬ç»“æœä¸ºè¡¨æ ¼
    
    Args:
        sample_results: æ ·æœ¬ç»“æœåˆ—è¡¨
    
    Returns:
        DataFrame
    """
    if not sample_results:
        return pd.DataFrame()
    
    rows = []
    for res in sample_results:
        row = {
            'æ ·æœ¬': res['index'] + 1,
            'é—®é¢˜': res['question'][:50] + '...' if len(res['question']) > 50 else res['question'],
            'F1': f"{res['metrics'].get('f1', 0):.3f}",
            'BLEU': f"{res['metrics'].get('bleu', 0):.3f}",
            'ROUGE-L': f"{res['metrics'].get('rouge_l', 0):.3f}",
            'ç²¾ç¡®åŒ¹é…': f"{res['metrics'].get('exact_match', 0):.3f}"
        }
        rows.append(row)
    
    return pd.DataFrame(rows)


def render():
    """
    æ¸²æŸ“è‡ªåŠ¨åŒ–è¯„æµ‹Pipelineé¡µé¢
    
    Returns:
        load äº‹ä»¶é…ç½® (å¦‚æœéœ€è¦)
    """
    with gr.Column():
        gr.HTML("""
        <div class="main-header">
            <h1 style="color: #1f2937; margin-bottom: 8px;">ğŸ”„ è‡ªåŠ¨åŒ–è¯„æµ‹Pipeline</h1>
            <p style="color: #6b7280; font-size: 1.1rem; margin: 0;">æ‰¹é‡è¯„æµ‹ã€ç»“æœåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ</p>
        </div>
        """)
        
        with gr.Tabs():
            # Tab 1: æ•°æ®å‡†å¤‡
            with gr.Tab("æ•°æ®å‡†å¤‡"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æ•°æ®è¾“å…¥")
                        
                        # æ•°æ®æ¥æºé€‰æ‹©
                        data_source = gr.Radio(
                            choices=["ä¸Šä¼ æ–‡ä»¶", "ä½¿ç”¨ç¤ºä¾‹æ•°æ®"],
                            value="ä½¿ç”¨ç¤ºä¾‹æ•°æ®",
                            label="æ•°æ®æ¥æº"
                        )
                        
                        # æ–‡ä»¶ä¸Šä¼ 
                        file_upload = gr.File(
                            label="ä¸Šä¼ æ•°æ®é›†æ–‡ä»¶ (JSON/CSV)",
                            file_types=[".json", ".csv"],
                            visible=False
                        )
                        
                        # ç¤ºä¾‹æ•°æ®é€‰æ‹©
                        sample_dataset_selector = gr.Dropdown(
                            choices=list(SAMPLE_DATASETS.keys()),
                            value=list(SAMPLE_DATASETS.keys())[0],
                            label="é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†"
                        )
                        
                        load_data_btn = gr.Button("åŠ è½½æ•°æ®", variant="primary")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### æ•°æ®æ ¼å¼è¯´æ˜")
                        gr.Markdown("""
                        **JSONæ ¼å¼ç¤ºä¾‹:**
                        ```json
                        [
                            {
                                "question": "é—®é¢˜å†…å®¹",
                                "reference": "å‚è€ƒç­”æ¡ˆ",
                                "prediction": "æ¨¡å‹é¢„æµ‹"
                            }
                        ]
                        ```
                        
                        **CSVæ ¼å¼ç¤ºä¾‹:**
                        ```csv
                        question,reference,prediction
                        é—®é¢˜1,å‚è€ƒç­”æ¡ˆ1,æ¨¡å‹é¢„æµ‹1
                        é—®é¢˜2,å‚è€ƒç­”æ¡ˆ2,æ¨¡å‹é¢„æµ‹2
                        ```
                        """)
                
                # æ•°æ®é¢„è§ˆ
                with gr.Row():
                    data_preview = gr.Dataframe(
                        label="æ•°æ®é¢„è§ˆ",
                        interactive=False
                    )
                
                # æ•°æ®ç»Ÿè®¡
                data_status = gr.Textbox(
                    label="æ•°æ®çŠ¶æ€",
                    interactive=False,
                    value="è¯·å…ˆåŠ è½½æ•°æ®"
                )
            
            # Tab 2: é…ç½®è¯„æµ‹
            with gr.Tab("é…ç½®è¯„æµ‹"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### è¯„æµ‹é…ç½®")
                        
                        # æŒ‡æ ‡é€‰æ‹©
                        metrics_selector = gr.CheckboxGroup(
                            choices=[
                                ('Exact Match', 'exact_match'),
                                ('F1 Score', 'f1'),
                                ('BLEU', 'bleu'), 
                                ('ROUGE-L', 'rouge_l'),
                                ('BERTScore F1', 'bertscore_f1')
                            ],
                            value=['exact_match', 'f1', 'bleu'],
                            label="é€‰æ‹©è¯„æµ‹æŒ‡æ ‡"
                        )
                        
                        # æ¨¡å‹åç§°è®¾ç½®
                        model_name_input = gr.Textbox(
                            value="GPT-4",
                            label="æ¨¡å‹åç§°",
                            info="ç”¨äºç”ŸæˆæŠ¥å‘Š"
                        )
                        
                        dataset_name_input = gr.Textbox(
                            value="è‡ªå®šä¹‰æ•°æ®é›†",
                            label="æ•°æ®é›†åç§°",
                            info="ç”¨äºç”ŸæˆæŠ¥å‘Š"
                        )
                        
                        run_eval_btn = gr.Button("å¼€å§‹è¯„æµ‹", variant="primary")
                    
                    with gr.Column():
                        gr.Markdown("### æŒ‡æ ‡è¯´æ˜")
                        gr.Markdown("""
                        - **Exact Match**: é¢„æµ‹ä¸å‚è€ƒå®Œå…¨åŒ¹é…çš„æ¯”ä¾‹
                        - **F1 Score**: åŸºäºtokençš„ç²¾ç¡®ç‡å’Œå¬å›ç‡è°ƒå’Œå¹³å‡
                        - **BLEU**: æœºå™¨ç¿»è¯‘è´¨é‡æŒ‡æ ‡ï¼ŒåŸºäºn-gramåŒ¹é…
                        - **ROUGE-L**: åŸºäºæœ€é•¿å…¬å…±å­åºåˆ—çš„æ–‡æœ¬ç›¸ä¼¼åº¦
                        - **BERTScore F1**: åŸºäºè¯­ä¹‰ç†è§£çš„ç›¸ä¼¼åº¦è¯„åˆ†
                        """)
            
            # Tab 3: ç»“æœåˆ†æ
            with gr.Tab("ç»“æœåˆ†æ"):
                with gr.Tabs():
                    with gr.Tab("æ•´ä½“æŒ‡æ ‡"):
                        with gr.Row():
                            with gr.Column():
                                # æ•´ä½“æŒ‡æ ‡è¡¨æ ¼
                                overall_metrics_table = gr.Markdown(
                                    value="è¯·å…ˆå®Œæˆè¯„æµ‹",
                                    label="æ•´ä½“è¯„æµ‹ç»“æœ"
                                )
                            
                            with gr.Column():
                                # é›·è¾¾å›¾
                                metrics_radar_chart = gr.Plot(label="æŒ‡æ ‡é›·è¾¾å›¾")
                    
                    with gr.Tab("æ ·æœ¬åˆ†æ"):
                        # æ ·æœ¬çº§åˆ«åˆ†æå›¾è¡¨
                        sample_analysis_chart = gr.Plot(label="æ ·æœ¬åˆ†æ")
                        
                        # è¯¦ç»†æ ·æœ¬ç»“æœè¡¨æ ¼
                        sample_results_table = gr.Dataframe(
                            label="æ ·æœ¬è¯¦ç»†ç»“æœ",
                            interactive=False
                        )
                    
                    with gr.Tab("å¤šæ¨¡å‹å¯¹æ¯”"):
                        gr.Markdown("""
                        ### å¤šæ¨¡å‹å¯¹æ¯”åŠŸèƒ½
                        
                        æ­¤åŠŸèƒ½ç”¨äºå¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨åŒä¸€æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚
                        åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨å¯ä»¥ï¼š
                        1. åˆ†åˆ«è¯„æµ‹ä¸åŒæ¨¡å‹çš„ç»“æœ
                        2. ä¿å­˜å„è‡ªçš„è¯„æµ‹ç»“æœ
                        3. åœ¨æ­¤è¿›è¡Œå¯è§†åŒ–å¯¹æ¯”
                        """)
                        
                        # é¢„ç½®ä¸€äº›å¯¹æ¯”æ•°æ®ç”¨äºæ¼”ç¤º
                        comparison_chart = gr.Plot(
                            value=create_metrics_comparison_chart({
                                'GPT-4': {'exact_match': 0.45, 'f1': 0.78, 'bleu': 0.52},
                                'Claude-3.5': {'exact_match': 0.38, 'f1': 0.82, 'bleu': 0.48},
                                'GPT-3.5': {'exact_match': 0.32, 'f1': 0.69, 'bleu': 0.41}
                            }),
                            label="æ¨¡å‹å¯¹æ¯”"
                        )
            
            # Tab 4: å¯¼å‡ºæŠ¥å‘Š
            with gr.Tab("å¯¼å‡ºæŠ¥å‘Š"):
                gr.Markdown("### è¯„æµ‹æŠ¥å‘Šå¯¼å‡º")
                
                export_format = gr.Radio(
                    choices=["Markdown", "JSON"],
                    value="Markdown",
                    label="å¯¼å‡ºæ ¼å¼"
                )
                
                generate_report_btn = gr.Button("ç”ŸæˆæŠ¥å‘Š", variant="primary")
                
                # æŠ¥å‘Šé¢„è§ˆ
                report_preview = gr.Textbox(
                    label="æŠ¥å‘Šé¢„è§ˆ",
                    lines=15,
                    interactive=False
                )
                
                # ä¸‹è½½é“¾æ¥
                report_download = gr.File(label="ä¸‹è½½æŠ¥å‘Š")
        
        # å­˜å‚¨è¯„æµ‹æ•°æ®çš„çŠ¶æ€å˜é‡
        eval_data_state = gr.State(value=[])
        eval_results_state = gr.State(value={})
        
        # äº‹ä»¶å¤„ç†å‡½æ•°
        def toggle_data_source(source):
            """åˆ‡æ¢æ•°æ®æ¥æºæ˜¾ç¤º"""
            if source == "ä¸Šä¼ æ–‡ä»¶":
                return gr.update(visible=True), gr.update(visible=False)
            else:
                return gr.update(visible=False), gr.update(visible=True)
        
        def load_sample_data(dataset_name):
            """åŠ è½½ç¤ºä¾‹æ•°æ®"""
            if dataset_name in SAMPLE_DATASETS:
                data = SAMPLE_DATASETS[dataset_name]['data']
                df = pd.DataFrame(data)
                status = f"âœ… å·²åŠ è½½ {len(data)} æ¡æ ·æœ¬æ•°æ®"
                return df, status, data
            else:
                return pd.DataFrame(), "âŒ æ•°æ®é›†ä¸å­˜åœ¨", []
        
        def load_uploaded_data(file):
            """åŠ è½½ä¸Šä¼ çš„æ–‡ä»¶æ•°æ®"""
            if file is None:
                return pd.DataFrame(), "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶", []
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                content = file.read().decode('utf-8')
                file_ext = file.name.split('.')[-1].lower()
                
                # è§£ææ–‡ä»¶
                data = parse_uploaded_file(content, file_ext)
                
                # éªŒè¯æ•°æ®æ ¼å¼
                is_valid, error_msg = validate_dataset(data)
                if not is_valid:
                    return pd.DataFrame(), f"âŒ {error_msg}", []
                
                df = pd.DataFrame(data)
                status = f"âœ… å·²åŠ è½½ {len(data)} æ¡æ ·æœ¬æ•°æ®"
                return df, status, data
                
            except Exception as e:
                return pd.DataFrame(), f"âŒ åŠ è½½å¤±è´¥: {str(e)}", []
        
        def run_evaluation(data, selected_metrics, model_name, dataset_name):
            """è¿è¡Œè¯„æµ‹"""
            if not data:
                return "âŒ è¯·å…ˆåŠ è½½æ•°æ®", gr.update(), gr.update(), gr.update(), {}
            
            if not selected_metrics:
                return "âŒ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¯„æµ‹æŒ‡æ ‡", gr.update(), gr.update(), gr.update(), {}
            
            # è¿è¡Œè¯„æµ‹
            results = run_batch_evaluation(data, selected_metrics)
            
            if 'error' in results:
                return f"âŒ {results['error']}", gr.update(), gr.update(), gr.update(), {}
            
            # æ ¼å¼åŒ–æ•´ä½“æŒ‡æ ‡è¡¨æ ¼
            metrics_table = format_metrics_table(results['overall_metrics'])
            
            # åˆ›å»ºé›·è¾¾å›¾
            radar_chart = create_metrics_radar_chart(results['overall_metrics'])
            
            # åˆ›å»ºæ ·æœ¬åˆ†æå›¾
            sample_chart = create_sample_analysis_chart(results['sample_results'])
            
            # æ ¼å¼åŒ–æ ·æœ¬ç»“æœè¡¨æ ¼
            sample_table = format_sample_results_table(results['sample_results'])
            
            status = f"âœ… è¯„æµ‹å®Œæˆï¼Œå…±å¤„ç† {results['total_samples']} ä¸ªæ ·æœ¬"
            
            # ä¿å­˜ç»“æœç”¨äºç”ŸæˆæŠ¥å‘Š
            report_results = {
                'model_name': model_name,
                'dataset_name': dataset_name,
                'metrics': results['overall_metrics'],
                'sample_results': results['sample_results']
            }
            
            return status, metrics_table, radar_chart, sample_chart, sample_table, report_results
        
        def generate_report(results, export_format):
            """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
            if not results:
                return "âŒ è¯·å…ˆå®Œæˆè¯„æµ‹", None
            
            try:
                # å‡†å¤‡æ ·æœ¬é¢„æµ‹æ•°æ®
                sample_predictions = []
                for res in results.get('sample_results', [])[:3]:  # åªå–å‰3ä¸ªæ ·æœ¬
                    sample_predictions.append((
                        res['question'],
                        res['reference'],
                        res['prediction']
                    ))
                
                # ç”ŸæˆæŠ¥å‘Š
                if export_format == "Markdown":
                    report_content = generate_eval_report(
                        results['dataset_name'],
                        results['model_name'],
                        results['metrics'],
                        sample_predictions
                    )
                    
                    # ä¿å­˜ä¸ºæ–‡ä»¶
                    report_file = f"eval_report_{results['model_name']}_{results['dataset_name']}.md"
                    with open(report_file, 'w', encoding='utf-8') as f:
                        f.write(report_content)
                    
                    return report_content, report_file
                
                else:  # JSONæ ¼å¼
                    report_data = {
                        'model_name': results['model_name'],
                        'dataset_name': results['dataset_name'],
                        'overall_metrics': results['metrics'],
                        'total_samples': len(results.get('sample_results', [])),
                        'sample_results': results.get('sample_results', [])
                    }
                    
                    report_content = json.dumps(report_data, ensure_ascii=False, indent=2)
                    
                    # ä¿å­˜ä¸ºæ–‡ä»¶
                    report_file = f"eval_report_{results['model_name']}_{results['dataset_name']}.json"
                    with open(report_file, 'w', encoding='utf-8') as f:
                        f.write(report_content)
                    
                    return report_content, report_file
                    
            except Exception as e:
                return f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}", None
        
        # äº‹ä»¶ç»‘å®š
        data_source.change(
            fn=toggle_data_source,
            inputs=[data_source],
            outputs=[file_upload, sample_dataset_selector]
        )
        
        load_data_btn.click(
            fn=lambda source, dataset_name, file: load_sample_data(dataset_name) if source == "ä½¿ç”¨ç¤ºä¾‹æ•°æ®" else load_uploaded_data(file),
            inputs=[data_source, sample_dataset_selector, file_upload],
            outputs=[data_preview, data_status, eval_data_state]
        )
        
        run_eval_btn.click(
            fn=run_evaluation,
            inputs=[eval_data_state, metrics_selector, model_name_input, dataset_name_input],
            outputs=[
                data_status, overall_metrics_table, metrics_radar_chart,
                sample_analysis_chart, sample_results_table, eval_results_state
            ]
        )
        
        generate_report_btn.click(
            fn=generate_report,
            inputs=[eval_results_state, export_format],
            outputs=[report_preview, report_download]
        )
        
        # åˆå§‹åŒ–åŠ è½½ç¤ºä¾‹æ•°æ®
        initial_data = SAMPLE_DATASETS[list(SAMPLE_DATASETS.keys())[0]]['data']
        initial_df = pd.DataFrame(initial_data)
        initial_status = f"âœ… å·²åŠ è½½ {len(initial_data)} æ¡æ ·æœ¬æ•°æ®"
        
        data_preview.value = initial_df
        data_status.value = initial_status
        eval_data_state.value = initial_data


if __name__ == "__main__":
    # æµ‹è¯•é¡µé¢
    with gr.Blocks(title="Eval Pipeline") as demo:
        render()
    
    demo.launch()
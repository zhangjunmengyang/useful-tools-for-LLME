"""
Dataset é€è§†é•œ - HuggingFace Dataset æµå¼é¢„è§ˆä¸åˆ†æ
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from typing import List, Dict, Any, Optional
import json


def analyze_field_content(values: List[Any], field_name: str) -> Dict:
    """åˆ†æå­—æ®µå†…å®¹çš„ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'name': field_name,
        'total': len(values),
        'null_count': 0,
        'unique_count': 0,
        'avg_length': 0,
        'min_length': 0,
        'max_length': 0,
        'type': 'unknown'
    }
    
    # è¿‡æ»¤ç©ºå€¼
    non_null = [v for v in values if v is not None and v != '']
    stats['null_count'] = len(values) - len(non_null)
    
    if not non_null:
        return stats
    
    # åˆ¤æ–­ç±»å‹
    first_val = non_null[0]
    if isinstance(first_val, str):
        stats['type'] = 'string'
        lengths = [len(str(v)) for v in non_null]
        stats['avg_length'] = np.mean(lengths)
        stats['min_length'] = np.min(lengths)
        stats['max_length'] = np.max(lengths)
        # å”¯ä¸€å€¼æ•°é‡ï¼ˆé™åˆ¶è®¡ç®—é‡ï¼‰
        if len(non_null) <= 1000:
            stats['unique_count'] = len(set(non_null))
    elif isinstance(first_val, (int, float)):
        stats['type'] = 'number'
        stats['avg_length'] = np.mean(non_null)
        stats['min_length'] = np.min(non_null)
        stats['max_length'] = np.max(non_null)
    elif isinstance(first_val, list):
        stats['type'] = 'list'
        lengths = [len(v) for v in non_null]
        stats['avg_length'] = np.mean(lengths)
        stats['min_length'] = np.min(lengths)
        stats['max_length'] = np.max(lengths)
    elif isinstance(first_val, dict):
        stats['type'] = 'dict'
        stats['unique_count'] = len(non_null)
    
    return stats


def render_length_distribution(df: pd.DataFrame, field: str) -> go.Figure:
    """æ¸²æŸ“å­—æ®µé•¿åº¦åˆ†å¸ƒç›´æ–¹å›¾"""
    lengths = df[field].astype(str).str.len()
    
    fig = go.Figure(data=go.Histogram(
        x=lengths,
        nbinsx=30,
        marker_color='#2563EB',
        opacity=0.8
    ))
    
    fig.add_vline(x=lengths.mean(), line_dash="dash", line_color="#DC2626",
                  annotation_text=f"å¹³å‡: {lengths.mean():.0f}")
    fig.add_vline(x=lengths.median(), line_dash="dot", line_color="#059669",
                  annotation_text=f"ä¸­ä½æ•°: {lengths.median():.0f}")
    
    fig.update_layout(
        title=f"'{field}' é•¿åº¦åˆ†å¸ƒ",
        xaxis_title="å­—ç¬¦é•¿åº¦",
        yaxis_title="æ ·æœ¬æ•°",
        height=350,
        margin=dict(l=50, r=50, t=50, b=50)
    )
    
    return fig


def render_word_count_distribution(df: pd.DataFrame, field: str) -> go.Figure:
    """æ¸²æŸ“å­—æ®µè¯æ•°åˆ†å¸ƒ"""
    word_counts = df[field].astype(str).str.split().str.len()
    
    fig = go.Figure(data=go.Histogram(
        x=word_counts,
        nbinsx=30,
        marker_color='#7C3AED',
        opacity=0.8
    ))
    
    fig.add_vline(x=word_counts.mean(), line_dash="dash", line_color="#DC2626",
                  annotation_text=f"å¹³å‡: {word_counts.mean():.0f}")
    
    fig.update_layout(
        title=f"'{field}' è¯æ•°åˆ†å¸ƒ (ç©ºæ ¼åˆ†è¯)",
        xaxis_title="è¯æ•°",
        yaxis_title="æ ·æœ¬æ•°",
        height=350,
        margin=dict(l=50, r=50, t=50, b=50)
    )
    
    return fig


def render_field_stats_chart(field_stats: List[Dict]) -> go.Figure:
    """æ¸²æŸ“å­—æ®µç»Ÿè®¡æ±‡æ€»å›¾"""
    string_fields = [f for f in field_stats if f['type'] == 'string']
    
    if not string_fields:
        return None
    
    names = [f['name'] for f in string_fields]
    avg_lengths = [f['avg_length'] for f in string_fields]
    null_rates = [f['null_count'] / f['total'] * 100 for f in string_fields]
    
    fig = go.Figure()
    
    fig.add_trace(go.Bar(
        name='å¹³å‡é•¿åº¦',
        x=names,
        y=avg_lengths,
        marker_color='#2563EB',
        yaxis='y',
        offsetgroup=0
    ))
    
    fig.add_trace(go.Bar(
        name='ç©ºå€¼ç‡ (%)',
        x=names,
        y=null_rates,
        marker_color='#DC2626',
        yaxis='y2',
        offsetgroup=1
    ))
    
    fig.update_layout(
        title="å­—æ®µç»Ÿè®¡æ±‡æ€»",
        xaxis_title="å­—æ®µ",
        yaxis=dict(title="å¹³å‡é•¿åº¦ (å­—ç¬¦)", side='left'),
        yaxis2=dict(title="ç©ºå€¼ç‡ (%)", side='right', overlaying='y', range=[0, 100]),
        barmode='group',
        height=400,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    return fig


def check_data_quality(df: pd.DataFrame) -> Dict:
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    quality = {
        'total_samples': len(df),
        'duplicate_count': df.duplicated().sum(),
        'fields': {}
    }
    
    for col in df.columns:
        col_quality = {
            'null_count': df[col].isna().sum() + (df[col].astype(str) == '').sum(),
            'null_rate': 0,
            'empty_string_count': (df[col].astype(str).str.strip() == '').sum()
        }
        col_quality['null_rate'] = col_quality['null_count'] / len(df) * 100
        
        # å¯¹æ–‡æœ¬å­—æ®µæ£€æŸ¥æ›´å¤š
        if df[col].dtype == 'object':
            col_quality['very_short'] = (df[col].astype(str).str.len() < 10).sum()
            col_quality['very_long'] = (df[col].astype(str).str.len() > 10000).sum()
        
        quality['fields'][col] = col_quality
    
    return quality


def render_quality_report(quality: Dict) -> None:
    """æ¸²æŸ“æ•°æ®è´¨é‡æŠ¥å‘Š"""
    st.markdown("### ğŸ“‹ æ•°æ®è´¨é‡æŠ¥å‘Š")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æ€»æ ·æœ¬æ•°", quality['total_samples'])
    with col2:
        dup_rate = quality['duplicate_count'] / quality['total_samples'] * 100
        st.metric("é‡å¤æ ·æœ¬", f"{quality['duplicate_count']} ({dup_rate:.1f}%)")
    with col3:
        # è®¡ç®—æ•´ä½“å¥åº·åº¦
        total_issues = quality['duplicate_count']
        for field_quality in quality['fields'].values():
            total_issues += field_quality['null_count']
        health_score = max(0, 100 - (total_issues / quality['total_samples'] * 10))
        st.metric("å¥åº·åº¦è¯„åˆ†", f"{health_score:.0f}/100")
    
    # å­—æ®µè´¨é‡è¯¦æƒ…
    st.markdown("#### å­—æ®µè´¨é‡è¯¦æƒ…")
    
    quality_data = []
    for field, field_quality in quality['fields'].items():
        row = {
            'å­—æ®µ': field,
            'ç©ºå€¼æ•°': field_quality['null_count'],
            'ç©ºå€¼ç‡': f"{field_quality['null_rate']:.1f}%",
            'ç©ºå­—ç¬¦ä¸²': field_quality['empty_string_count']
        }
        if 'very_short' in field_quality:
            row['è¿‡çŸ­ (<10å­—ç¬¦)'] = field_quality['very_short']
        if 'very_long' in field_quality:
            row['è¿‡é•¿ (>10kå­—ç¬¦)'] = field_quality['very_long']
        quality_data.append(row)
    
    st.dataframe(pd.DataFrame(quality_data), width='stretch', hide_index=True)


def render_sample_viewer(samples: List[Dict], fields: List[str]) -> None:
    """æ¸²æŸ“æ ·æœ¬æŸ¥çœ‹å™¨"""
    st.markdown("### ğŸ” æ ·æœ¬æŸ¥çœ‹å™¨")
    
    # æ ·æœ¬é€‰æ‹©
    sample_idx = st.slider("é€‰æ‹©æ ·æœ¬", 0, len(samples) - 1, 0)
    sample = samples[sample_idx]
    
    # æ˜¾ç¤ºæ ·æœ¬
    for field in fields:
        if field in sample:
            value = sample[field]
            
            # æ ¹æ®ç±»å‹æ˜¾ç¤º
            if isinstance(value, str):
                if len(value) > 500:
                    with st.expander(f"**{field}** ({len(value)} å­—ç¬¦)", expanded=False):
                        st.text(value)
                else:
                    st.markdown(f"**{field}**")
                    st.info(value)
            elif isinstance(value, (list, dict)):
                with st.expander(f"**{field}** ({type(value).__name__})"):
                    st.json(value)
            else:
                st.markdown(f"**{field}**: {value}")
        
        st.markdown("---")


def render():
    """æ¸²æŸ“é¡µé¢"""
    st.markdown('<h1 class="module-title">Dataset é€è§†é•œ</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="tip-box">
    ğŸ’¡ æ— éœ€ä¸‹è½½å…¨é‡æ•°æ®ï¼Œæµå¼é¢„è§ˆ HuggingFace Datasetï¼Œå¿«é€Ÿåˆ†ææ•°æ®åˆ†å¸ƒå’Œè´¨é‡ã€‚
    æ”¯æŒæ•°æ®ç»Ÿè®¡ã€åˆ†å¸ƒå¯è§†åŒ–ã€è´¨é‡æ£€æŸ¥ç­‰åŠŸèƒ½ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # æ•°æ®é›†é…ç½®
    st.markdown("### âš™ï¸ æ•°æ®é›†é…ç½®")
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        dataset_id = st.text_input(
            "Dataset ID",
            value="tatsu-lab/alpaca",
            placeholder="è¾“å…¥ HuggingFace Dataset ID",
            help="ä¾‹å¦‚: tatsu-lab/alpaca, databricks/dolly-15k"
        )
    
    with col2:
        config_name = st.text_input(
            "Config (å¯é€‰)",
            value="",
            placeholder="default",
            help="æŸäº›æ•°æ®é›†æœ‰å¤šä¸ªé…ç½®ï¼Œå¦‚è¯­è¨€å­é›†"
        )
    
    with col3:
        split = st.selectbox(
            "Split",
            ["train", "validation", "test"],
            help="æ•°æ®é›†åˆ†ç‰‡"
        )
    
    col_samples, col_action = st.columns([2, 1])
    
    with col_samples:
        num_samples = st.slider(
            "é¢„è§ˆæ ·æœ¬æ•°",
            min_value=10,
            max_value=500,
            value=100,
            step=10,
            help="æµå¼åŠ è½½çš„æ ·æœ¬æ•°é‡"
        )
    
    with col_action:
        st.markdown("<br>", unsafe_allow_html=True)
        load_button = st.button("ğŸš€ åŠ è½½æ•°æ®é›†", type="primary", width='stretch')
    
    if load_button:
        try:
            from datasets import load_dataset
            
            with st.spinner(f"æµå¼åŠ è½½ {dataset_id}..."):
                # æ„å»ºåŠ è½½å‚æ•°
                load_kwargs = {
                    "path": dataset_id,
                    "split": split,
                    "streaming": True,
                    "trust_remote_code": True
                }
                
                if config_name:
                    load_kwargs["name"] = config_name
                
                # å°è¯•åŠ è½½
                try:
                    ds = load_dataset(**load_kwargs)
                except Exception as e:
                    # å¦‚æœ streaming å¤±è´¥ï¼Œå°è¯•é streaming
                    st.warning(f"æµå¼åŠ è½½å¤±è´¥ï¼Œå°è¯•æ ‡å‡†åŠ è½½...")
                    load_kwargs["streaming"] = False
                    ds = load_dataset(**load_kwargs)
                    ds = iter(ds)
                
                # è·å–æ ·æœ¬
                samples = []
                progress_bar = st.progress(0)
                for i, item in enumerate(ds):
                    if i >= num_samples:
                        break
                    samples.append(item)
                    if i % 10 == 0:
                        progress_bar.progress(i / num_samples)
                
                progress_bar.progress(1.0)
            
            if not samples:
                st.error("æœªèƒ½åŠ è½½ä»»ä½•æ ·æœ¬")
                return
            
            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} æ¡æ ·æœ¬")
            
            # ä¿å­˜åˆ° session_state
            st.session_state['dataset_samples'] = samples
            st.session_state['dataset_id'] = dataset_id
            
        except ImportError:
            st.error("è¯·ç¡®ä¿å·²å®‰è£… `datasets` åº“ï¼š`pip install datasets`")
            return
        except Exception as e:
            st.error(f"åŠ è½½å¤±è´¥: {str(e)}")
            st.info("""
            **å¸¸è§é—®é¢˜æ’æŸ¥**:
            - æ£€æŸ¥ç½‘ç»œè¿æ¥
            - ç¡®è®¤ Dataset ID æ­£ç¡®
            - æŸäº›æ•°æ®é›†éœ€è¦æŒ‡å®š config
            - éƒ¨åˆ†æ•°æ®é›†éœ€è¦ç™»å½• HuggingFace
            """)
            return
    
    # å¦‚æœæœ‰å·²åŠ è½½çš„æ•°æ®
    if 'dataset_samples' not in st.session_state:
        # æ˜¾ç¤ºå¸¸ç”¨æ•°æ®é›†æ¨è
        st.markdown("---")
        st.markdown("### ğŸ“š å¸¸ç”¨æ•°æ®é›†æ¨è")
        
        datasets_info = [
            {"id": "tatsu-lab/alpaca", "desc": "52K æŒ‡ä»¤å¾®è°ƒæ•°æ®", "type": "SFT"},
            {"id": "databricks/dolly-15k", "desc": "15K é«˜è´¨é‡æŒ‡ä»¤æ•°æ®", "type": "SFT"},
            {"id": "HuggingFaceH4/ultrachat_200k", "desc": "200K å¤šè½®å¯¹è¯", "type": "Chat"},
            {"id": "Open-Orca/OpenOrca", "desc": "å¤§è§„æ¨¡æ¨ç†æ•°æ®é›†", "type": "Reasoning"},
            {"id": "garage-bAInd/Open-Platypus", "desc": "é«˜è´¨é‡ STEM æ•°æ®", "type": "SFT"},
            {"id": "teknium/openhermes", "desc": "é«˜è´¨é‡ç»¼åˆæ•°æ®", "type": "SFT"},
            {"id": "HuggingFaceFW/fineweb", "desc": "é«˜è´¨é‡ Web é¢„è®­ç»ƒ", "type": "Pretrain"},
            {"id": "allenai/c4", "desc": "Common Crawl æ¸…æ´—ç‰ˆ", "type": "Pretrain"}
        ]
        
        # åˆ†ç±»å±•ç¤º
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ğŸ¯ SFT æ•°æ®é›†**")
            for ds in datasets_info:
                if ds['type'] == 'SFT':
                    st.markdown(f"- `{ds['id']}`: {ds['desc']}")
        
        with col2:
            st.markdown("**ğŸ’¬ å¯¹è¯ & é¢„è®­ç»ƒ**")
            for ds in datasets_info:
                if ds['type'] in ['Chat', 'Reasoning', 'Pretrain']:
                    st.markdown(f"- `{ds['id']}`: {ds['desc']}")
        
        return
    
    # æœ‰æ•°æ®æ—¶æ˜¾ç¤ºåˆ†æ
    samples = st.session_state['dataset_samples']
    df = pd.DataFrame(samples)
    fields = list(samples[0].keys())
    
    # åˆ›å»º tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š æ•°æ®æ¦‚è§ˆ", "ğŸ“ˆ åˆ†å¸ƒåˆ†æ", "ğŸ“‹ è´¨é‡æ£€æŸ¥", "ğŸ” æ ·æœ¬æµè§ˆ"])
    
    with tab1:
        st.markdown("### å­—æ®µç»“æ„")
        
        # å­—æ®µä¿¡æ¯è¡¨æ ¼
        field_info = []
        for field in fields:
            sample_val = samples[0][field]
            val_type = type(sample_val).__name__
            
            # ç¤ºä¾‹å€¼
            if isinstance(sample_val, str):
                preview = sample_val[:100] + '...' if len(sample_val) > 100 else sample_val
            elif isinstance(sample_val, (list, dict)):
                preview = str(sample_val)[:100] + '...'
            else:
                preview = str(sample_val)
            
            field_info.append({
                'å­—æ®µå': field,
                'ç±»å‹': val_type,
                'ç¤ºä¾‹å€¼': preview
            })
        
        st.dataframe(
            pd.DataFrame(field_info), 
            width='stretch',
            hide_index=True,
            column_config={
                "ç¤ºä¾‹å€¼": st.column_config.TextColumn(width="large")
            }
        )
        
        # å­—æ®µç»Ÿè®¡
        st.markdown("### å­—æ®µç»Ÿè®¡")
        
        field_stats = []
        for field in fields:
            values = [s[field] for s in samples]
            stats = analyze_field_content(values, field)
            field_stats.append(stats)
        
        # æ˜¾ç¤ºç»Ÿè®¡è¡¨æ ¼
        stats_df = pd.DataFrame([{
            'å­—æ®µ': s['name'],
            'ç±»å‹': s['type'],
            'ç©ºå€¼æ•°': s['null_count'],
            'å¹³å‡é•¿åº¦/å€¼': f"{s['avg_length']:.0f}" if s['avg_length'] > 0 else '-',
            'æœ€å°': f"{s['min_length']:.0f}" if s['min_length'] > 0 else '-',
            'æœ€å¤§': f"{s['max_length']:.0f}" if s['max_length'] > 0 else '-'
        } for s in field_stats])
        
        st.dataframe(stats_df, width='stretch', hide_index=True)
        
        # æ±‡æ€»å›¾è¡¨
        fig = render_field_stats_chart(field_stats)
        if fig:
            st.plotly_chart(fig, width='stretch')
    
    with tab2:
        st.markdown("### åˆ†å¸ƒåˆ†æ")
        
        # é€‰æ‹©è¦åˆ†æçš„å­—æ®µ
        string_fields = [f for f in fields if isinstance(samples[0].get(f), str)]
        
        if not string_fields:
            st.info("æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å­—æ®µç”¨äºåˆ†æ")
        else:
            selected_field = st.selectbox(
                "é€‰æ‹©åˆ†æå­—æ®µ",
                string_fields,
                help="é€‰æ‹©ä¸€ä¸ªæ–‡æœ¬å­—æ®µè¿›è¡Œåˆ†å¸ƒåˆ†æ"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                # é•¿åº¦åˆ†å¸ƒ
                fig_len = render_length_distribution(df, selected_field)
                st.plotly_chart(fig_len, width='stretch')
            
            with col2:
                # è¯æ•°åˆ†å¸ƒ
                fig_word = render_word_count_distribution(df, selected_field)
                st.plotly_chart(fig_word, width='stretch')
            
            # è¯¦ç»†ç»Ÿè®¡
            st.markdown("#### è¯¦ç»†ç»Ÿè®¡")
            
            lengths = df[selected_field].astype(str).str.len()
            word_counts = df[selected_field].astype(str).str.split().str.len()
            
            col_a, col_b, col_c, col_d = st.columns(4)
            
            with col_a:
                st.metric("å¹³å‡å­—ç¬¦æ•°", f"{lengths.mean():.0f}")
            with col_b:
                st.metric("ä¸­ä½æ•°å­—ç¬¦æ•°", f"{lengths.median():.0f}")
            with col_c:
                st.metric("å¹³å‡è¯æ•°", f"{word_counts.mean():.0f}")
            with col_d:
                st.metric("æœ€é•¿æ ·æœ¬", f"{lengths.max():.0f} å­—ç¬¦")
            
            # Token ä¼°ç®—
            st.markdown("#### Token æ•°ä¼°ç®—")
            st.caption("ç²—ç•¥ä¼°ç®—ï¼Œå®é™…æ•°é‡å–å†³äºå…·ä½“ tokenizer")
            
            # å‡è®¾å¹³å‡ 4 å­—ç¬¦ = 1 token (è‹±æ–‡)
            # ä¸­æ–‡å¤§çº¦ 1.5-2 å­—ç¬¦ = 1 token
            avg_tokens_en = lengths.mean() / 4
            avg_tokens_zh = lengths.mean() / 1.5
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ä¼°ç®— Token (è‹±æ–‡æ¨¡å‹)", f"~{avg_tokens_en:.0f}")
            with col2:
                st.metric("ä¼°ç®— Token (ä¸­æ–‡æ¨¡å‹)", f"~{avg_tokens_zh:.0f}")
    
    with tab3:
        # æ•°æ®è´¨é‡æ£€æŸ¥
        quality = check_data_quality(df)
        render_quality_report(quality)
        
        # é—®é¢˜æ ·æœ¬å±•ç¤º
        st.markdown("### âš ï¸ æ½œåœ¨é—®é¢˜æ ·æœ¬")
        
        problem_samples = []
        
        for field in fields:
            if df[field].dtype == 'object':
                # æ‰¾å‡ºç©ºå€¼æˆ–å¾ˆçŸ­çš„æ ·æœ¬
                short_mask = df[field].astype(str).str.len() < 10
                empty_mask = df[field].astype(str).str.strip() == ''
                
                for idx in df[short_mask | empty_mask].index[:3]:
                    problem_samples.append({
                        'index': idx,
                        'field': field,
                        'issue': 'å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º',
                        'value': str(df.loc[idx, field])[:100]
                    })
        
        if problem_samples:
            st.dataframe(
                pd.DataFrame(problem_samples),
                width='stretch',
                hide_index=True
            )
        else:
            st.success("âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®è´¨é‡é—®é¢˜")
    
    with tab4:
        render_sample_viewer(samples, fields)
        
        # åŸå§‹ JSON æŸ¥çœ‹
        st.markdown("### ğŸ“„ åŸå§‹ JSON")
        
        json_idx = st.number_input(
            "æ ·æœ¬ç´¢å¼•",
            min_value=0,
            max_value=len(samples) - 1,
            value=0
        )
        
        st.json(samples[json_idx])
        
        # å¯¼å‡ºåŠŸèƒ½
        st.markdown("### ğŸ’¾ æ•°æ®å¯¼å‡º")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # JSON å¯¼å‡º
            json_str = json.dumps(samples, ensure_ascii=False, indent=2)
            st.download_button(
                "ğŸ“¥ ä¸‹è½½ JSON",
                data=json_str,
                file_name=f"{st.session_state.get('dataset_id', 'dataset').replace('/', '_')}_samples.json",
                mime="application/json"
            )
        
        with col2:
            # CSV å¯¼å‡º (ä»…é€‚ç”¨äºæ‰å¹³ç»“æ„)
            try:
                csv_str = df.to_csv(index=False)
                st.download_button(
                    "ğŸ“¥ ä¸‹è½½ CSV",
                    data=csv_str,
                    file_name=f"{st.session_state.get('dataset_id', 'dataset').replace('/', '_')}_samples.csv",
                    mime="text/csv"
                )
            except:
                st.caption("CSV å¯¼å‡ºä¸é€‚ç”¨äºå¤æ‚åµŒå¥—ç»“æ„")

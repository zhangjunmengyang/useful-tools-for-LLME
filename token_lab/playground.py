"""
åˆ†è¯ç¼–ç  - Gradio ç‰ˆæœ¬
äº¤äº’å¼åˆ†è¯ã€ç¼–è§£ç ä¸å­—èŠ‚åˆ†æ
"""

import gradio as gr
import pandas as pd
from token_lab.tokenizer_utils import (
    load_tokenizer,
    get_token_info,
    get_tokenizer_info,
    decode_token_ids,
    calculate_compression_stats,
    get_normalization_info,
    get_unicode_info,
    get_special_tokens_map,
    get_models_by_category,
    MODEL_CATEGORIES,
    TOKEN_COLORS
)


def get_category_choices():
    """è·å–æ¨¡å‹å‚å•†åˆ—è¡¨"""
    return list(MODEL_CATEGORIES.keys())


def get_model_choices(category):
    """æ ¹æ®å‚å•†è·å–æ¨¡å‹åˆ—è¡¨"""
    if not category:
        return []
    models = get_models_by_category(category)
    return [name for name, _ in models]


def get_model_id(category, model_name):
    """è·å–æ¨¡å‹ ID"""
    if not category or not model_name:
        return None
    models = get_models_by_category(category)
    for name, model_id in models:
        if name == model_name:
            return model_id
    return None


def render_tokens_html(token_info_list, show_ids=True):
    """æ¸²æŸ“åˆ†è¯ç»“æœä¸º HTML"""
    if not token_info_list:
        return "<div style='color: #6B7280; padding: 20px;'>è¯·è¾“å…¥æ–‡æœ¬</div>"
    
    html_parts = [
        '<div style="background-color: #F3F4F6; border: 1px solid #E5E7EB; '
        'border-radius: 8px; padding: 16px; line-height: 2.4; '
        'font-family: \'JetBrains Mono\', monospace;">'
    ]
    
    for idx, info in enumerate(token_info_list):
        color = TOKEN_COLORS[idx % len(TOKEN_COLORS)]
        token_str = info['token_str']
        token_id = info['token_id']
        raw_token = info.get('raw_token', token_str)
        is_special = info.get('is_special', False)
        is_byte_fallback = info.get('is_byte_fallback', False)
        
        # è½¬ä¹‰å’Œæ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        display_str = token_str.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        display_str = display_str.replace(' ', 'â£')
        display_str = display_str.replace('\n', 'â†µ')
        display_str = display_str.replace('\t', 'â†’')
        
        if not display_str.strip():
            display_str = repr(token_str)[1:-1] if token_str else '[EMPTY]'
        
        # æ ·å¼
        border_style = ""
        label = ""
        if is_special:
            border_style = "border: 2px solid #DC2626;"
            label = '<span style="color:#DC2626;font-size:10px;margin-left:2px;">[S]</span>'
        elif is_byte_fallback:
            border_style = "border: 2px dashed #D97706;"
            label = '<span style="color:#D97706;font-size:10px;margin-left:2px;">[B]</span>'
        
        id_html = f'<sub style="font-size: 10px; color: #6B7280; margin-left: 3px;">{token_id}</sub>' if show_ids else ''
        
        html_parts.append(
            f'<span title="Token: {raw_token} | ID: {token_id}" '
            f'style="background-color: {color}; color: #111827; padding: 4px 8px; margin: 2px; '
            f'border-radius: 4px; display: inline-block; font-size: 13px; cursor: default; {border_style}">'
            f'{display_str}{id_html}{label}</span>'
        )
    
    html_parts.append('</div>')
    return ''.join(html_parts)


def encode_text(category, model_name, text, show_ids, add_special):
    """ç¼–ç æ–‡æœ¬"""
    if not category or not model_name:
        return (
            "<div style='color: #DC2626;'>è¯·é€‰æ‹©æ¨¡å‹</div>",
            "", "", "", "",
            pd.DataFrame()
        )
    
    model_id = get_model_id(category, model_name)
    if not model_id:
        return (
            "<div style='color: #DC2626;'>æ¨¡å‹ä¸å­˜åœ¨</div>",
            "", "", "", "",
            pd.DataFrame()
        )
    
    tokenizer = load_tokenizer(model_id)
    if not tokenizer:
        return (
            "<div style='color: #DC2626;'>æ¨¡å‹åŠ è½½å¤±è´¥</div>",
            "", "", "", "",
            pd.DataFrame()
        )
    
    if not text:
        return (
            render_tokens_html([], show_ids),
            "0", "0", "-", "-",
            pd.DataFrame()
        )
    
    # è·å– token ä¿¡æ¯
    if add_special:
        encoding = tokenizer(text, add_special_tokens=True)
        token_ids = encoding["input_ids"]
        token_info_list = []
        special_ids = set(tokenizer.all_special_ids) if hasattr(tokenizer, 'all_special_ids') else set()
        
        for idx, tid in enumerate(token_ids):
            raw_token = tokenizer.convert_ids_to_tokens([tid])[0] if hasattr(tokenizer, 'convert_ids_to_tokens') else ""
            token_str = tokenizer.decode([tid])
            is_byte_token = '\ufffd' in token_str
            display_token_str = raw_token if (is_byte_token and raw_token) else token_str
            
            try:
                byte_seq = ' '.join([f'0x{b:02X}' for b in (raw_token or token_str).encode('utf-8')])
            except:
                byte_seq = "N/A"
            
            token_info_list.append({
                "token_str": display_token_str,
                "decoded_str": token_str,
                "raw_token": raw_token,
                "token_id": tid,
                "byte_sequence": byte_seq,
                "is_special": tid in special_ids,
                "is_byte_fallback": raw_token.startswith('<0x') and raw_token.endswith('>') if raw_token else False,
                "is_byte_token": is_byte_token,
                "index": idx
            })
    else:
        token_info_list = get_token_info(tokenizer, text)
    
    # è®¡ç®—ç»Ÿè®¡
    stats = calculate_compression_stats(text, len(token_info_list))
    
    # Token IDs
    ids = [info['token_id'] for info in token_info_list]
    
    # è¯¦ç»†ä¿¡æ¯è¡¨æ ¼
    df_data = []
    for info in token_info_list:
        df_data.append({
            "Index": info['index'],
            "Token": info['raw_token'],
            "Decoded": repr(info['token_str']),
            "ID": info['token_id'],
            "Bytes": info.get('byte_sequence', 'N/A'),
            "Special": "âœ“" if info.get('is_special') else "",
            "Fallback": "âœ“" if info.get('is_byte_fallback') else ""
        })
    
    return (
        render_tokens_html(token_info_list, show_ids),
        str(stats['token_count']),
        str(stats['char_count']),
        str(stats['chars_per_token']),
        str(stats['bytes_per_token']),
        pd.DataFrame(df_data)
    )


def decode_ids(category, model_name, id_input):
    """è§£ç  Token IDs"""
    if not category or not model_name:
        return "è¯·é€‰æ‹©æ¨¡å‹", ""
    
    model_id = get_model_id(category, model_name)
    if not model_id:
        return "æ¨¡å‹ä¸å­˜åœ¨", ""
    
    tokenizer = load_tokenizer(model_id)
    if not tokenizer:
        return "æ¨¡å‹åŠ è½½å¤±è´¥", ""
    
    if not id_input:
        return "", ""
    
    try:
        cleaned = id_input.strip().strip('[]')
        token_ids = [int(x.strip()) for x in cleaned.split(',') if x.strip()]
        
        if not token_ids:
            return "è¯·è¾“å…¥æœ‰æ•ˆçš„ Token IDs", ""
        
        decoded_text, individual_tokens = decode_token_ids(tokenizer, token_ids)
        
        details = []
        for tok in individual_tokens:
            details.append(f"**ID {tok['token_id']}** â†’ `{tok['raw_token']}` â†’ \"{tok['token_str']}\"")
        
        return decoded_text, "\n\n".join(details)
    
    except ValueError as e:
        return f"æ ¼å¼é”™è¯¯: {str(e)}", ""


def analyze_bytes(category, model_name, text):
    """å­—èŠ‚åˆ†æ"""
    if not category or not model_name:
        return "<div>è¯·é€‰æ‹©æ¨¡å‹</div>", "0", "0", "0", "0", pd.DataFrame()
    
    model_id = get_model_id(category, model_name)
    if not model_id:
        return "<div>æ¨¡å‹ä¸å­˜åœ¨</div>", "0", "0", "0", "0", pd.DataFrame()
    
    tokenizer = load_tokenizer(model_id)
    if not tokenizer:
        return "<div>æ¨¡å‹åŠ è½½å¤±è´¥</div>", "0", "0", "0", "0", pd.DataFrame()
    
    if not text:
        return render_tokens_html([], True), "0", "0", "0", "0", pd.DataFrame()
    
    token_info = get_token_info(tokenizer, text)
    
    total = len(token_info)
    fallback_count = sum(1 for t in token_info if t.get('is_byte_fallback', False))
    byte_token_count = sum(1 for t in token_info if t.get('is_byte_token', False))
    special_count = sum(1 for t in token_info if t.get('is_special', False))
    
    # Fallback è¯¦æƒ…
    fallback_data = []
    for t in token_info:
        if t.get('is_byte_fallback'):
            fallback_data.append({
                "Index": t['index'],
                "Token": t['raw_token'],
                "ID": t['token_id'],
                "Byte": t.get('byte_sequence', 'N/A')
            })
    
    return (
        render_tokens_html(token_info, True),
        str(total),
        str(fallback_count),
        str(byte_token_count),
        str(special_count),
        pd.DataFrame(fallback_data) if fallback_data else pd.DataFrame()
    )


def analyze_unicode(text):
    """Unicode åˆ†æ"""
    if not text:
        return pd.DataFrame(), pd.DataFrame()
    
    # è§„èŒƒåŒ–å¯¹æ¯”
    norm_info = get_normalization_info(text)
    norm_data = [
        {"å½¢å¼": "åŸå§‹", "é•¿åº¦": norm_info['original_len'], "æ–‡æœ¬": norm_info['original'], "ç›¸åŒ": "-"},
        {"å½¢å¼": "NFC", "é•¿åº¦": norm_info['NFC_len'], "æ–‡æœ¬": norm_info['NFC'], "ç›¸åŒ": "âœ“" if norm_info['nfc_equal'] else "âœ—"},
        {"å½¢å¼": "NFD", "é•¿åº¦": norm_info['NFD_len'], "æ–‡æœ¬": norm_info['NFD'], "ç›¸åŒ": "âœ“" if norm_info['nfd_equal'] else "âœ—"},
        {"å½¢å¼": "NFKC", "é•¿åº¦": norm_info['NFKC_len'], "æ–‡æœ¬": norm_info['NFKC'], "ç›¸åŒ": "-"},
        {"å½¢å¼": "NFKD", "é•¿åº¦": norm_info['NFKD_len'], "æ–‡æœ¬": norm_info['NFKD'], "ç›¸åŒ": "-"},
    ]
    
    # Unicode è¯¦æƒ…
    unicode_data = []
    for char in text[:50]:
        info = get_unicode_info(char)
        unicode_data.append({
            "å­—ç¬¦": char,
            "åç§°": info.get('name', 'N/A'),
            "åˆ†ç±»": info.get('category', 'N/A'),
            "ç ç‚¹": info.get('codepoint', 'N/A'),
            "åè¿›åˆ¶": info.get('decimal', 'N/A'),
            "UTF-8": info.get('utf8_bytes', 'N/A')
        })
    
    return pd.DataFrame(norm_data), pd.DataFrame(unicode_data)


def get_special_tokens(category, model_name):
    """è·å–ç‰¹æ®Š Token"""
    if not category or not model_name:
        return pd.DataFrame(), pd.DataFrame()
    
    model_id = get_model_id(category, model_name)
    if not model_id:
        return pd.DataFrame(), pd.DataFrame()
    
    tokenizer = load_tokenizer(model_id)
    if not tokenizer:
        return pd.DataFrame(), pd.DataFrame()
    
    special_map = get_special_tokens_map(tokenizer)
    
    if not special_map:
        return pd.DataFrame(), pd.DataFrame()
    
    # æ ‡å‡†ç‰¹æ®Š Token
    standard = []
    for name in ['bos_token', 'eos_token', 'pad_token', 'unk_token', 'cls_token', 'sep_token', 'mask_token']:
        if name in special_map:
            standard.append({
                "åç§°": name,
                "Token": special_map[name]['token'],
                "ID": special_map[name]['id']
            })
    
    # é¢å¤–ç‰¹æ®Š Token
    additional = []
    if 'additional_special_tokens' in special_map and special_map['additional_special_tokens']:
        for t in special_map['additional_special_tokens'][:20]:
            additional.append({"Token": t['token'], "ID": t['id']})
    
    return pd.DataFrame(standard), pd.DataFrame(additional)


def get_model_info(category, model_name):
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    if not category or not model_name:
        return "è¯·é€‰æ‹©æ¨¡å‹"
    
    model_id = get_model_id(category, model_name)
    if not model_id:
        return "æ¨¡å‹ä¸å­˜åœ¨"
    
    tokenizer = load_tokenizer(model_id)
    if not tokenizer:
        return "æ¨¡å‹åŠ è½½å¤±è´¥"
    
    info = get_tokenizer_info(tokenizer)
    
    return f"""
**Tokenizer**: `{model_id}`

| å±æ€§ | å€¼ |
|------|-----|
| è¯è¡¨å¤§å° | {info['vocab_size']:,} |
| æœ€å¤§é•¿åº¦ | {info['model_max_length']} |
| ç®—æ³•ç±»å‹ | {info['algorithm'].split('(')[0].strip()} |
| BOS Token | `{info.get('bos_token', 'N/A')}` |
| EOS Token | `{info.get('eos_token', 'N/A')}` |
| PAD Token | `{info.get('pad_token', 'N/A')}` |
| UNK Token | `{info.get('unk_token', 'N/A')}` |
"""


def render():
    """æ¸²æŸ“é¡µé¢"""
    
    gr.Markdown("## åˆ†è¯ç¼–ç ")
    
    # æ¨¡å‹é€‰æ‹©
    initial_category = get_category_choices()[0] if get_category_choices() else None
    initial_models = get_model_choices(initial_category) if initial_category else []
    
    with gr.Row():
        with gr.Column(scale=1):
            category_dropdown = gr.Dropdown(
                choices=get_category_choices(),
                label="æ¨¡å‹å‚å•†",
                value=initial_category,
                interactive=True
            )
        with gr.Column(scale=2):
            model_dropdown = gr.Dropdown(
                choices=initial_models,
                label="é€‰æ‹©æ¨¡å‹",
                value=initial_models[0] if initial_models else None,
                interactive=True
            )
    
    # æ¨¡å‹ ID æ˜¾ç¤º
    model_id_text = gr.Markdown("", elem_id="model-id-display")
    
    # æ¨¡å‹ä¿¡æ¯æŠ˜å 
    with gr.Accordion("æ¨¡å‹ä¿¡æ¯", open=False):
        model_info_md = gr.Markdown("")
    
    gr.Markdown("---")
    
    # åŠŸèƒ½ Tabs
    with gr.Tabs() as tabs:
        
        # ========== ç¼–ç  Tab ==========
        with gr.Tab("ç¼–ç "):
            encode_input = gr.Textbox(
                label="è¾“å…¥æ–‡æœ¬",
                placeholder="è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ç¼–ç ...",
                lines=4
            )
            
            with gr.Row():
                show_ids_cb = gr.Checkbox(label="æ˜¾ç¤º Token ID", value=True)
                add_special_cb = gr.Checkbox(label="æ·»åŠ ç‰¹æ®Š Token", value=False)
            
            with gr.Row():
                with gr.Column(scale=1):
                    stat_tokens = gr.Textbox(label="Token æ•°", interactive=False)
                with gr.Column(scale=1):
                    stat_chars = gr.Textbox(label="å­—ç¬¦æ•°", interactive=False)
                with gr.Column(scale=1):
                    stat_chars_per_token = gr.Textbox(label="å­—ç¬¦/Token", interactive=False)
                with gr.Column(scale=1):
                    stat_bytes_per_token = gr.Textbox(label="å­—èŠ‚/Token", interactive=False)
            
            gr.Markdown("### åˆ†è¯ç»“æœ")
            tokens_html = gr.HTML("")
            
            with gr.Accordion("è¯¦ç»†ä¿¡æ¯", open=False):
                detail_df = gr.Dataframe(
                    headers=["Index", "Token", "Decoded", "ID", "Bytes", "Special", "Fallback"],
                    interactive=False
                )
        
        # ========== è§£ç  Tab ==========
        with gr.Tab("è§£ç "):
            decode_input = gr.Textbox(
                label="Token IDs",
                placeholder="ä¾‹å¦‚: 128000, 50256 æˆ– [128000, 50256]",
                lines=1,
                value="128000, 50256"
            )
            
            gr.Markdown("### è§£ç ç»“æœ")
            decode_result = gr.Textbox(label="è§£ç æ–‡æœ¬", interactive=False, lines=3)
            decode_details = gr.Markdown("")
        
        # ========== å­—èŠ‚åˆ†æ Tab ==========
        with gr.Tab("å­—èŠ‚åˆ†æ"):
            byte_input = gr.Textbox(
                label="è¾“å…¥æ–‡æœ¬",
                placeholder="è¾“å…¥ Emojiã€ç”Ÿåƒ»å­—ç­‰ç‰¹æ®Šå­—ç¬¦æŸ¥çœ‹å­—èŠ‚çº§åˆ†è¯",
                lines=2,
                value="ğŸ‰ Hello ä½ å¥½ ğ €€"
            )
            
            with gr.Row():
                byte_total = gr.Textbox(label="æ€» Token", interactive=False)
                byte_fallback = gr.Textbox(label="Byte Fallback", interactive=False)
                byte_bpe = gr.Textbox(label="å­—èŠ‚çº§ BPE", interactive=False)
                byte_special = gr.Textbox(label="ç‰¹æ®Š Token", interactive=False)
            
            gr.Markdown("### åˆ†è¯ç»“æœ")
            byte_tokens_html = gr.HTML("")
            
            with gr.Accordion("Byte Fallback è¯¦æƒ…", open=False):
                byte_fallback_df = gr.Dataframe(interactive=False)
        
        # ========== Unicode Tab ==========
        with gr.Tab("Unicode"):
            unicode_input = gr.Textbox(
                label="è¾“å…¥æ–‡æœ¬",
                placeholder="è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬",
                lines=1,
                value="cafÃ© ä½ å¥½ ğŸ‰"
            )
            
            gr.Markdown("### è§„èŒƒåŒ–å¯¹æ¯”")
            norm_df = gr.Dataframe(interactive=False)
            
            gr.Markdown("### Unicode è¯¦æƒ…")
            unicode_df = gr.Dataframe(interactive=False)
        
        # ========== ç‰¹æ®Š Token Tab ==========
        with gr.Tab("ç‰¹æ®Š Token"):
            gr.Markdown("### æ ‡å‡†ç‰¹æ®Š Token")
            standard_df = gr.Dataframe(interactive=False)
            
            gr.Markdown("### é¢å¤–ç‰¹æ®Š Token")
            additional_df = gr.Dataframe(interactive=False)
    
    # ==================== äº‹ä»¶ç»‘å®š ====================
    
    # å‚å•†å˜åŒ– -> æ›´æ–°æ¨¡å‹åˆ—è¡¨
    def update_models(category):
        models = get_model_choices(category)
        return gr.update(choices=models, value=models[0] if models else None)
    
    category_dropdown.change(
        fn=update_models,
        inputs=[category_dropdown],
        outputs=[model_dropdown]
    )
    
    
    # ç¼–ç åŠŸèƒ½
    encode_inputs = [category_dropdown, model_dropdown, encode_input, show_ids_cb, add_special_cb]
    encode_outputs = [tokens_html, stat_tokens, stat_chars, stat_chars_per_token, stat_bytes_per_token, detail_df]
    
    encode_input.change(fn=encode_text, inputs=encode_inputs, outputs=encode_outputs)
    show_ids_cb.change(fn=encode_text, inputs=encode_inputs, outputs=encode_outputs)
    add_special_cb.change(fn=encode_text, inputs=encode_inputs, outputs=encode_outputs)
    
    # è§£ç åŠŸèƒ½ - è‡ªåŠ¨è§¦å‘
    decode_input.change(
        fn=decode_ids,
        inputs=[category_dropdown, model_dropdown, decode_input],
        outputs=[decode_result, decode_details]
    )
    
    # å­—èŠ‚åˆ†æ - è‡ªåŠ¨è§¦å‘
    byte_input.change(
        fn=analyze_bytes,
        inputs=[category_dropdown, model_dropdown, byte_input],
        outputs=[byte_tokens_html, byte_total, byte_fallback, byte_bpe, byte_special, byte_fallback_df]
    )
    
    # Unicode åˆ†æ - è‡ªåŠ¨è§¦å‘
    unicode_input.change(
        fn=analyze_unicode,
        inputs=[unicode_input],
        outputs=[norm_df, unicode_df]
    )
    
    # æ¨¡å‹å˜åŒ–æ—¶æ›´æ–°æ‰€æœ‰ Tab çš„æ•°æ®
    def on_model_change_all(category, model_name):
        """æ¨¡å‹å˜åŒ–æ—¶æ›´æ–°æ‰€æœ‰ç›¸å…³å†…å®¹"""
        model_id = get_model_id(category, model_name)
        id_text = f"**Tokenizer**: `{model_id}`" if model_id else ""
        info = get_model_info(category, model_name)
        special_standard, special_additional = get_special_tokens(category, model_name)
        return id_text, info, special_standard, special_additional
    
    model_dropdown.change(
        fn=on_model_change_all,
        inputs=[category_dropdown, model_dropdown],
        outputs=[model_id_text, model_info_md, standard_df, additional_df]
    )
    
    # åˆå§‹åŒ–åŠ è½½å‡½æ•°
    def on_load():
        """é¡µé¢åŠ è½½æ—¶è®¡ç®—é»˜è®¤å€¼"""
        # è·å–æ¨¡å‹ä¿¡æ¯å’Œç‰¹æ®Š Token
        id_text, info, special_standard, special_additional = on_model_change_all(
            initial_category, initial_models[0] if initial_models else None
        )
        # è§£ç é»˜è®¤å€¼
        decode_text, decode_detail = decode_ids(initial_category, initial_models[0] if initial_models else None, "128000, 50256")
        # å­—èŠ‚åˆ†æé»˜è®¤å€¼
        byte_result = analyze_bytes(initial_category, initial_models[0] if initial_models else None, "ğŸ‰ Hello ä½ å¥½ ğ €€")
        # Unicode åˆ†æé»˜è®¤å€¼
        norm_result, unicode_result = analyze_unicode("cafÃ© ä½ å¥½ ğŸ‰")
        
        return (
            id_text, info, special_standard, special_additional,
            decode_text, decode_detail,
            byte_result[0], byte_result[1], byte_result[2], byte_result[3], byte_result[4], byte_result[5],
            norm_result, unicode_result
        )
    
    # è¿”å› load äº‹ä»¶ä¿¡æ¯ä¾›ä¸» app è°ƒç”¨
    return {
        'load_fn': on_load,
        'load_outputs': [
            model_id_text, model_info_md, standard_df, additional_df,
            decode_result, decode_details,
            byte_tokens_html, byte_total, byte_fallback, byte_bpe, byte_special, byte_fallback_df,
            norm_df, unicode_df
        ]
    }
